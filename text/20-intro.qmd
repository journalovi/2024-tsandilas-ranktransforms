## Introduction {#intro}
In Human-Computer Interaction (HCI) and various fields within the behavioral sciences, researchers often gather data through user studies. Such data are typically messy, have small sample sizes, and may violate common statistical assumptions, such as the normality assumption. To address these challenges, researchers commonly employ nonparametric tests, which require fewer assumptions about the data. However, while nonparametric tests for simple one-factorial designs are well-established, researchers face challenges in selecting appropriate methods when dealing with multifactorial designs that require testing for both main effects and interactions. The Aligned Rank Transform or ART [@higgins:1990; @Salter:1993; @wobbrock:2011] addresses this problem by bridging the gap between nonparametric tests and ANOVAs. Its popularity in HCI research has surged, facilitated in part by the ARTool toolkit [@wobbrock:2011; @artool], which simplifies the use of the method. 

Early Monte Carlo experiments in the 1990s [@Salter:1993;@Mansouri:1995] and more recent studies [@elkin:2021] suggested that ART is a robust alternative to ANOVA when normality assumptions are violated. These results have contributed to ART's reputation as a well-established method. However, other research [@luepsen:2017; @luepsen:2018] raised concerns about the robustness of the method, demonstrating that ART fails to control the Type I error rate in many scenarios, such as when data are ordinal or are drawn from skewed distributions. Unfortunately, these warnings have not received sufficient attention, and many authors still rely on Wobbrock et al.'s [-@wobbrock:2011] assertion that *"The ART is for use in circumstances similar to the parametric ANOVA, except that the response variable may be continuous or ordinal, and is not required to be normally distributed."*

Our goal is to clarify the severity of these issues and understand the potential risks of the method. We present the outcomes of a series of Monte Carlo experiments, following a distinctive simulation methodology grounded in latent variable modeling. This approach enables us to simulate effects consistently across a broad spectrum of distributions, both discrete and continuous, and allows us to address scaling issues in interpreting interactions. To ensure the clarity of our findings and facilitate their reproducibility, we divide the experimental process into multiple smaller experiments, where each experiment focuses on a distinct variable (e.g., sample size, experimental design, variance ratio) or measure (Type I error rate, power, precision of effect size estimates).

Our findings corroborate LÃ¼psen's alarming conclusions. We provide overwhelming evidence that ART confounds effects and raises Type I error rates at very high levels across a diverse array of non-normal distributions, including skewed, binomial, and ordinal distributions, as well as distributions with unequal variances. These issues persist for both main and interaction effects. Our results further show that simpler rank transformation methods outperform ART, while parametric ANOVA generally poses fewer risks than ART when distributions deviate from normal. Given these new insights, we conclude that ART is not a viable analysis method and advocate for its abandonment. We provide recommendations for alternative analysis methods, while we also raise warnings about the interpretation of interaction effects.  

### Illustrative example 
We will begin with an illustrative example to demonstrate how the aligned rank transform can lead to an increase in false positives and a significant inflation of observed effects. This example will also serve as a brief introduction to the key concepts and methods employed throughout the paper.

Suppose an HCI researcher conducts an experiment to compare the performance of three user interface techniques (A, B, and C) that help users complete image editing tasks of four different difficulty levels. The experiment follows a fully balanced $4 \times 3$ repeated-measures factorial design, where each participant (N = 12) performs 12 tasks in a unique order. The researcher measures the time that it takes participants to complete each task. The following table presents the experimental results: 

::: {#tbl-example}
```{r, echo=FALSE, warning=FALSE}
df <- read.csv("example_data.csv", sep=",", header=TRUE, strip.white=TRUE)
kbl(df) %>% kable_paper(position = "center") %>% scroll_box(width = "740px", height = "170px")
``` 
Example dataset: Time (in minutes) spent by 12 participants for four difficulty levels and three user interface techniques. Scroll down to see the full results.
:::

The experiment is hypothetical but has similarities with real-world experiments, e.g., see the experiments of @Fruchard:2023. Time performances have been randomly sampled from a population in which: (1) *Difficulty* has a large effect; (2) *Technique* has no effect; and (3) there is no interaction effect between the two factors. To generate time values, we drew samples from a log-normal distribution. The log-normal distribution is often a good fit for real-world measurements that are bounded by zero and have low means but large variance [@Limbert:2001]. Task-completion times are good examples of such measurements [@Sauro:2010]. 

@fig-example presents two boxplots that visually summarize the main effects observed through the experiment. We plot medians to account for the fact that distributions are skewed. We observe that differences in the overall time performance of the three techniques are not visually clear, although the overall median time is somewhat higher for Technique B. In contrast, time performance clearly deteriorates as task difficulty increases. 
We also observe that for the most difficult tasks (Level 4), the median time for Technique C is lower than the median time for Techniques A and B, so we may suspect that *Difficulty* interacts with *Technique*. However, since the spread of observed values is extremely large and the number of data points is small, such differences could result from random noise. 

::: {#fig-example}
```{r, echo=FALSE, warning=FALSE, fig.height=2.7, fig.width=8}
cbPalette <- c("#999999", "#E69F00", "#F15854")

p1 <- (df %>%  group_by(Participant,Technique) %>% summarise(Time = median(Time), .groups="drop") %>%   
        ggplot(aes(x = Technique, y = Time, fill = Technique)) + 
        geom_boxplot(outlier.shape = NA) +
        ylab("Median Time (min)") +
        ylim(0, 3) + 
        geom_jitter(shape=20, position=position_jitter(0.1)) +
        theme_bw() + theme(legend.position = "none")) + scale_fill_manual(values=cbPalette)


p2 <- (df %>%  group_by(Participant,Difficulty,Technique) %>% summarise(Time = median(Time), .groups="drop") %>%   
        ggplot(aes(x = Difficulty, y = Time, fill = Technique)) + 
        geom_boxplot(outlier.shape = NA) +
        ylab("Median Time (min)") +
        ylim(0, 6) +
        #geom_jitter(shape=20, position=position_jitter(0.1)) +
        geom_point(size = 0.7, position=position_jitterdodge(0.1)) +
        theme_bw() + theme(legend.position = "none")) +
        #scale_fill_brewer()
        scale_fill_manual(values=cbPalette)

grid.arrange(p1, p2, nrow = 1, widths = c(1.5, 2.5))
```
Boxplots summarizing the results of our illustrative example. Dots represent the median time performance of each individual participant. 
:::

We opt for a multiverse analysis [@Dragicevic:2019] to analyze the data, where we conduct a repeated-measures ANOVA with four different data-transformation methods:

1. *Log transformation (LOG).* Data are transformed with the logarithmic function. For our data, this is the most appropriate method as we drew samples from a log-normal distribution. 

2. *Aligned rank transformation (ART).* Data are transformed and analyzed with the ARTool [@wobbrock:2011;@elkin:2021].

3. *Pure rank transformation (RNK).* Data are transformed with the original rank transformation [@conover:1981], which does not require any data alignment.

4. *Inverse normal transformation (INT).* The data are transformed by using their normal scores. This rank-based method is simple to implement and has been commonly used in some disciplines. However, it has also received criticism [@Beasley:2009].  

For comparison, we also report the results of the regular parametric ANOVA with no transformation (*PAR*). For each ANOVA analysis, we use a linear mixed-effects model, treating the participant identifier as a random effect. To simplify our analysis and like @elkin:2021, we consider random intercepts but no random slopes. For example, we use the following R code to create the model for the log-transformed response: 

```{r, echo=TRUE, warning=FALSE}
m.log <- lmer(log(Time) ~ Difficulty*Technique + (1|Participant), data = df)
```

The table below presents the *p*-values for the main effects of the two factors and their interaction: 

|        | PAR  | LOG  | ART | RNK | INT |
|--------|------|------|-----|-----|-----|
| Difficulty  | $1.8 \times 10^{-26}$ | $8.1 \times 10^{-47}$  |  $9.0 \times 10^{-43}$ | $4.3 \times 10^{-46}$ | $4.4 \times 10^{-44}$ |
| Technique   | $.10$ | $.18$  | $.00061$ | $.38$ | $.17$ |
| Difficulty $\times$ Technique | $.056$ | $.10$ | $.0017$ | $.24$ | $.23$ |
: *p*-values for main and interaction effects {.sm}

The disparity in findings between ART and the three alternative transformation methods is striking. ART suggests that all three effects are statistically significant. What adds to the intrigue is the fact that ART's *p*-values for *Technique* and its interaction with *Difficulty* are orders of magnitude lower than the *p*-values obtained from all other methods. We will observe similar discrepancies if we conduct contrast tests with the ART procedure [@elkin:2021], though we leave this as an exercise for the reader.

We also examine effect size measures, which are commonly reported in scientific papers. The table below presents results for partial $\eta^2$, which describes the ratio of variance explained by a variable or an interaction: 

|        | PAR  | LOG  | ART | RNK | INT |
|--------|------|------|-----|-----|-----|
| Difficulty  | $.64\ [.55, 1.0]$  | $.83\ [.79, 1.0]$  |  $.80\ [.76, 1.0]$ | $.83\ [.79, 1.0]$ | $.81\ [.77, 1.0]$  |
| Technique   | $.04\ [.00, 1.0]$ | $.03\ [.00, 1.0]$  | $.11\ [.03, 1.0]$ | $.02\ [.00, 1.0]$ | $.03\ [.00, 1.0]$ |
| Difficulty $\times$ Technique | $.10\ [.00, 1.0]$ | $.08\ [.00, 1.0]$  | $.16\ [.04, 1.0]$| $.06\ [.00, 1.0]$| $.06\ [.00, 1.0]$ |
: partial $\eta^2$ and its 95\% confidence interval {.sm}

We observe that ART exaggerates both the effect of *Technique* and its interaction with *Difficulty*. 

### Overview
The above example does not capture a rare phenomenon. We will show that ART's error inflation is systematic for a range of distributions that deviate from normality, both continuous and discrete. We will explain how the problem emerges. We will also see that ART often performs worse than simpler methods that ART is widely considered to repair or improve, such as the pure rank transformation or no transformation at all. 

The paper is structured as follows. [Section 2](#background) introduces nonparametric tests and rank transformations, and explains how ART is constructed. It also provides a summary of previous experimental results regarding the robustness of ART and other related rank-based procedures, along with a survey of recent studies using ART. [Section 3](#interpretation) investigates issues regarding effect interpretation and introduce our distribution simulation approach. [Section 4](#methodology) outlines our experimental methodology, while [Section 5](#findings) presents our findings. [Section 6](#casestudies) revisits results of previous studies employing ART and  illustrates how its application can lead to erroneous conclusions. [Section 7](#recommendations) offers recommendations for researchers. Finally, [Section 8](#conclusions) concludes the paper.

In addition to the main sections of the paper, we provide an [appendix](appendix.html) with results from additional Monte Carlo experiments.