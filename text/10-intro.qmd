## Introduction {#intro}
In Human-Computer Interaction (HCI) and various fields within the behavioral sciences, researchers often gather data through user studies. Such data are typically messy, have small sample sizes, and may violate common statistical assumptions, such as the normality assumption. To address these challenges, researchers commonly employ nonparametric tests, which require fewer assumptions about the data. However, while nonparametric tests for simple one-factorial designs are well-established, researchers face challenges in selecting appropriate methods when dealing with multifactorial designs that require testing for both main effects and interactions. The Aligned Rank Transform or ART [@higgins:1990; @Salter:1993; @wobbrock:2011] addresses this problem by bridging the gap between nonparametric tests and ANOVAs. Its popularity in HCI research has surged, in part due to the ARTool toolkit [@wobbrock:2011; @artool], which makes the method easy to use. @Oppenlaender:2025 identify the paper by @wobbrock:2011 as major milestone in HCI research, ranking third in terms of citations in the ACM CHI Proceedings (1981-2024), behind Braun and Clark's [-@Braun:2006] article on thematic analysis and Hart and Staveland's [-@hart1988development] article on the NASA-TXL index. 

However, given their substantial impact on research outcomes in our community, these widely adopted methods deserve closer scrutiny. As the first author previously argued [@tsandilas:2018], the HCI community lacks the necessary expertise to critically assess the validity of statistical techniques. Our findings suggest that the widespread use of ART largely stems from a lack of awareness of its underlying assumptions. Many researchers treat ART as a general-purpose nonparametric alternative when ANOVA's assumptions are violated, overlooking the fact that ART’s alignment mechanism imposes even stricter assumptions --- including a linear relationship between the response and predictors, equal variances, and continuous data.

Early Monte Carlo experiments in the 1990s [@Salter:1993;@Mansouri:1995] and more recent studies [@elkin:2021] suggested that ART is a robust alternative to ANOVA when normality assumptions are violated. These results have contributed to ART's reputation as a well-established method. However, other research [@luepsen:2017; @luepsen:2018] raised concerns about the robustness of the method, demonstrating that ART fails to control the Type I error rate in many scenarios, such as when data are ordinal or are drawn from skewed distributions. Unfortunately, these warnings have not received sufficient attention, and many authors still rely on Wobbrock et al.'s [-@wobbrock:2011] assertion that *"The ART is for use in circumstances similar to the parametric ANOVA, except that the response variable may be continuous or ordinal, and is not required to be normally distributed."*

This paper aims to clarify the severity of these issues and understand the potential risks of the method. We present the outcomes of a series of Monte Carlo experiments, following a distinctive simulation methodology grounded in latent variable modeling. This approach enables us to simulate effects consistently across a broad spectrum of distributions, both discrete and continuous, and allows us to address issues in interpreting main and interaction effects. To ensure the clarity of our findings and facilitate their reproducibility, we divide the experimental process into multiple smaller experiments, where each experiment focuses on a distinct variable (e.g., sample size, experimental design, variance ratio) or measure (Type I error rate, power, precision of effect size estimates).

Our findings corroborate Lüpsen's alarming conclusions. We provide overwhelming evidence that ART confounds effects and raises Type I error rates at very high levels across a diverse array of non-normal distributions, including skewed, binomial, and ordinal distributions, as well as distributions with unequal variances. These issues persist for both main and interaction effects. Our results further show that simpler rank transformation methods outperform ART, while parametric ANOVA generally poses fewer risks than ART when distributions deviate from normal. Given these new insights, we conclude that ART is not a viable analysis method and advocate for its abandonment. We provide recommendations for alternative analysis methods, while we also raise warnings about the interpretation of interaction effects.  

### Illustrative example 
We will begin with an illustrative example to demonstrate how the aligned rank transform can lead to an increase in false positives and a significant inflation of observed effects. This example will also serve as a brief introduction to the key concepts and methods employed throughout the paper.

Suppose a team of HCI researchers conduct an experiment to compare the performance of three user interface techniques (A, B, and C) that help users complete image editing tasks of four different difficulty levels. The experiment follows a fully balanced $4 \times 3$ repeated-measures factorial design, where each participant (N = 12) performs 12 tasks in a unique order. The researchers measure the time that it takes participants to complete each task. In addition, the participants rate their perceived efficiency completing each task, using an ordinal scale with five levels: (1) *inefficient*, (2) *somewhat inefficient*, (3) *neutral*, (4) *somewhat efficient*, and (5) *efficient*.

The following table presents the experimental results: 

::: {#tbl-example}
```{r, echo=FALSE, warning=FALSE}
df <- read.csv("example-data.csv", sep=",", header=TRUE, strip.white=TRUE)
kbl(df) %>% kable_paper(position = "center") %>% scroll_box(width = "740px", height = "170px")
``` 
Example dataset: Time (in minutes) spent by 12 participants and their perceived efficiency (ordinal scale) for four difficulty levels and three user interface techniques. Scroll down for full results.
:::

The experiment is hypothetical but has similarities with real-world experiments, e.g., see the experiments of @Fruchard:2023. 

**Time performance.** Time measurements have been randomly sampled from a population in which: (1) *Difficulty* has a large main effect; (2) *Technique* has no main effect; and (3) there is no interaction effect between the two factors. To generate time values, we drew samples from a log-normal distribution. The log-normal distribution is often a good fit for real-world measurements that are bounded by zero and have low means but large variance [@Limbert:2001]. Task-completion times are good examples of such measurements [@Sauro:2010]. 

@fig-example presents two boxplots that visually summarize the main effects observed through the experiment. We plot medians to account for the fact that distributions are skewed. We observe that differences in the overall time performance of the three techniques are not visually clear, although the overall median time is somewhat higher for Technique B. In contrast, time performance clearly deteriorates as task difficulty increases. 
We also observe that for the most difficult tasks (Level 4), the median time for Technique C is lower than the median time for Techniques A and B, so we may suspect that *Difficulty* interacts with *Technique*. However, since the spread of observed values is extremely large and the number of data points is small, such differences could result from random noise. 

::: {#fig-example}
```{r, echo=FALSE, warning=FALSE, fig.height=2.7, fig.width=8}
cbPalette <- c("#999999", "#E69F00", "#F15854")

p1 <- (df %>%  group_by(Participant,Technique) %>% summarise(Time = median(Time), .groups="drop") %>%   
        ggplot(aes(x = Technique, y = Time, fill = Technique)) + 
        geom_boxplot(outlier.shape = NA) +
        ylab("Median Time (min)") +
        ylim(0, 3) + 
        geom_jitter(shape=20, position=position_jitter(0.1, 0)) +
        theme_bw() + theme(legend.position = "none") + scale_fill_manual(values=cbPalette))


p2 <- (df %>%  group_by(Participant,Difficulty,Technique) %>% summarise(Time = median(Time), .groups="drop") %>%   
        ggplot(aes(x = Difficulty, y = Time, fill = Technique)) + 
        geom_boxplot(outlier.shape = NA) +
        ylab("Median Time (min)") +
        ylim(0, 6) +
        #geom_jitter(shape=20, position=position_jitter(0.1)) +
        geom_point(size = 0.7, position=position_jitterdodge(0.1)) +
        theme_bw() + theme(legend.position = "none") +
        #scale_fill_brewer()
        scale_fill_manual(values=cbPalette))

grid.arrange(p1, p2, nrow = 1, widths = c(1.5, 2.5))
```
Boxplots summarizing time performance results for our illustrative example. Dots represent the median time for each individual participant. 
:::

We opt for a multiverse analysis [@Dragicevic:2019] to analyze the data, where we conduct a repeated-measures ANOVA with four different data-transformation methods:

1. *Log transformation (LOG).* Data are transformed with the logarithmic function. For our data, this is the most appropriate method as we drew samples from a log-normal distribution. 

2. *Aligned rank transformation (ART).* Data are transformed and analyzed with the ARTool [@wobbrock:2011;@elkin:2021].

3. *Pure rank transformation (RNK).* Data are transformed with the original rank transformation [@conover:1981], which does not perform any data alignment.

4. *Inverse normal transformation (INT).* The data are transformed by using their normal scores. This rank-based method is simple to implement and has been commonly used in some disciplines. However, it has also received criticism [@Beasley:2009].  

For comparison, we also report the results of the regular parametric ANOVA with no transformation (*PAR*). For each ANOVA analysis, we use a linear mixed-effects model, treating the participant identifier as a random effect. To simplify our analysis and like @elkin:2021, we consider random intercepts but no random slopes. For example, we use the following R code to create the model for the log-transformed response: 

```{r, echo=TRUE, warning=FALSE}
m.log <- lmer(log(Time) ~ Difficulty*Technique + (1|Participant), data = df)
```

The table below presents the *p*-values for the main effects of the two factors and their interaction: 

|        | PAR  | LOG  | ART | RNK | INT |
|--------|------|------|-----|-----|-----|
| Difficulty  | $1.8 \times 10^{-26}$ | $8.1 \times 10^{-47}$  |  $9.0 \times 10^{-43}$ | $4.3 \times 10^{-46}$ | $4.4 \times 10^{-44}$ |
| Technique   | $.10$ | $.18$  | $.00061$ | $.38$ | $.17$ |
| Difficulty $\times$ Technique | $.056$ | $.10$ | $.0017$ | $.24$ | $.23$ |
: *p*-values for main and interaction effects {.sm}

The disparity in findings between ART and the three alternative transformation methods is striking. ART suggests that all three effects are statistically significant. What adds to the intrigue is the fact that ART's *p*-values for *Technique* and its interaction with *Difficulty* are orders of magnitude lower than the *p*-values obtained from all other methods. We will observe similar discrepancies if we conduct contrast tests with the ART procedure [@elkin:2021], though we leave this as an exercise for the reader.

We also examine effect size measures, which are commonly reported in scientific papers. The table below presents results for partial $\eta^2$, which describes the ratio of variance explained by a variable or an interaction: 

|        | PAR  | LOG  | ART | RNK | INT |
|--------|------|------|-----|-----|-----|
| Difficulty  | $.64\ [.55, 1.0]$  | $.83\ [.79, 1.0]$  |  $.80\ [.76, 1.0]$ | $.83\ [.79, 1.0]$ | $.81\ [.77, 1.0]$  |
| Technique   | $.04\ [.00, 1.0]$ | $.03\ [.00, 1.0]$  | $.11\ [.03, 1.0]$ | $.02\ [.00, 1.0]$ | $.03\ [.00, 1.0]$ |
| Difficulty $\times$ Technique | $.10\ [.00, 1.0]$ | $.08\ [.00, 1.0]$  | $.16\ [.04, 1.0]$| $.06\ [.00, 1.0]$| $.06\ [.00, 1.0]$ |
: partial $\eta^2$ and its 95\% confidence interval {.sm}

We observe that ART exaggerates both the effect of *Technique* and its interaction with *Difficulty*. This example demonstrates a more general problem with ART’s alignment mechanism under long-tailed distributions. ART tends to confound effects: a strong effect on a single factor often leads the method to detect spurious effects on other factors or to identify non-existent interactions.

**Perceived efficiency.**  The perceived efficiency ratings were randomly sampled from a population with no main effect on any of the two factors and no interaction effect. @fig-example-ordinal shows that participants' ratings are concentrated at the higher end of the scale but with no clear trends. 

::: {#fig-example-ordinal}
```{r, echo=FALSE, warning=FALSE, fig.height=2.7, fig.width=8}
df$Perceived_Efficiency <- ordered(df$Perceived_Efficiency, levels = c("inefficient", "somewhat inefficient", "neutral", "somewhat efficient", "efficient"))

p3 <- (df %>%  group_by(Participant,Technique) %>% summarise(Perceived_Efficiency = mean(as.integer(Perceived_Efficiency)), .groups="drop") %>%   
        ggplot(aes(x = Technique, y = Perceived_Efficiency, fill = Technique)) + 
        geom_boxplot(outlier.shape = NA) +
        ylab("Perceived Efficiency") +
        scale_y_continuous(breaks = 1:5, limits = c(1,5), labels=levels(df$Perceived_Efficiency)) +
        geom_jitter(shape=1, position=position_jitter(0.2,0), size = 2) +
        theme_bw() + theme(legend.position = "none")) + scale_fill_manual(values=cbPalette) +  
        theme(axis.text.y=element_text(angle=50, hjust=0.7))

#diffPalette <- c("#f9f9f9", "#e2e2e2", "#b9b9b9", "#999999")
diffPalette <- c("#f9f9fd", "#e2e6fa", "#bacafb", "#99b9f9")
p4 <- (df %>%  group_by(Participant,Difficulty) %>% summarise(Perceived_Efficiency = mean(as.integer(Perceived_Efficiency)), .groups="drop") %>%   
        ggplot(aes(x = Difficulty, y = Perceived_Efficiency, fill = Difficulty)) + 
        geom_boxplot(outlier.shape = NA) + ylim(1,5)+
        geom_jitter(shape=1, position=position_jitter(0.2,0), size = 2) +
        theme_bw() + theme(legend.position = "none")) + scale_fill_manual(values=diffPalette) +  
        theme(axis.title.y=element_blank(), axis.text.y=element_blank())

grid.arrange(p3, p4, nrow = 1, widths = c(3, 3))
```
Boxplots summarizing the results on efficiency as perceived by participants. Circles represent the mean rating of each individual participant. 
:::

For our analysis, we compare the results of PAR, ART, RNK, and INT, since LOG is not relevant for this type of data. We also report the results of the Friedman test [@Friedman] for each factor, where we first aggregate the data by taking the mean over the other factor. Results are as follows:  

|        | PAR  | ART | RNK | INT | Friedman Test |
|--------|------|-----|-----|-----|-----|
| Difficulty  | $.93$ | $.0011$  |  $.86$ | $.88$ | $.83$ |
| Technique   | $.18$ | $.00016$  | $.32$ | $.23$ | $.24$ |
| Difficulty $\times$ Technique | $.27$ | $.10$ | $.33$ | $.34$ | |
: *p*-values for main and interaction effects {.sm}

ART's *p*-values for the two main effects are surprisingly low, while no other method shows any substantial evidence for such effects. We also present the methods' effect size estimates:

|        | PAR  | ART | RNK | INT |
|--------|------|-----|-----|-----|
| Difficulty  | $.004\ [.00, 1.0]$  | $.12\ [.03, 1.0]$  |  $.006\ [.00, 1.0]$ | $.005\ [.00, 1.0]$ |
| Technique   | $.03\ [.00, 1.0]$ | $.13\ [.05, 1.0]$  | $.02\ [.00, 1.0]$ | $.02\ [.00, 1.0]$ |
| Difficulty $\times$ Technique | $.06\ [.00, 1.0]$ | $.04\ [.00, 1.0]$  | $.05\ [.00, 1.0]$| $.05\ [.00, 1.0]$|
: partial $\eta^2$ and its 95\% confidence interval {.sm}

The discrepancies between ART's estimates for the two main effects and those of the other methods are large. As we demonstrate in this paper, such discrepancies stem from ART's inability to handle discrete distributions. The issue becomes more pronounced with larger sample sizes. @luepsen:2017 previously warned researchers about this problem, but his warnings have largely been ignored.

### Overview
The above example does not reflect a rare phenomenon. We will show that ART's error inflation is systematic for a range of distributions that deviate from normality, both continuous and discrete.

The paper is structured as follows. [Section 2](#background) introduces nonparametric tests and rank transformations, and explains how ART is constructed. It also provides a summary of previous experimental results regarding the robustness of ART and other related rank-based procedures, along with a survey of recent studies using ART. [Section 3](#assumptions) clarifies the method's distributional assumptions. [Section 4](#interpretation) investigates issues regarding effect interpretation and introduce our distribution simulation approach. [Section 5](#methodology) outlines our experimental methodology, while [Section 6](#findings) presents our findings. [Section 7](#casestudies) revisits results of previous studies employing ART and  illustrates how its application can lead to erroneous conclusions. [Section 8](#recommendations) offers recommendations for researchers. Finally, [Section 9](#conclusions) concludes the paper.

In addition to the main sections of the paper, we provide an [appendix](appendix.html) with results from additional Monte Carlo experiments.
