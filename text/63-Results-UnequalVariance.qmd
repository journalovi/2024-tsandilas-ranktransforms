### Type I errors under unequal variances
Many statistical procedures assume equal variances among all levels of each independent variable, or more strictly, among all possible pairs of independent variable levels (known as sphericity assumption in repeated-measures ANOVA). Nonparametric tests are often mistakenly considered to be free of such as assumptions, but as we discussed earlier, this is generally incorrect.

Our fourth experiment evaluates the behavior of the four methods when populations on the latent variable $Y$ have unequal variances. We examine three 2-factor designs: (i) a $4 \times 3$ within-subjects design; (ii) a $2 \times 3$ between-subjects design; and (iii) a $2 \times 4$ mixed design. We set all effects to zero and then vary the ratio $r_{sd}$ of standard deviations between the levels of the first factor $X_1$. @fig-unequal shows the five ratios that we test when $X_1$ has two levels. When $X_1$ has additional levels ($4 \times 3$ design), their standard deviation is randomly drawn in a range between the standard deviations of the first two levels. 

::: {#fig-unequal}
```{r, echo=FALSE, message=FALSE, fig.height=1.5, fig.width = 9, warning=FALSE}
plotVariances()
```
Varying the ratio $r_{sd}$ of standard deviations between the levels of $X_1$. The plots show examples of population distributions for factors with two categorical levels.    
:::

**Main effects**. We begin by investigating how the four methods detect main effects on $X_1$. The interpretation of results in this scenario is challenging due to differences in the original populations â€” although they share the same means (and medians), their variances differ. Depending on the null hypothesis of interest, conclusions may vary. Complicating matters further, non-linear transformations can result in distributions with differing means and medians. Consequently, the choice of statistical method may lead to different outcomes; one method may be sensitive to mean differences while others to median differences.

@fig-unequal-main-1 presents the percentage of times each method rejects the null hypothesis ($\%$ positives) at a significance level of $\alpha = .05$. If the null hypothesis of interest posits that *"$X_1$ has no effect on the mean of the latent variable $Y$,"* then these instances should be considered as Type I errors. Under normal distributions, all methods either moderately inflate or deflate the positives ratio as $r_{sd}$ increases. For the $4 \times 3$ within-subjects design ($r_{sd} \ge 2.5$), the ratio for PAR, RNK, and ART reaches levels of around $7-8\%$, while for INT, it drops below $3\%$ under the $2 \times 4$ mixed design ($r_{sd} \ge 2.5$).

::: {#fig-unequal-main-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "4_test_Unequal_Variances"
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readData(prefix, n = 20, alpha, effectType = 1, distributions)
df <- reshapeByDesign(df, dnames, effectvars = c("sd_ratio", "effectX1","effectX2","effectX1X2"))
plotlyErrorByDesign2(df, xlab = "max ratio of variances between levels of X1", var = "rateX1", xvar = "sd_ratio", max = 82, ytitle = 'Positives (%)')
```
The percentage of positives (i.e., rejecting the null hypothesis) for the **main effect of $X_1$** as the ratio of standard deviations $r_{sd}$ on $X_1$ increases ($n=20$). Depending on the hypothesis of interest, these percentages can be interpreted as Type I error rates ($\alpha = .05$).
:::

The performance of RNK and INT remains unchanged across continuous distributions, a result expected due to the preservation of medians in such scales. Conversely, mean differences grow with increasing $r_{sd}$, which explains why PAR's ratio of positives reaches high levels under these distributions. However, this trend is not consistent across all three designs. The behavior of ART is less clear, further supporting our argument that its null hypothesis of interest is ill-defined. Under discrete distributions, the trends of positives vary across designs, with PAR and ART exhibiting the highest rates. 

We also investigate the influence of unequal variances among the levels of $X_1$ on positive rates for $X_2$, as depicted in @fig-unequal-main-2. Those can be reliably considered as Type I errors because source populations defined by $X_2$ are identical. The error rates of PAR, RNK, and INT do not seem to be affected by an increase of $r_{sd}$. In contrast, ART yields notably high error rates, surpassing acceptable levels even in the case of normal distributions (check results for the $2 \times 4$ mixed design). The observed error rates are comparatively lower for the $2 \times 3$ between-subjects design.

::: {#fig-unequal-main-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotlyErrorByDesign2(df, xlab = "max ratio of variances between levels of X1", var = "rateX2", xvar = "sd_ratio", max = 52)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as the ratio of standard deviations $r_{sd}$ on $X_1$ increases ($n=20$)
:::

**Interaction effects**. Finally, we measure Type I error rates for the interaction effect $X_1 \times X_2$, shown in @fig-unequal-interaction. The error rates of ART exhibit similar trends, with a somewhat lower level under the $4 \times 3$ within-subjects design (compared to @fig-unequal-main-2). Additionally, for this design, we observe error inflation for the three other methods, albeit to a significantly lesser degree.

::: {#fig-unequal-interaction}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotlyErrorByDesign2(df, xlab = "max ratio of variances between levels of X1", var = "rateX1X2", xvar = "sd_ratio", max = 52)
```
Type I error rates ($\alpha = .05$) for the **interaction effect $X_1 \times X_2$** as the ratio of standard deviations $r_{sd}$ on $X_1$ increases ($n=20$)
:::
