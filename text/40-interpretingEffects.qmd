## Defining the null hypothesis of interest {#interpretation}
Before comparing different statistical methods, it is essential to assess whether they are actually comparable. If two methods are not designed to test the same null hypothesis, then direct comparisons between them can be misleading. In what follows, we clarify the interpretation of main and interaction effects and explain how we address potential issues in our analysis.

### Interpreting main effects
The traditional ANOVA is used to test differences between two or more means. However, nonparametric tests often target other population parameters. For example, the Wilcoxon sign-rank test is commonly described as a test of medians for paired samples [@McDonald:2014] and is used when population means are not of interest, e.g., when population distributions are skewed. The Mann-Whitney U and the Kruskal–Wallis tests are used, instead, to assess whether two or more independent samples come from the same population, or more technically, whether the mean ranks of the groups are the same. They can be only interpreted as tests of medians under the strict assumption that the population distributions of all groups have identical shapes and scales [@Divine:2018]. 

Defining the null hypothesis of interest of a rank transformation is more challenging. @conover:1981 show that the simple rank transformation procedure (RNK) is equivalent to the Mann-Whitney U and Kruskal–Wallis tests for independent samples. For paired samples, however, it results in a new test, which is different from the Wilcoxon sign-rank test and the Friedman test. Defining the null hypothesis of interest of ART is even more challenging because of the hybrid nature of the method. In particular, while ART is a rank-based transformation procedure, it aligns data with respect to means, where alignment is performed independently for each group.

**Dealing with interpretation issues.** To avoid such interpretation issues, we focus --- unless otherwise stated --- on effects that apply monotonic transformations to population distributions. This also ensures a monotonic relationship between different measures of central tendency such as medians and means (with the exception of the Cauchy distribution, where the mean is undefined). In other words, if a treatment increases the population mean, it will also increase the population median. We present an example in @fig-distributions. The figure shows two population distributions corresponding to the two intermediate levels of difficulty of our illustrative example (see @fig-example). We observe that the increased difficulty of the task translates both the population mean and the median to the right. In this case, we expect a statistical test to reject the null hypothesis, no matter whether it tests the population mean, the median, or the overall distribution shape.   

::: {#fig-distributions}
```{r, echo=FALSE, message=FALSE, fig.height=3, fig.width = 9, warning=FALSE}
library(plotly)

palette <- c("#E69F00", "#009E73")

logsd <- 0.5
logm1 <- -0.3
logm2 <- 0.3

xs <- seq(0, 10, length.out = 1000)
y1s <- dlnorm(xs, logm1, logsd)
y2s <- dlnorm(xs, logm2, logsd)

m1 <- exp(logm1 + (logsd^2)/2)
m2 <- exp(logm2 + (logsd^2)/2)
med1 <- exp(logm1)
med2 <- exp(logm2)

anot_m1 <- list(x = m1, y = 0, text = "mean", xref = "x", yref = "y",
  showarrow = TRUE, arrowhead = 6, arrowsize = 1, arrowcolor = palette[1],
  ax = 0, ay = -40
)

anot_med1 <- list(x = med1, y = 0, text = "median", xref = "x", yref = "y", 
  showarrow = TRUE, arrowhead = 7, arrowsize = 1, arrowcolor = palette[1],
  ax = 0, ay = 30
)

anot_m2 <- list(x = m2, y = 0, text = "mean", xref = "x", yref = "y",
  showarrow = TRUE, arrowhead = 6, arrowsize = 1, arrowcolor = palette[2],
  ax = 0, ay = -43
)

anot_med2 <- list(x = med2, y = 0, text = "median", xref = "x", yref = "y", 
  showarrow = TRUE, arrowhead = 7, arrowsize = 1, arrowcolor = palette[2],
  ax = 0, ay = 30
)

annot1 <- paste("Log-normal: meanlog = ", logm1, ", sdlog = ", logsd)
annot2 <- paste("Log-normal: meanlog = ", logm2, ", sdlog = ", logsd)

fig <- plot_ly() %>%  
      add_lines(x = xs, y = y1s, visible = TRUE, type = 'scatter', mode = 'lines', text = annot1, hoverinfo = 'text', line=list(color=palette[1])) %>%
      add_lines(x = xs, y = y2s, visible = TRUE, type = 'scatter', mode = 'lines', text = annot2, hoverinfo = 'text', line=list(color=palette[2])) %>%      
      add_annotations(x = med1, y = max(y1s), text = "Difficulty Level 2", xref = "x", yref = "y", showarrow = F, font = list(color = palette[1]), xanchor = 'left') %>% 
      add_annotations(x = med2, y = max(y2s), text = "Difficulty Level 3", xref = "x", yref = "y", showarrow = F, font = list(color = palette[2]), xanchor = 'left') %>% 
      layout(annotations = anot_m1) %>% layout(annotations = anot_med1) %>%      
      layout(annotations = anot_m2) %>% layout(annotations = anot_med2) %>%    
      layout(
        showlegend = FALSE,
        xaxis = list(title = "Time (min)", range = c(0, 4.5), showgrid = F, showticks = T, ticks="outside", zeroline = F), 
        yaxis = list(showgrid = F, showticklabels = F, showline = F, range=c(0, max(y1s) + .05), fixedrange=T)
      ) %>% 
	      config(displayModeBar = TRUE, scrollZoom = FALSE, displaylogo = FALSE, modeBarButtonsToRemove = c("lasso2d", "select2d",  "zoomIn2d", "zoomOut2d", "autoscale", "hoverclosest", 'hoverCompare'))  %>% layout(dragmode='pan')

fig

```
Time distributions for two task populations with difficulty levels 2 and 3 (see @fig-example). 
:::

### Interpreting interaction effects
The ART procedure was proposed as an alternative to the rank transformation [@conover:1981] for testing interactions. As @higgins:1990 explained, the rank transformation is non-linear and, as a result, it changes the structure of interactions. Therefore, *"interaction may exist in the transformed data but not in the original data, or vice versa"* [@higgins:1990]. @fig-interactions-rank demonstrates the problem. In this example, the data have been sampled from perfectly normal distributions with equal variances. We observe that while no interaction effect appears in the original data (lines are parallel), the rank transformation deforms the trend. In particular, differences are more pronounced for the middle points of the three-level factor ("medium difficulty"). The figure also shows that the inverse normal transformation also deforms the interaction but to a lesser extent. Note that the problem emerges when the main effect is strong on all interacting factors. 

::: {#fig-interactions-rank}
```{r, echo=FALSE, message=FALSE, fig.height=3, fig.width = 7, warning=FALSE}
source("interactions_plot.R")
library("ARTool")

#library(plotly)

INT <- function(x){
	qnorm((rank(x) - 0.5)/length(x))
}

df <- read.csv("interactions_rank.csv", sep=",", header=TRUE, strip.white=TRUE)
df$Difficulty <- ordered(df$Difficulty, levels = c("easy", "medium", "hard"))

df_aggr <- aggregate(Time ~ Difficulty+Technique, data = df, mean)
df_rank <- aggregate(rank(Time) ~ Difficulty+Technique, data = df, mean)
df_int <- aggregate(INT(Time) ~ Difficulty+Technique, data = df, mean)
colnames(df_rank) <- c("Difficulty", "Technique", "Time")
colnames(df_int) <- c("Difficulty", "Technique", "Time")

fig1 <- createRankInteractionPlot(df_aggr) %>% layout( yaxis = list(hoverformat = '.2f'))
fig2 <- createRankInteractionPlot(df_rank, rnkscale = TRUE) %>% layout( yaxis = list(hoverformat = '.2f'))
fig3 <- createRankInteractionPlot(df_int) %>% layout( yaxis = list(title = 'INT(Time)', range = c(-2.1,2.1), hoverformat = '.2f'))

fig <- subplot(fig1, fig2, fig3, titleY = TRUE, titleX = TRUE, margin = 0.05, widths = c(0.32, 0.32, 0.32)) %>% 
	        layout(margin = list(l = 40, r = 0, b = 0, t = 50, pad = 0)) %>% layout(hovermode = 'x', dragmode='pan')

fig
```
Visualization of interaction effect for a 3 $\times$ 2 experimental design before and after applying the rank transformation and the inverse normal transformation (INT) on an [example dataset](interactions_rank.csv) (within-participants design, $n = 20$). All data points represent means.
:::

ART aims to correct this problem. However, non-linear transformations come into place in various ways in experimental designs [@Loftus:1978; @Wagenmakers:2012]. They can deform distributions, making the interpretation of observed effects especially challenging. Before presenting our experimental method, we discuss these problems and explain how our approach takes them into consideration. 

**Removable interactions.** Let us take a different [dataset](removable_interactions.csv) from a fictional experiment (within-participants design with $n = 24$) that evaluates the performance of two techniques (*Tech A* and *Tech B*) under two task difficulty levels (*easy* vs. *hard*). The experiment, for example, could test a mobile typing task, where the levels of difficulty correspond to texts of different lengths (*short* vs. *long*) under two typing techniques (*with* vs. *without auto-completion*). We assume that the researchers measure two dependent variables: task-completion time and perceived performance, which is measured through a five-level ordinal scale (from "very quick" to "very slow"). In this example, the main effects of task difficulty and technique are large. It is less clear, however, whether there is also an interaction between the two factors. 

@fig-interactions visualizes the means for each combination of the levels of the factors and highlights the possible interactions. Let us first concentrate on the first two plots that present results for the time measure. The trends in the left plot indicate an interaction effect, since the two lines seem to diverge as the task difficulty increases. 

::: {#fig-interactions}
```{r, echo=FALSE, message=FALSE, fig.height=3.5, fig.width = 9, warning=FALSE}
df <- read.csv("removable_interactions.csv", sep=",", header=TRUE, strip.white=TRUE)
dftime <- aggregate(Time ~ Difficulty+Technique, data = df, mean)
dfpref <- aggregate(PerceivedPerformance ~ Difficulty+Technique, data = df, mean)

fig1 <- createInteractionPlot(dftime) %>% layout( yaxis = list(hoverformat = '.2f'))
fig2 <- createInteractionPlot(dftime, logscale = TRUE) %>% layout( yaxis = list(hoverformat = '.2f'))
fig3 <- createInteractionPlot(dfpref, likert = TRUE) %>% layout( yaxis = list(hoverformat = '.1f'))
#fig <- subplot(fig1, fig2, fig3, titleY = TRUE, titleX = TRUE, margin = 0.08, widths = c(0.32, 0.36, 0.32))

fig <- subplot(fig1, fig2, titleY = TRUE, titleX = TRUE, margin = 0.06) %>% subplot(fig3, titleY = TRUE, titleX = TRUE, margin = 0.07, widths = c(0.66, 0.33)) %>% layout(hovermode = 'x', dragmode='pan')

fig
```
The line charts visualize the effects of task difficulty (*easy* vs. *hard*) and technique (*Tech* A vs. *Tech B*) for two measures: task completion time (left and middle) and perceived performance (right). All data points represent group means. 
:::

But how meaningful is this interpretation of interaction? As we dicussed earlier, time measurements are often taken from skewed distributions where the variance is not constant. Therefore, large effects are harder to observe in quick tasks than in slow ones. However, such trends do not necessarily reveal any real interactions, because they are simply due to observations at different time scales. @fig-interactions (middle) visualizes the effects using a logarithmic scale. Notice that the lines in the plot are now almost parallel, suggesting no interaction effect.

The concept of *removable* or *uninterpretable* interactions, that is, interactions that disappear after applying a monotonic non-linear transformation, was introduced by @Loftus:1978.  Over three decades later, @Wagenmakers:2012 revisited this work and found that psychology researchers are largely unaware of the concept, drawing incorrect conclusions about psychological effects on the basis of meaningless interactions. 

This issue also extends to data collected from questionnaires. The right plot in @fig-interactions shows results for perceived performance. Again, the line trends suggest an interaction effect. Unfortunately, the scale is ordinal, which means that distances between the five levels of the scale may not be perceived as equal by people. Furthermore, the scale is bounded, so the reason that the two lines are not parallel might be simply due to the absence of additional levels beyond the extreme "very slow" ranking. Concluding that there is a meaningful interaction here could be incorrect. @Liddell:2018 extensively discuss how ordinal scales deform interactions.

**Formal testing.** We now formally test the above interactions by using ANOVA with different transformation methods. Below, we present the *p*-value returned by each method for task-completion time: 

| PAR  | LOG  | ART | RNK | INT |
|------|------|-----|-----|-----|
| $.023$ | $.67$ | $.00073$ | $.66$ | $.67$ |
: *p*-values for interaction effect on task-completion time {.sm}

We observe that *RNK* and *INT* lead to *p*-values very close to the *p*-value of *LOG*, which suggests a similar interpretation of interaction effects. In contrast, ART returns a very low *p*-value (lower than the *p*-value of the regular ANOVA), indicating a different interpretation.  

We also test the interaction effect on the ordinal dependent variable:

| PAR  | ART | RNK | INT | ATS |
|------|-----|-----|-----|-----|
| $.0020$ | $.00075$ | $.0067$ | $.0037$ | $.0081$ |
: *p*-values for interaction effect on perceived performance {.sm}

Notice that we omit the log-transformation method (*LOG*), as it is not relevant in this context. Instead, we conduct an analysis with the nonparametric ATS method [@Brunner_ATS:2001], as implemented in the R package *nparLD* [@nparLD]. All *p*-values are low, suggesting that an interaction effect exists. However, if we conduct a more appropriate analysis using an ordered probit model [@Burkner:2019;@Christensen2023ordinal], we find no supportive evidence for such an effect (check our analysis in the supplementary material). 

In conclusion, focusing solely on the issues with the rank tranformation illustrated in @fig-interactions-rank is akin to missing the forest for the trees. When parallel main effects are present, interpreting interactions can be challenging for all methods --- and ART offers no solution to this problem. 

### Disagreement on the interpretation of effects in GLMs
We further discuss interpetation issues in the broader context of generalized linear models (GLMs).

GLMs are defined by a link function $g$ that connects a variable --- linearly defined by the predictors --- to a response variable $Y$, which follows an arbitrary distribution. For an experimental design with two factors, $x_1$ and $x_2$, the expected value (or mean) of $Y$ conditional on $x_1$ and $x_2$ is expressed as follows:

$$
E(Y|x_1, x_2) = g^{-1}(a_0 + a_1 x_1 + a_2 x_2 + a_{12}x_1 x_2)
$${#eq-glm}

where $g^{-1}$ is the inverse of the link function.

Statistical procedures based on GLMs define interaction in terms of the interaction coefficient $a_{12}$ in the linear component of the model. That is, the null hypothesis should be rejected when the coefficient is non-zero. However, there is no consensus among researchers on this approach. We clarify the debate and explain how we address it.

**Defining interactions on the scale of the response variable.** @Ai:2003 and, more recently, @McCabe:2022 show that the interaction coefficient alone is not sufficient to describe the actual interaction trend on the natural scale of the response variable. They argue that an interaction effect should instead be defined as the observed change in the marginal effect of $x_1$ as a function of $x_2$. Thus, the trends shown in @fig-interactions do represent interactions, since the difference between *Tech* A and *Tech B* changes as a function of *Difficulty* on the original reponse scale (time or perceived performance). 

Using this definition, the authors show that a zero interaction coefficient ($a_{12} = 0$) does not necessarily imply the absence of an interaction effect. In fact, for continuous predictors, they demonstrate that when $a_{12} = 0$, the interaction effect can still emerge and is proportional to $a_1$ and $a_2$.

<!--
For continuous predictors, this can be formalized as the second-order cross partial derivative of the expected value (which is $\frac{\partial^2 E(Y|x_1, x_2)}{\partial x_1 \partial x_2}$), while for discrete predictors, it can be expressed using discrete differences between levels of the predictors. -->

**Responding to @Ai:2003.** @Greene:2010 responds that the interpretation of @Ai:2003 *“produces generally uninformative and sometimes contradictory and misleading results”* and instead advocates for using the model’s coefficients as the basis for inference:

> *“Statistical testing about the model specification is done at this step. Hypothesis tests are about model coefficients and about the structural aspects of the model specifications. Partial effects are neither coefficients nor elements of the specification of the model. They are implications of the specified and estimated model”* [@Greene:2010, Page 295].

Our position is fully aligned with Greene's argument. As our earlier example (see @fig-interactions) demonstrates, interactions assessed on the scale of the response variable can be misleading about the true underlying effects that generated the data. How meaningful is it to declare interaction effects that arise solely from parallel main effects and lack any theoretical interpretation? It is worth noting that @Ai:2003, as well as @McCabe:2022, overlook Loftus' [-@Loftus:1978] critique of uninterpretable interactions. 

**When the interpretation of effects becomes ambiguous.** Issues with interpreting interaction effects arise only when there are parallel main effects --- that is, when the coefficients of all interacting factors are non-zero. Interpretation issues may also extend to main effects when the interaction coefficient $a_{12}$ is non-zero. In such cases, we may observe a change in the marginal effect of a factor (e.g., $x_1$), even if its associated model coefficient (e.g., $a_1$) is zero. We discuss such scenarios in more depth in the [appendix](appendix.html#removable-main).

By contrast, interpretation is unambiguous in these situations:

1. *Main effects*: when the interaction coefficient $a_{12}$ is zero.

2. *Interaction effect*: when either $a_{1}$ or $a_{2}$ is zero.    

For example, the divergence in ART's results in our illustrative example (see @fig-example) cannot be attributed to interpretation issues, since the sample was drawn from a population with no effect of *Technique* ($a_{1} = 0$) and no interaction ($a_{12}=0$). 

Throughout the remainder of the article, we make every effort to clarify when the interpretation of an effect is ambiguous. Because different methods may rely on different assumptions about the null hypothesis, we analyze such situations separately. However, it is important to note that the existing literature offers no guidance on how ART is expected to interpret main and interaction effects under these conditions. Early evaluations of ART did not consider nonlinear models, and therefore this issue was not raised. While @elkin:2021 do examine such models, they do not address interpretation issues.

### Our approach {#approach}
Responses in a generalized linear model can follow arbitrary distributions. A key challenge is how to simulate such distributions while maintaining control over the parameters of the linear submodel. We address this by adopting a latent variable modeling approach, in which a normally distributed latent variable is transformed into the observed response variable.

**Latent variables.** We assume there is a single latent variable $\mathcal{Y}$ that is distinct from the variable $Y$ we observe. For example, the latent variable may represent a population's performance potential, working memory capacity, the perceived utility of a new technology, or quality of life. For convenience, we assume that this latent variable is linear, continuous, and normally distributed. This is a common assumption in latent variable modeling, e.g., in diffusion-decision models that predict response time and error in two-choice decision tasks [@Ratcliff_diffusion:2008], and in ordinal models [@Liddell:2018]. This assumption allows us to define null hypotheses within a shared, normalized space, regardless of the scale of the observed data.

**Observed variables.** Then, the response variable $Y$ is derived from this latent variable through a monotonic transformation, thus $Y = \mathcal{T}(\mathcal{Y})$, where $\mathcal{T}(y_1) \le \mathcal{T}(y_2)$ if and only if $y_1 \le y_2$. A transformation for example occurs when study participants perform a selection task or repond to a Likert-scale item through a questionnaire. @fig-conversions shows how we transform normal distributions to log-normal, binomial, and ordinal scales.

::: {#fig-conversions}
```{r, echo=FALSE, message=FALSE, fig.height=3, fig.width = 9, warning=FALSE}
source("conversions_plot.R")

n <- 2000

xs <- c(rnorm(n, mean = 1.5, sd = 1), rnorm(n, mean = -1.5, sd = 1))

ys1 <- norm2lnorm(xs, meanlog = 0, sdlog = 1, mu = 0, sd = sd(xs))
plot1 <- plotConversion(xs, ys1, ylab = "Log-normal scale", ymax = 10)

ys2 <- norm2binom(xs, size = 10, prob=.1)
plot2 <- plotConversion(xs, ys2, ymin = - 0.3, ylab = "Binomial scale", as.density = F)

ys3 <- toOrdinal(xs, thresholds = c(-2.5, -1, 0, 2.5))
plot3 <- plotConversion(xs, ys3, ymin = 0.7, ymax = 5.3, ylab = "Ordinal scale", as.density = F)

fig <- cowplot::plot_grid(plot1, plot2, plot3, ncol= 3) 

fig
```
Transformation of normal latent variables to other continuous or discrete scales: log-normal (left), binomial (middle), and ordinal (right).  
:::

To transform the latent variable to a ratio scale (e.g., a log-normal and binomial scale), we adopt the distribution conversion approach of *faux* v1.2.1 [@faux], an R package for experimental simulations. We first derive the cumulative density distribution of the latent variable. We then use the inverse quantile function of the target distribution to derive the observed variable. For example, in @fig-conversions (left), where we transform the latent variable to a log-normal scale with parameters $\mu = 0$ and $\sigma = 1$, we use the following R function:  

```{r}
norm2lnorm <- function(x, meanlog = 0, sdlog = 1, mu = mean(x), sd = sd(x), ...) {
	 p <- pnorm(x, mu, sd) 
 	 qlnorm(p, meanlog, sdlog, ...) 
}
```
For the binomial scale of @fig-conversions (left), we use instead the inverse quantile function of the binomial distribution ```qbinom(p,size,prob)``` with parameters ```size = 10``` and ```prob = .1```, which respectively represent the number of Bernoulli trials and their success probability.

To transform the latent variable to an ordinal scale, we implement an ordered-probit model, as explained by @Liddell:2018. According to this model, we discretize the latent variable with thresholds that determine the ordinal levels of interest. For our example in @fig-conversions (right), we use as threshold the values $(-2.5, -1, 0, 2.5)$, defining an ordinal scale of five levels. Observe that these thresholds are not equidistant. 

**Interpreting effects.** Our approach allows us to simulate main and interaction effects on the latent variable $\mathcal{Y}$ and observe how these effects manifest on the transformed variable $Y = \mathcal{T}(\mathcal{Y})$. 

As discussed earlier, we make clear distinction between the following two scenarios: 

1. The definition of the null hypothesis remains the same regardless of whether we evaluate effects on the latent variable $\mathcal{Y}$ or the response variable $Y$. 

2. The definition of the null hypothesis changes with the scale: a null effect on the latent variable $\mathcal{Y}$ may not remain null when evaluated on the response variable $Y$. 

Unless we explicitely mention interpretation issues, we focus on the first scenario. Note that in the case of discrete response variables, the function $\mathcal{T}$ is monotonic but not *strictly* monotonic, since it involves a loss of information. Consequently, an effect on the latent variable may not be observable in the responses, or the observed magnitude of the effect may appear reduced.
