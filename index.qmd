---
title: "The illusory promise of the Aligned Rank Transform"
author: 
  - name: Theophanis Tsandilas
    orcid: 0000-0002-0158-228X
    email: theophanis.tsandilas@inria.fr
    affiliations:
      - name: Université Paris-Saclay, CNRS, Inria, LISN
        country: France
  - name: Géry Casiez
    orcid: 0000-0003-1905-815X
    email: gery.casiezuniv-lille.fr
    affiliations:
      - name: Université de Lille, CNRS, Inria, CRIStAL
        country: France
bibliography: bibliography.bib

tbl-cap-location: top
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Useful libraries
library(crosstalk)
library(kableExtra)
library(gridExtra)
library(lmerTest)
library(tidyverse)
library(plotly)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Source code for reading data from experimental results and for plotting
source("dataReaders.R")
source("plotting.R")
```

```{=html}
<style>
.math.inline .MathJax  {
  font-size: 105% !important;
}

.g::before {
  content: "[G: ";
}

.g::after {
  content: "]";
}
.g {
  color: #F28C28;
}

.f {
  color: #d62b1c;
}
</style>
```

::: {.callout-note appearance="simple" icon=false collapse=true}
## Abstract {.unnumbered}

###### Introduction
The research question and a succinct motivation for answering it. 

###### Background
Background research. 

:::


::: {.callout-note appearance="simple" icon=false collapse=true}
## Materials, Authorship, License, Conflicts

Fill in the following sections in this infobox per the [JoVI Author Guide](https://www.journalovi.org/author-guide.html#sections), then delete this line.

###### Research materials

See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#research-material-statements).

###### Authorship

See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#authorship).

###### License

See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#license).

###### Conflicts of interest

See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#conflicts-of-interest).

:::


## Introduction {#intro}
We will demonstrate that the aligned rank transform procedure [@higgins:1990; @Salter:1993; @wobbrock:2011] is problematic, raising Type I error rates at very high levels for a range of non-normal data distributions. We will show that the more recent implementation of the procedure for contrasts [@elkin:2021] does not address these issues. Although warnings have been raised by other researchers in the past [@luepsen:2017; @luepsen:2018], those have been largely ignored. 

We will also demonstrate that simpler transformation methods exhibit a better behavior, although they have their own limitations. In light of these new results, we will argue that the aligned rank transform is not a viable analysis method. From now on, researchers should consider the method as obsolete and replace it by better alternatives. Main and interaction effects reported in the past through statistical analyses conducted with the method should not be trusted unless samples come from populations that do not significantly deviate from normal and the assumption of equal variance holds. Past analyses over ordinal data, such as Likert items with five or seven levels are also concerned. 

### Illustrative example
<!----- 
The aligned rank transfom was introduced as a remedy to the problematic behavior of the simple rank transformation [@conover:1981]. @higgins:1990, as well as later @Salter:1993 used an example of normally distributed data to illustrate how the rank transform fails to correctly assess interaction effects when large main effects appear. I will use a different example to illustrate how the aligned rank transform fails when data follow instead a log-normal distribution [@Limbert:2001]. Older results from the 90s [@Salter:1993; @Mansouri:1995] suggest that the method is robust under log-normal distributions, and more recent work [@elkin:2021] states the same for contrast tests. But to what extent can we trust these results?   
----->
We will start with an example to illustrate how the aligned rank transform can increase false positives and significantly inflate observed effects. The example will also serve as a quick introduction to key concepts and methods used throughout the paper. 

Suppose an HCI researcher conducts an experiment to compare the performance of three user interface techniques (A, B, and C) that help users complete image editing tasks of four different difficulty levels. The experiment is structured using a fully balanced 4 x 3 repeated-measures factorial design, where each participant (N = 12) performs 12 tasks in a unique order. The researcher measures the time that it takes participants to complete each task. The following table presents the experimental results: 

::: {#tbl-example}
```{r, echo=FALSE, warning=FALSE}
df <- read.csv("example_data.csv", sep=",", header=TRUE, strip.white=TRUE)
kbl(df) %>% kable_paper(position = "center") %>% scroll_box(width = "740px", height = "170px")
``` 
Example dataset: Time (in minutes) spent by 12 participants for four difficulty levels and three user interface techniques. Scroll down to see the full results.
:::

The experiment is hypothetical but has similarities with real-world experiments, e.g., see the experiments of @Fruchard:2023. Time performances have been randomly sampled from a population in which: (1) *Difficulty* has a large effect; (2) *Technique* has no effect; and (3) there is no interaction effect between the two factors. To generate time values, we drew samples from a log-normal distribution. The log-normal distribution is often a good fit for real-world measurements that are bounded by zero and have low means but large variance [@Limbert:2001]. Task-completion times are good examples of such measurements [@Sauro:2010]. 

@fig-example presents two boxplots that visually summarize the main effects observed through the experiment. We plot medians to account for the fact that distributions are skewed. We observe that differences in the overall time performance of the three techniques are not visually clear, although the overall median time is somewhat higher for Technique B. In contrast, time performance clearly deteriorates as task difficulty increases. 
We also observe that for the most difficult tasks (Level 4), the median time for Technique C is lower than the median time for Techniques A and B, so we may suspect that *Difficulty* interacts with *Technique*. However, since the spread of observed values is extremely large and the number of data points is small, such differences could be the result of random noise. 

::: {#fig-example}
```{r, echo=FALSE, warning=FALSE, fig.height=2.7, fig.width=8}
cbPalette <- c("#999999", "#E69F00", "#F15854")

p1 <- (df %>%  group_by(Participant,Technique) %>% summarise(Time = median(Time), .groups="drop") %>%   
        ggplot(aes(x = Technique, y = Time, fill = Technique)) + 
        geom_boxplot(outlier.shape = NA) +
        ylab("Median Time (min)") +
        ylim(0, 3) + 
        geom_jitter(shape=20, position=position_jitter(0.1)) +
        theme_bw() + theme(legend.position = "none")) + scale_fill_manual(values=cbPalette)


p2 <- (df %>%  group_by(Participant,Difficulty,Technique) %>% summarise(Time = median(Time), .groups="drop") %>%   
        ggplot(aes(x = Difficulty, y = Time, fill = Technique)) + 
        geom_boxplot(outlier.shape = NA) +
        ylab("Median Time (min)") +
        ylim(0, 6) +
        #geom_jitter(shape=20, position=position_jitter(0.1)) +
        geom_point(size = 0.7, position=position_jitterdodge(0.1)) +
        theme_bw() + theme(legend.position = "none")) +
        #scale_fill_brewer()
        scale_fill_manual(values=cbPalette)

grid.arrange(p1, p2, nrow = 1, widths = c(1.5, 2.5))
```
Boxplots summarizing the results of our example. Dots represent the median time performance of each individual participant. 
:::


<!-- 
[I've made a try with plotly: individual plots look good but when using subplot, the bars are pretty narrow and I can't figure out how to fix that. ]{.g}

::: {#fig-exampleUsingPlotly}
```{r, echo=FALSE, warning=FALSE, fig.height=2.7, fig.width=8}
cbPalette <- c("#999999", "#E69F00", "#F15854")

df <- read.csv("example_data.csv", sep=",", header=TRUE, strip.white=TRUE)

p1 <- df %>%  
  group_by(Participant, Technique) %>%
  summarise(Time = median(Time), .groups="drop") %>%
  plot_ly(x = ~Technique, y = ~Time, type = "box", color = ~Technique,
          colors = cbPalette,
          line = list(color = "black"),
          marker = list(color = cbPalette), 
          boxpoints = "all", jitter = 0.3, pointpos = 0
          ) %>%
  layout(yaxis = list(title = "Median Time (min)", range = c(0,3)),
         xaxis = list(title = "Technique"),
         showlegend = FALSE, boxgap=0.01
         )


p2 <- df %>%  
  group_by(Participant, Difficulty, Technique) %>%
  summarise(Time = median(Time), .groups="drop") %>%
  plot_ly(x = ~Difficulty, y = ~Time, type = "box", color = ~Technique,
          colors = cbPalette,
          line = list(color = "black"),
          marker = list(color = cbPalette), 
          boxpoints = "all", jitter = 0.3, pointpos = 0
          ) %>%
  layout( yaxis = list(title = "Median Time (min)", range = c(0,6)),
         xaxis = list(title = "Difficulty"),
         showlegend = FALSE, boxmode = "group"
        )
#p1
#p2
subplot(p1, p2, widths = c(0.6, 0.4))
```
Boxplots summarizing the results of our example. Dots represent the median time performance of each individual participant. 
::: 
-->

We opt for a multiverse analysis [@Dragicevic:2019] to analyze the data, where we conduct a repeated-measures ANOVA with four different data-transformation methods:

1. *Log transformation (LOG).* Data are transformed with the logarithmic function. For our data, this is the most appropriate method as we drew samples from a log-normal distribution. 

2. *Aligned rank transformation (ART).* Data are transformed and analyzed with the ARTool [@wobbrock:2011;@elkin:2021].

3. *Pure rank transformation (RNK).* Data are transformed with the original rank transformation [@conover:1981], which does not require any data alignment.

4. *Inverse normal transformation (INT).* The data are transformed by using their normal scores. This rank-based method is simple to implement and has been commonly used in some disciplines. However, it has also received criticisms [@Beasley:2009].  

For comparison, we also report the results of the regular parameteric ANOVA with no tranformation (*PAR*). For each ANOVA analysis, we use a linear mixed-effects model, treating the participant identifier as a random effect. To simplify our analysis and like @elkin:2021, we consider random intercepts but no random slopes. For example, we use the following R code to create the model for the log-transformed response: 

```{r, echo=TRUE, warning=FALSE}
m.log <- lmer(log(Time) ~ Difficulty*Technique + (1|Participant), data = df)
```

The table below presents the *p*-values for the main effects of the two factors and for their interaction: [Would it be worth also running LOG + ART? I guess it would provide the expected results then.]{.g} [F: Yes, I don't see how this could reinforce our argumentation.]{.f}[It would not reinforce our argumentation. Just a sanity check, as according to what is below, ART should work on normally distributed data. If it does not work, it could mean there is something special with our example. If it works, it can be a teaser for after.]{.g}

|        | PAR  | LOG  | ART | RNK | INT |
|--------|------|------|-----|-----|-----|
| Difficulty  | $1.8 \times 10^{-26}$ | $8.1 \times 10^{-47}$  |  $9.0 \times 10^{-43}$ | $4.3 \times 10^{-46}$ | $4.4 \times 10^{-44}$ |
| Technique   | $.10$ | $.18$  | $.00061$ | $.38$ | $.17$ |
| Difficulty $\times$ Technique | $.056$ | $.10$ | $.0017$ | $.24$ | $.23$ |
: *p*-values for main and interaction effects {.sm}

The disparity in findings between ART and the three alternative transformation methods is striking. ART suggests that all three effects are statistically significant. What adds to the intrigue is the fact that ART's *p*-values for *Technique* and its interaction with *Difficulty* are orders of magnitude lower than the *p*-values obtained from all other methods. We will observe similar discripancies if we conduct contrast tests with the ART procedure [@elkin:2021], though we leave this as an exercise for the reader.

We also examine effect size measures, which are commonly reported in scientific papers. The table below presents results for partial $\eta^2$, which describes the ratio of variance explained by a variable or an interaction: 

|        | PAR  | LOG  | ART | RNK | INT |
|--------|------|------|-----|-----|-----|
| Difficulty  | $.64\ [.55, 1.00]$  | $.83\ [.79, 1.00]$  |  $.80\ [.76, 1.00]$ | $.83\ [.79, 1.00]$ | $.81\ [.77, 1.00]$  |
| Technique   | $.04\ [.00, 1.00]$ | $.03\ [.00, 1.00]$  | $.11\ [.03, 1.00]$ | $.02\ [.00, 1.00]$ | $.03\ [.00, 1.00]$ |
| Difficulty $\times$ Technique | $.10\ [.00, 1.00]$ | $.08\ [.00, 1.00]$  | $.16\ [.04, 1.00]$| $.06\ [.00, 1.00]$| $.06\ [.00, 1.00]$ |
: partial $\eta^2$ and its 95\% confidence interval {.sm}


We observe that ART exagerates both the effect of *Technique* and its interaction with *Difficulty*. 


### Overview
The above example does not capture a rare phenomenon. We will show that ART's error inflation is systematic for a range of distributions that deviate from normality, both continuous and ordinal. We will explain how the problem emerges. We will also see that ART often performs worse than simpler methods that ART is widely considered to repair or improve, such as the pure rank transformation or no transformation at all. Even worse, while the rank transformation has been criticized for inflating errors for interaction only effects, ART inflates errors for both interaction and main effects, as we observed in our example. 

## Background {#background}

### Non-parametric statistics
Parametric statistical procedures, such as ANOVA, are known to be fairly robust to deviations from normality assumptions, and this is especially the case when sample sizes are large. However, in many situations [...]
(Continue with an overview of assumption violations and non-parametric statistical methods and common tests.)

### Rank transformations
Unfortunately, each non-parametric statistical test addresses a specific experimental design. There is no handy test for more complex models such as ones that contain multiple independent variables. Rank transformations [@conover:1981] 
[@higgins:1990]
[@Salter:1993]
[@Mansouri:1995]
[@Beasley:2009]

[@Payton:2006] To check: This paper tests Poisson data and surprisingly, it shows that PAR and ART work well for interaction effects.

### The use of ART in experimental research 
[F: Pesent the survey of papers using ART here?]{.f}

### Past warnings
Include here all the background related to the 

[@casiez:2022] [@luepsen:2017; @luepsen:2018]

Also, check those for heteroscedacity issues: [@Carletti:2005; @Leys:2010]

- The problem of unequal variances

-  Data discretization and ordinal scales


## Interpreting effects
The ART procedure was proposed as an alterantive to the rank transformation [@conover:1981] for testing interactions. As @higgins:1990 explained, the rank transformation is non-linear and, as a result, it changes the structure of interactions. Therefore, *"interaction may exist in the transformed data but not in the original data, or vice versa"* [@higgins:1990]. @fig-interactions-rank demonstrates the problem through a representative example. In this example, the data have been sampled from perfectly normal distributions with equal variances. 

::: {#fig-interactions-rank}
```{r, echo=FALSE, message=FALSE, fig.height=3, fig.width = 7, warning=FALSE}
source("interactions_plot.R")

df <- read.csv("interactions_rank.csv", sep=",", header=TRUE, strip.white=TRUE)
df$Difficulty <- ordered(df$Difficulty, levels = c("easy", "medium", "hard"))
df_aggr <- aggregate(Y ~ Difficulty+Technique, data = df, mean)
df_rank <- aggregate(rank(Y) ~ Difficulty+Technique, data = df, mean)
colnames(df_rank) <- c("Difficulty", "Technique", "Y")

fig1 <- createRankInteractionPlot(df_aggr)
fig2 <- createRankInteractionPlot(df_rank, rnkscale = TRUE)
fig <- subplot(fig1, fig2, titleY = TRUE, titleX = TRUE, margin = 0.05, widths = c(0.5, 0.5)) %>% 
	        layout(margin = list(l = 40, r = 0, b = 0, t = 50, pad = 0))

fig
```
Visualization of interaction effect for a 3 $\times$ 2 experimental design before and after applying a rank transformation on an [example dataset](interactions_rank.csv) (within-participants design, $n = 20$). All data points represent means.
:::

We observe that while no interaction effect appears in the original data (lines are parallel), the rank transformation causes the trends to slightly change. In particular, differences are more pronounced for the middle points of the three-level factor ("medium difficulty"). This problem emerges when at least one factor of the interation of interest has more than two levels, and when main effects appear on all interacting factors. The trend deformation is weak, so the problem may only become apparent when the statistical power of the study is high, that is, main effects and/or sample sizes are sufficiently large.  

ART aims to correct this problem. However, non-linear transformations come into place in various ways in experimental designs [@Loftus:1978; @Wagenmakers:2012]. They can deform distributions, making the interpretation of observed effects especially challenging. Thus, before moving to our evaluation method and results, we discuss these problems and explain how our experimental approach takes them into consideration. 

### What is the null hypothesis of interest?
To compare different statistical methods, we first need to assess whether these methods are comparable. If two methods are not designed to test the same null hypothesis, then making direct comparisons between them could be misleading. Let us elucidate this problem.  

**ANOVA and non-parametric tests.** The traditional ANOVA is used to test differences between two or more means. However, non-parametric tests often target other population parameters. For example, the Wilcoxon sign-rank test is commonly described as a test of medians for paired samples [@McDonald:2014] and is used when population means are not of interest, e.g., when population distributions are skewed. The Mann-Whitney U and the Kruskal–Wallis tests are used, instead, to assess whether two or more independent samples come from the same population, or more technically, whether the mean ranks of the groups are the same. They can be only interpreted as tests of medians under the strict assumption that the population distributions of all groups have identical shapes and scales [@Divine:2018]. 

**Rank transformations.** Interpreting the null hypothesis of interest of a rank transformation is more challenging. @conover:1981 show that the simple rank transformation procedure *RNK* is equivalent to the Mann-Whitney U and Kruskal–Wallis tests for independent samples. For paired samples, however, it results in a new test, which is different than the Wilcoxon sign-rank test. Defining the null hypothesis of interest of ART is even more challenging because of the hybrid nature of the method. In particular, while ART is a rank-based transformation procedure, it aligns data with respect to means, where alignment is performed independently for each group.

**Dealing with the interpretation of main effects.** To partly avoid these interpretation issues, we focus on effects that apply monotonic transformations to population distributions. This also ensures a monotonic relationship between different measures of central tendancy such medians and means (with the exception of the Cauchy distribution, where the mean is undefined). In other words, if a treament increases the population mean, it will also increase the population median. We present an example in @fig-distributions. The figure shows two population distributions corresponding to the two intermediate levels of difficulty of our illustrative example (see @fig-example). We observe that the increased difficulty of the task translates both the population mean and the median to the right. In this case, we expect a statistical test to reject the null hypothesis, no matter whether it tests the population mean, the median, or the overall population shape.   

::: {#fig-distributions}
```{r, echo=FALSE, message=FALSE, fig.height=3, fig.width = 9, warning=FALSE}
library(plotly)

palette <- c("#E69F00", "#009E73")

logsd <- 0.5
logm1 <- -0.3
logm2 <- 0.3

xs <- seq(0, 10, length.out = 1000)
y1s <- dlnorm(xs, logm1, logsd)
y2s <- dlnorm(xs, logm2, logsd)

m1 <- exp(logm1 + (logsd^2)/2)
m2 <- exp(logm2 + (logsd^2)/2)
med1 <- exp(logm1)
med2 <- exp(logm2)

anot_m1 <- list(x = m1, y = 0, text = "mean", xref = "x", yref = "y",
  showarrow = TRUE, arrowhead = 6, arrowsize = 1, arrowcolor = palette[1],
  ax = 0, ay = -40
)

anot_med1 <- list(x = med1, y = 0, text = "median", xref = "x", yref = "y", 
  showarrow = TRUE, arrowhead = 7, arrowsize = 1, arrowcolor = palette[1],
  ax = 0, ay = 30
)

anot_m2 <- list(x = m2, y = 0, text = "mean", xref = "x", yref = "y",
  showarrow = TRUE, arrowhead = 6, arrowsize = 1, arrowcolor = palette[2],
  ax = 0, ay = -43
)

anot_med2 <- list(x = med2, y = 0, text = "median", xref = "x", yref = "y", 
  showarrow = TRUE, arrowhead = 7, arrowsize = 1, arrowcolor = palette[2],
  ax = 0, ay = 30
)

annot1 <- paste("Log-normal: meanlog = ", logm1, ", sdlog = ", logsd)
annot2 <- paste("Log-normal: meanlog = ", logm2, ", sdlog = ", logsd)

fig <- plot_ly() %>%  
      add_lines(x = xs, y = y1s, visible = TRUE, type = 'scatter', mode = 'lines', text = annot1, hoverinfo = 'text', line=list(color=palette[1])) %>%
      add_lines(x = xs, y = y2s, visible = TRUE, type = 'scatter', mode = 'lines', text = annot2, hoverinfo = 'text', line=list(color=palette[2])) %>%      
      add_annotations(x = med1, y = max(y1s), text = "Difficulty Level 2", xref = "x", yref = "y", showarrow = F, font = list(color = palette[1]), xanchor = 'left') %>% 
      add_annotations(x = med2, y = max(y2s), text = "Difficulty Level 3", xref = "x", yref = "y", showarrow = F, font = list(color = palette[2]), xanchor = 'left') %>% 
      layout(annotations = anot_m1) %>% layout(annotations = anot_med1) %>%      
      layout(annotations = anot_m2) %>% layout(annotations = anot_med2) %>%    
      layout(
        showlegend = FALSE,
        xaxis = list(title = "Time (min)", range = c(0, 4.5), showgrid = F, showticks = T, ticks="outside", zeroline = F), 
        yaxis = list(showgrid = F, showticklabels = F, showline = F, range=c(0, max(y1s) + .05), fixedrange=T)
      ) %>% 
	      config(displayModeBar = TRUE, scrollZoom = FALSE, displaylogo = FALSE, modeBarButtonsToRemove = c("lasso2d", "select2d",  "zoomIn2d", "zoomOut2d", "autoscale", "hoverclosest", 'hoverCompare'))

fig

```
Time distributions for two task populations with dificulty levels 2 and 3 (see @fig-example). 
:::

Nevertheless, the increased difficulty of the task does not simply translate the distribution to the rigth. The shape and scale of the distribution also change -- the variance increases, and the mean and median do not increase by the same amount. As we discuss below, such non-linear transformations complexify the interpretation of interactions.     

### Interaction interpretation problems

Let us take a different [dataset](removable_interactions.csv) from a fictional experiment (within-participants design with $n = 24$) that evaluates the performance of two techniques (*Tech A* and *Tech B*) under two task difficulty levels (*easy* vs. *hard*). The experiment, for example, could test a mobile typing task, where the levels of difficulty correspond to texts of different lengths (*short* vs. *long*) under two typing techniques (*with* vs. *without auto-completion*). We assume that the researchers measure two dependent variables: task-completion time and perceived performance, which is measured through a five-level ordinal scale (from "very quick" to "very slow"). In this example, the main effects of task difficulty and technique are large. What is less clear however is whether there is an interaction between the two factors. 

@fig-interactions visualizes the means for each combination of the levels of the factors and highlights the possible interactions. Let us first concentrate on the first two plots that present results for the time measure. The trends in the left plot indicate an interaction effect, since the two lines seem to diverge as the task difficulty increases. 

::: {#fig-interactions}
```{r, echo=FALSE, message=FALSE, fig.height=3.5, fig.width = 9, warning=FALSE}
df <- read.csv("removable_interactions.csv", sep=",", header=TRUE, strip.white=TRUE)
dftime <- aggregate(Time ~ Difficulty+Technique, data = df, mean)
dfpref <- aggregate(PerceivedPerformance ~ Difficulty+Technique, data = df, mean)

fig1 <- createInteractionPlot(dftime)
fig2 <- createInteractionPlot(dftime, logscale = TRUE)
fig3 <- createInteractionPlot(dfpref, likert = TRUE)
#fig <- subplot(fig1, fig2, fig3, titleY = TRUE, titleX = TRUE, margin = 0.08, widths = c(0.32, 0.36, 0.32))

fig <- subplot(fig1, fig2, titleY = TRUE, titleX = TRUE, margin = 0.06) %>% subplot(fig3, titleY = TRUE, titleX = TRUE, margin = 0.07, widths = c(0.66, 0.33)) 

fig
```
The linecharts visualize the effects of task difficulty (*easy* vs. *hard*) and technique (*Tech* A vs. *Tech B*) for two measures: task-completion time (left and middle) and perceived speed (right). All data points represent group means. 
:::

**The problem of different scales.** But how meaningful is this interpretation of interaction? Time measurements are often taken from distributions of different scales, that is, large effects are harder to observe in quick tasks than in slow ones. For example, performance differences in sprint races are in the range of milli- or centiseconds, while differences in long-distance races can be in the range of several seconds or minutes. So if we compare the time performance of any two groups of people (e.g., 14- vs. 12-year-old children), we will always find that absolute differences grow as race distance increases. However, such trends do not necessarily reveal any real interactions, because they are simply due to observations at different time scales. This issue is not specific to running races. @Wagenmakers:2007 show that the standard deviation of reponse times increases linearly with their mean. This relationship is depicted in @fig-example and @fig-distributions, where the mean and the spread of the time distributions grow together as task difficulty increases. 

In our example in @fig-interactions, each task (typing a piece of text) is a sequence of elementary tasks (typing a word). We thus expect both means and standard deviations to grow as a function of the number of words in the text. In this case, meaningful interaction effects (e.g., Tech B suffers from more intense fatigue effects in longer texts) will be manifested as growing time ratios rather than growing time differences. An easy way to visually assess the presence of such interactions is to show time on a logarithmic scale, as shown in the middle plot of @fig-interactions. Notice that the lines in the plot are now almost parallel, suggesting no interaction effect.

**Removable interactions.** The concept of *removable interactions*, that is, interactions that disappear after applying a monotonic non-linear transformation, was introduced by @Loftus:1978. @Wagenmakers:2012 revisited this work 33 years later. Their litterature review showed that psychology researchers are largely unaware of the concept, drawing incorrect conclusions about psychological effects on the basis of meaningless interactions. The problem, of course, is not specific to time measurements and skewed distributions with unequal variances. The right plot in @fig-interactions shows experimental results for perceived performance. The line trends suggest an interaction effect, in the inverse now direction. Unfortunately, the scale is ordinal, which means that distances between the five levels of the scale may not be perceived as equal by people. Furthermore, the scale is bounded, so the reason that the two lines are not parallel might be simply due to the absence of additional levels beyond the extreme "very slow" ranking. Concluding that there is a meaningful interaction here is likely to be incorrect. 

**Formally testing for statistical significance**. Let us now formally test the above interactions, using ANOVA with different transformation methods. Below, we present the *p*-value returned by each method for task-completion time: 

| PAR  | LOG  | ART | RNK | INT |
|------|------|-----|-----|-----|
| $.023$ | $.67$ | $.00073$ | $.66$ | $.67$ |
: *p*-values for interaction effect on task-completion time {.sm}

We observe that *RNK* and *INT* lead to *p*-values very close to the *p*-value of *LOG*, which suggests a similar interpretation of interaction effects. In contrast, ART returns a very low *p*-value (lower than the *p*-value of the regular ANOVA), showing that the method is extermely sensitive to scale effects.  

We also test the interaction effect on the ordinal dependant variable:

| PAR  | ART | RNK | INT |
|------|-----|-----|-----|
| $.0020$ | $.00075$ | $.0067$ | $.0037$ |
: *p*-values for interaction effect on perceived performance {.sm}

Notice that we omit the log-transformation method (*LOG*), as it is not relevant here. All *p*-values are low, suggesting that an interaction effect exists. However, if we conduct a more appropriate analysis using an ordered probit model [@Liddell:2018; @Burkner:2019], we will reach the conclusion that there is no supportive evidence for such an effect. We do not present this analysis here, but we return to these models later in the paper. 

### Approach
Our analysis shows that inference errors are not simply due to the lack of robustness of a statistical procedure. In the case of interaction effects, errors will also emerge when the procedure makes inference on the wrong scale. As @Wagenmakers:2012 explain, *"the dependent variable reflects merely the output of a latent psychological process",* and unfortunally, *"in most experimental paradigms the exact relationship between unobserved process and observed behavior is unknown ..."* 

Ideally, a statistical procedure should lead to conclusions that capture the true effects on the latent variable of interest. But as we discussed above, this might not be the case for the rank transformation methods that we study. For example, all the four methods (*PAR*, *RNK*, *ART*, and *INT*) suggest that task difficulty interacts with use of technique on perceived performance because they disregard the fact that the observed data are simply projections on a discrete ordinal scale. Our goal is to understand how ART and the other methods deal with such problems. At the same time, we want to distinguish between failures due to the lack of statistical robustness and failures due to the choice of measurement scales. 

**Latent variable.** To approach the problem, we assume that there is a latent variable of interest that is different than the variable we observe. For example, a latent variable may represent the performance potential of a population of people, their working memory capacity, their perceived utility of a new technology, or their quality of life. For convenience, we assume that the latent variable is continuous and normally distributed. This assumption is common in latent variable modeling, e.g., in diffusion-decision models that predict response time and error in two-choice decision tasks [@Ratcliff_diffusion:2008], and ordinal models [@Liddell:2018].

**Transformation to observed variable.** Then, all dependent variables that we observe are derived from this latent variable through a monotonic transformation. A transformation for example occurs when study participants perform a selection task or repond to a Likert-scale item through a questionnaire. We study two types of transformations: 

[add figures to explain the transformation mechanisms]

1. *Transformations to ratio scales.* We rely our on the distribution conversion approach of *faux* v1.2.1 [@faux], an R package for experimental simulations. First, we derive the cumulative density distribution of the latent variable. Then, we use the inverse quantile function of the target distribution to derive the final distribution. 

2. *Transformations to ordinal scales.* 


## Experimental method {#methodology}
To evaluate ART, we compare it to the regular parmateric ANOVA without transformation (*PAR*) and the two rank-based transformations that we introduced earlier: the regular rank transformation (*RNK*) and the inverse normal transformation (*INT*). We conduct a series of Monte Carlo experiments that assess performance under a variety of experimental configurations:

1. We evaluate *ratio* and *interval* data. For ratio data, we examine four representative *continuous* distributions (normal, log-normal, exponential, and Cauchy distribution) and two *discrete* distributions (Poisson and binomial distribution). For interval data, we examine distributions for 5- and 7-level Likert item responses, as well as responses with 20 levels, as they are typically used in NASA-TXL evaluation questionnaires.

2. We present results for four experimental designs. To simplify our presentation, we start with a 4 $\times$ 3 repeated-measures factorial design. We then show how our conclusions generalize to three additional designs: (i) a 2 $\times$ 3 between-subjects design; (ii) a 2 $\times$ 4 mixed design, with a between-subjects factor and a repeated-measures factor; and (iii) a 3 $\times$ 3 $\times$ 3 repeated-measures design.

3. We evaluate three sample sizes, $n=10$, $n=20$, and $n=30$, where $n$ represents the cell size in an experimental design. For within-subjects designs where all factors are treated as repeated measures, $n$ coincides with the number of subjects (or commonly participants in HCI research). 

4. We test the robustness of the methods under unequal variances. 

5. In addition to Type I error rates, we compare the statistical power of the methods and also evaluate the quality of their effect size estimates.  

6. We assess the above measures for both main and interaction effects. In distinction with the experimental method of @elkin:2021, we examine how growing effects on one or two factors affect the Type I error rate on other factors or their interactions. 

Some previous evaluations of rank transformation methods [@Beasley:2009; @luepsen:2018] have also evaluated unbalanced designs, where the cell size is not constant across all levels of a factor. When combined with unequal variances, unbalanced designs are often problematic for both regular ANOVA [?] and rank transformation methods [@Beasley:2009; @luepsen:2018]. Since unbalanced designs are not common in HCI experimental research, we do not consider them here. 

### Statistical modeling
We assume that observations for the response variable $Y$ come from a two-way (two factors) or three-way (three factors) mixed-effects model. For clarity, we explain here the model for two factors, but its extention to three factors is straightforward. The model has the following form:

$$ 
y_{ijk} = \mu + s_k + a_1 x_{1i} + a_2 x_{2j} + a_{12} x_{1i} x_{2j} + \epsilon_{ijk}
$${#eq-linear-model}

 - $\mu$ is the grand mean

 - $s_k$ is the random intercept effect of the *k*-th subject, where $k = 1..n$ 

 - $x_{1i}$ is a numerical encoding of the *i*-th level of factor $X_1$, where $i = 1..m_1$

 - $x_{2j}$ is a numerical encoding of the *j*-th level of factor $X_2$, where $j = 1..m_2$

 - $a_1$, $a_2$, and $a_{12}$ express the magnitude of main and interaction effects 

 - $\epsilon_{ijk}$ is the experimental error effect

To encode the levels of the two factors $x_{1i} \in X_1$ and $x_{2j} \in X_2$ we proceed as follows: 

1. We normalize the distance between their first and their second levels such that $x_{12} - x_{11} = 1$ and $x_{22} - x_{21} = 1$. This approach enables us to conveniently control for the main and interaction effects by simply varying the parameters $a_1$, $a_2$, and $a_{12}$.

2. For the remaining levels, we randomly sample from a uniform distribution that spans the range between these two extreme levels, i.e., between $x_{11}$ and $x_{12}$ for $X_1$, and between $x_{21}$ and $x_{22}$ for $X_2$. This approach allows us to generate and evaluate a substantial variety of configurations, each representing different relative effects between levels.  

3. We require all levels to sum up to 0, or $\sum\limits_{i=1}^{m_1}  x_{1i} = 0$ and $\sum\limits_{j=1}^{m_2}  x_{2j} = 0$, which ensures that the grand mean is $\mu$.

For example, we can encode a factor with four levels as $\{-.6, .4, .1, .1\}$ or as $\{-.5, .5, .3, -.3\}$. 

While random slope effects can have an impact on real experimental data [@barr:2013], we do not incorporate them into our simulations for three reasons: (1) to be consistent with previous evaluations of the ART procedure [@elkin:2021]; (2) because random slope effects make the control for equal variances and sphericity assumptions more challenging; and (3) because mixed-effects procedures that account for random slope effects are more computationally demanding, adding strain to our simulation resources. However, there is no good reason to believe that adding random slope effects would affect our key findigns and conclusions.

### Population control, sampling, and distribution conversions 
To simplify our simulations, we fix the following population parameters: $\mu = 0$, $\sigma = 1$, and $\sigma_s = 1$. We then control the magnitude of effects by varying $a_1$, $a_2$, and $a_{12}$. 

We follow the approach of @DeBruine:2021 and use the R package *faux* v1.2.1 [@faux] to simulate data for our mixed-effects models. As a starting point, we assume that the distributions of random intercepts and errors are normal, or $s_k \sim N(0,\sigma_s)$ and $\epsilon_{ijk} \sim N(0,\sigma)$. To simulate non-normal distributions, we convert response values $y_{ijk}$ as follows. First, we derive their cumulative density distribution using the cumulative density function of the normal distribution. Then, we use the inverse quantile function of the target distribution to derive the final distribution. For example, to convert normal responses to log-normal, we use the following R function:  
```{r}
norm2lnorm <- function(x, meanlog = 0, sdlog = 1, mu = mean(x), sd = sd(x), ...) {
	 p <- pnorm(x, mu, sd) 
 	 qlnorm(p, meanlog, sdlog, ...) 
}
```

### Implementation of rank transformation methods 
For the aligned rank transformation (ART), we use the R implementation of ARTool v0.11.1 [@artool]. For the pure rank transformation (RNK), we use R's *rank()* function. We use the *Rankit* formulation [@Bliss:1956] for the inverse normal transformation (INT), as past simulation studies [@Solomon2009] have shown that it is more accurate than alternative formulations. We implement it in R as follows: 

```{r}
inverseNormalTransform <- function(x){
	qnorm((rank(x) - 0.5)/length(x))
}
```

We compare the three rank-based methods against the naive parametric method (PAR) that assumes normality and does not apply any data transformation.

### Evaluation measures
Significance tests have two types of errors. *Type I errors*, or false positives, are mistaken rejections of the null hypothesis. Type II errors, of false negatives, are failures to reject a null hypothesis that is actually true. In our illustrative example in @fig-example, a Type I error is finding that there is an effect of the choice of the technique on time performance. A *Type II error*, in turn, is finding that the task difficulty has no effect on time performance. 

Statistical significance testing requires setting a significance threshold known as significance or $\alpha$ (alpha) level, with typical values $\alpha = .05$ and $\alpha = .01$. The Type I error rate of a well-behaved significance test should be close to this nominal alpha level. An error rate clearly above this level suggests that the singificance test is too liberal, while an error rate clearly below this level suggests that the test is too conservative. [Sections 4 - 7](#ratio) specifically focus on evaluating the Type I error rate of the methods. We test two significance levels: $\alpha = .05$ and $\alpha = .01$. For brievity, we only report results for $\alpha = .05$ in the main paper. We include additional results in our supplementary material. 

We do not directly evaluate Type II errors. Instead, we report on statistical *power* defined as $Power = 1 - \beta$, where $\beta$ is the rate of Type II errors. Significance tests do not provide any power guarantees. However, we can compare the power of different methods to evaluate their relative performance. Since there is a tradeoff between power and Type I errors, we avoid power comparisons when Type I error rates are inflated beyond acceptable levels.

In addition to power, we assess effect size estimates, focusing on the partial $\eta^2$ measure, which is commonly used in ANOVA. To evaluate the quality of estimates of different methods, we measure how well they correlate with the estimates of the optimal ground-truth method, e.g., parametric ANOVA under normal distributions. We present our results on power and effect sizes in [Section 8](#power). 

### Apparatus and iterations
Our experimental code is written in R and is available in our supplementary material. We ran our experiments seperately in a cluster of 8 machines Dell R640 Intel Xeon Silver 4112 2.6GHz with 4 cores and 64 GB memory. Our R code was parallelized to use all four cores of each machine. Some experiments took a few hours to complete, while others took several days. 

To estimate the power and Type I error rates of the four methods with enough precision, we ran $5000$ iterations for each population configuration and each sample size. 

<!--
We evaluate the Type I error rate, that is the rate of false positives of each method. To evaluate the Type I error rate for the interaction effect, we set $a_{12} = 0$. Likewise, to evaluate the Type I error rate for the second factor $X_2$, we set $a_{2} = 0$. For each population configuration and sample size $n$, we run $5000$ iterations and test two significance levels: $\alpha = .05$ and $\alpha = .01$. For brievity, we only report results for $\alpha = .05$ in the main paper, while additional results are presented in supplementary materialsxa
-->

## Ratio scales {#ratio}
We first evaluate Type I error rates for ratio variables. We test four continuous and two discrete distributions with a fixed set of parameters:  

1. The normal distribution $N(\mu, \sigma)$ with parameters $\mu = 0$ and $\sigma = 1$. 

2. The log-normal distribution $LogN(\mu, \sigma)$ with parameters $\mu = 0$ and $\sigma = 1$. As discussed in [the introduction](#intro), the log-normal distribution is a good model for a range of measures bounded by zero, such as task-completion times.

3. The exponential distribution $Exp(\lambda)$ with a single parameter $\lambda = 2$. The exponential distribution naturally emerges when describing the time ellapsed between events. For example, it could be used to model the time random person spends with a public display or the waiting time before a new passenger approaches to interact with the display, when the average waiting time is $\frac{1}{\lambda}$. 

4. The Cauchy distribution $Cauchy(x_0,\gamma)$ with parameters $x_0 = 0$ and $\gamma = 1$. The Cauchy distribution is the distribution of the ratio of two independent normally distributed random variables. It rarely emerges in practice. However, it is commonly used in statistics to test the robustness of statistical procedures because both its mean and variance are undefined. We include it in our evaluation because previous evaluations of ART [@elkin:2021]  

5. The Poisson distribution $Pois(\lambda)$ with a single parameter $\lambda = 3$

6. The binomial distribution $B(n,p)$ with parameters $n = 10$ and $p=.1$ 
 
For this experiment, we tested a 4 $\times$ 3 repeated-measures design. We refer to the 4-level factor as $X_1$ and the 3-level factor as $X_2$.

### Results

**Interaction effects**. @fig-results1 presents Type I error rates ($\alpha = .05$) for the interaction effect when increasing the magnitude of effect of both factors $X_1$ and $X_2$. 


::: {#fig-results1}
```{r, echo=FALSE, message=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
library(dplyr)
library(tidyverse)
source("dataReaders.R")
source("plotlying.R")

prefix <- "1_test_4x3_Ratio"
alpha <- .05
distributions <- c("norm", "lnorm", "exp", "cauchy", "poisson", "binom")
dnames <- c("Normal", "Log-normal", "Exponential", "Cauchy", "Poisson", "Binomial")
df <- readlyData(prefix, alpha, 0, distributions, dnames)

#xlab <- TeX("\\text{main effect of factors }X_1\\text{ and }X_2\\text{ }(a_1 = a_2)")
plotlyError(df, xlab = "magnitude of main effects", var = "rateX1X2", xvar = "effectX1", max = 105)
```
Type I error rates ($\alpha = .05$) for the **interaction effect** as a function of the magnitude of the main effects $a_1 = a_2$
:::

Let's first examine the performance of each method individually. 

- *PAR*. As expected, the regular parametric ANOVA only performs well when normality assumptions are met. Contrary to a widespread belief that violations of the normality assumption are less serious when samples become larger, this does not seem to be the case for interaction effects.

- *RNK.* We confirm that errors under the rank transformation explode when main effects increase beyond a certain level. Note that the method's performance is identical across all continuous distributions.

- *INT.* The inverse normal transformation demonstrates a better behavior. Errors start again increasing but only when main effects become large. 

- *ART.* As opposed to the previous two methods, ART keeps the error rate at correct levels when population distributions are normal. However, the method's performance is the worst for all non-normal distributions. Type I error rates start growing fast as main effects increase and quickly explode. For the Cauchy and the binomial distribution, errors are high even when main effects are zero.  

Let us now examine the performance of the four methods in the presence of a single main effect. @fig-results2 presents the Type I error rates ($\alpha = .05$) for the interaction effect as the magnitude of the main effect on factor $X_1$ increases, while the effect of factor $X_2$ is zero. 

::: {#fig-results2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- readlyData(prefix, alpha, 1, distributions, dnames)

plotlyError(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX1", max = 45)
```
Type I error rates ($\alpha = .05$) for the **interaction effect** as a function of the magnitude of $X_1$'s main effect $a_1$
:::

RNK and INT keep Type I error rates close to $5\%$ across all distributions. The regular parametric ANOVA does not inflate errors, but error rates for some configurations are significantly lower than $5\%$, which may suggest loss of statistical power. ART's behavior is again problematic for all non-normal distributions, inflating Type I error rates beyond acceptable levels. 

**Main effects**. However, ART does not only inflate error rates on interaction effects. @fig-results3 presents the Type I error rates for the main effect of $X_2$ when the magnitude of the main effect of $X_1$ increases. We observe similar patterns as for interaction effects, although error rates are even larger now.   

::: {#fig-results3}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotlyError(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 64)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitude of $X_1$'s main effect $a_1$
:::

**Contrasts**.

::: {#fig-contrasts}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "5_test_4x3_Contrasts"

df <- readlyData(prefix, alpha, 1, distributions, dnames)
plotlyError(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 52)
```
Average Type I error rates ($\alpha = .05$) for **contrasts on factor $X_2$** as a function of the magnitude of $X_1$'s main effect $a_1$
:::


### Summary
Our first results on a balanced repeated-measures 4 $\times$ 3 factorial design demonstrate that unless distributions are normal, ART does not provide any grarantee for the Type I errors of main and interaction effects. Its failure under the Cauchy distribution is not a surprise, since it was also observed by @Salter:1993, and later by @elkin:2021. However, the authors had argued that ART is robust under other distributions, such as the exponential and the log-normal distributions [What parameters did they use for their distributions?]{.g}. Our results demonstrate that this is not the case and corroborate the warnings of @luepsen:2018.

<!--
If we only focus on interaction effects, we observe that all four techniques fail to control for errors when both main effects grow, but ART's error rates explode significantly quicker. 

For non-normal distributions, INT tends to exhibit the lowest error rates, which corroborates previous results [@luepsen:2018]. Although the method also fails when effects sizes become large, we should mention that the effect sizes that we tested are especially large. Effects with $a_1$ and $a_2$ greater than $0.8$ result in $100\%$ statistical power for sample sizes as low as $n=10$. Such power levels are uncommon in real experimental designs. Thus, given its simplicity, the inverse normal transformation might still be a viable alternative. We will evaluate its performance with additional tests 
-->

## Ordinal scales {#ordinal}

### Distributions


### Results

**Interaction effects**.

::: {#fig-results-ordinal-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "2_test_4x3_Ordinal"

distributions <- c("likert5", "likert5B", "likert7", "likert7B", "likert20", "likert20B")
dnames <- c("5 - equidistant", "5 - variable", "7 - equidistant", "7 - variable", "20 - equidistant", "20 - variable")

df <- readlyData(prefix, alpha, 0, distributions, dnames)
plotlyError(df, xlab = "magnitude of main effects", var = "rateX1X2", xvar = "effectX1", max = 105)

```
Type I error rates ($\alpha = .05$) for the **interaction effect** as a function of the magnitude of the main effects $a_1 = a_2$
:::


::: {#fig-results-ordinal-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}

distr = distributions
df2 <- rbind(
  readData(prefix, 10, alpha, 1, distributions = distr), 
  readData(prefix, 20, alpha, 1, distributions = distr),
  readData(prefix, 30, alpha, 1, distributions = distr)
)

names <- c("5 levels (equidistant)", "5 levels (variable)", "7 levels (equidistant)", "7 levels (variable)", "20 levels (equidistant)", "20 levels (variable)")

df <- readlyData(prefix, alpha, 1, distributions, dnames)
plotlyError(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX1", max = 45)
```
Type I error rates ($\alpha = .05$) for the **interaction effect** as a function of the magnitude of $X_1$'s main effect $a_1$
:::

**Main effects**.

::: {#fig-results-ordinal-3}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotlyError(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 64)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitude of $X_1$'s main effect $a_1$
:::


## Performance accross experimental designs {#extra}

### Results

**Interaction effects**.

::: {#fig-designs-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "3_test-Designs"
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readlyDataByDesign(prefix, 20, alpha, 0, distributions, dnames)
plotlyError(df, xlab = "magnitude of main effects", var = "rateX1X2", xvar = "effectX1", max = 105, byDesign = TRUE)

```
Type I error rates ($\alpha = .05$) for the **interaction effect** as a function of the magnitude of the main effects $a_1 = a_2$ $(n = 20)$
:::

::: {#fig-designs-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- readlyDataByDesign(prefix, 20, alpha, 1, distributions, dnames)
plotlyError(df, xlab = "magnitude of main effects", var = "rateX1X2", xvar = "effectX1", max = 81, byDesign = TRUE)

```
Type I error rates ($\alpha = .05$) for the **interaction effect** $X_1 \times X_2$ as a function of $X_1$'s main effect $a_1$ ($n=20$)
:::


**Main effects**.

::: {#fig-designs-3}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotlyError(df, xlab = "magnitude of main effects", var = "rateX2", xvar = "effectX1", max = 80, byDesign = TRUE)
```
Type I error rates ($\alpha = .05$) for the **main effect** $X_2$ as a function of $X_1$'s main effect $a_1$ ($n=20$)
:::

## Unequal variances
Experiments for a 2 x 4 mixed design 

### Results

**Interaction effects**.

::: {#fig-results-hetero-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "4_test_2x4_Hetero"
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readlyData(prefix, alpha, 0, distributions, dnames, effectvars = c("effectX1","effectX2","effectX1X2", "sd_ratio"))
plotlyError(df, xlab = "ratio of variances between level of X1", var = "rateX1X2", xvar = "sd_ratio", max = 33)
```
Type I error rates ($\alpha = .05$) for the **interaction effect**, as the variance difference on $X_1$ increases
:::


**Main effects**.

::: {#fig-results-hetero-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotlyError(df, xlab = "ratio of variances between level of X1", var = "rateX2", xvar = "sd_ratio", max = 33)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$**, as the variance difference on $X_1$ increases
:::

## Statistical power and effect sizes {#power}

<!--
# Understanding ART's breakdown
--> 

## Case studies {#casestudies}


## Recommendations {#recommendations}

## Conclusion {#conclusion}

## References {.unnumbered}

::: {#refs}
:::
