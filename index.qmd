---
title: "The illusory promise of the Aligned Rank Transform --- A procedure that should be abandoned"
author: 
  - name: Theophanis Tsandilas
    orcid: 0000-0002-0158-228X
    email: theophanis.tsandilas@inria.fr
    affiliations:
      - name: Université Paris-Saclay, CNRS, Inria, LISN
        country: France
  - name: Géry Casiez
    orcid: 0000-0003-1905-815X
    email: gery.casiezuniv-lille.fr
    affiliations:
      - name: Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL
        country: France
bibliography: bibliography.bib

tbl-cap-location: top
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Useful libraries
library(crosstalk)
library(kableExtra)
library(gridExtra)
library(lmerTest)
library(tidyverse)
library(plotly)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Source code for reading data from experimental results and for plotting
source("dataReaders.R")
source("plotting.R")
```

```{=html}
<style>
.math.inline .MathJax  {
  font-size: 105% !important;
}

.g::before {
  content: "[G: ";
}

.g::after {
  content: "]";
}
.g {
  color: #F28C28;
}

.f {
  color: #d62b1c;
}
</style>
```

::: {.callout-note appearance="simple" icon=false collapse=true}
## Abstract {.unnumbered}

###### Introduction
The research question and a succinct motivation for answering it. 

###### Background
Background research. 

:::


::: {.callout-note appearance="simple" icon=false collapse=true}
## Materials, Authorship, License, Conflicts

Fill in the following sections in this infobox per the [JoVI Author Guide](https://www.journalovi.org/author-guide.html#sections), then delete this line.

###### Research materials

See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#research-material-statements).

###### Authorship

See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#authorship).

###### License

See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#license).

###### Conflicts of interest

See [the corresponding section in the author guide](https://www.journalovi.org/author-guide.html#conflicts-of-interest).

:::


## Introduction {#intro}
We will demonstrate that the aligned rank transform procedure [@higgins:1990; @Salter:1993; @wobbrock:2011] is problematic, raising Type I error rates at very high levels for a range of non-normal data distributions. We will show that the more recent implementation of the procedure for contrasts [@elkin:2021] does not address these issues. Although warnings have been raised by other researchers in the past [@luepsen:2017; @luepsen:2018], those have been largely ignored. 

We will also demonstrate that simpler transformation methods exhibit better behavior, although they have their limitations. In light of these new results, we will argue that the aligned rank transform is not a viable analysis method. From now on, researchers should consider the method obsolete and replace it with better alternatives. Main and interaction effects reported in the past through statistical analyses conducted with the method should not be trusted unless samples come from populations that do not significantly deviate from normal and the assumption of equal variance holds. Past analyses over ordinal data, such as Likert items with five or seven levels are also concerned. 

### Illustrative example
<!----- 
The aligned rank transfom was introduced as a remedy to the problematic behavior of the simple rank transformation [@conover:1981]. @higgins:1990, as well as later @Salter:1993 used an example of normally distributed data to illustrate how the rank transform fails to correctly assess interaction effects when large main effects appear. I will use a different example to illustrate how the aligned rank transform fails when data follow instead a log-normal distribution [@Limbert:2001]. Older results from the 90s [@Salter:1993; @Mansouri:1995] suggest that the method is robust under log-normal distributions, and more recent work [@elkin:2021] states the same for contrast tests. But to what extent can we trust these results?   
----->
We will start with an example to illustrate how the aligned rank transform can increase false positives and significantly inflate observed effects. The example will also serve as a quick introduction to key concepts and methods used throughout the paper. 

Suppose an HCI researcher conducts an experiment to compare the performance of three user interface techniques (A, B, and C) that help users complete image editing tasks of four different difficulty levels. The experiment is structured using a fully balanced $4 \times 3$ repeated-measures factorial design, where each participant (N = 12) performs 12 tasks in a unique order. The researcher measures the time that it takes participants to complete each task. The following table presents the experimental results: 

::: {#tbl-example}
```{r, echo=FALSE, warning=FALSE}
df <- read.csv("example_data.csv", sep=",", header=TRUE, strip.white=TRUE)
kbl(df) %>% kable_paper(position = "center") %>% scroll_box(width = "740px", height = "170px")
``` 
Example dataset: Time (in minutes) spent by 12 participants for four difficulty levels and three user interface techniques. Scroll down to see the full results.
:::

The experiment is hypothetical but has similarities with real-world experiments, e.g., see the experiments of @Fruchard:2023. Time performances have been randomly sampled from a population in which: (1) *Difficulty* has a large effect; (2) *Technique* has no effect; and (3) there is no interaction effect between the two factors. To generate time values, we drew samples from a log-normal distribution. The log-normal distribution is often a good fit for real-world measurements that are bounded by zero and have low means but large variance [@Limbert:2001]. Task-completion times are good examples of such measurements [@Sauro:2010]. 

@fig-example presents two boxplots that visually summarize the main effects observed through the experiment. We plot medians to account for the fact that distributions are skewed. We observe that differences in the overall time performance of the three techniques are not visually clear, although the overall median time is somewhat higher for Technique B. In contrast, time performance clearly deteriorates as task difficulty increases. 
We also observe that for the most difficult tasks (Level 4), the median time for Technique C is lower than the median time for Techniques A and B, so we may suspect that *Difficulty* interacts with *Technique*. However, since the spread of observed values is extremely large and the number of data points is small, such differences could result from random noise. 

::: {#fig-example}
```{r, echo=FALSE, warning=FALSE, fig.height=2.7, fig.width=8}
cbPalette <- c("#999999", "#E69F00", "#F15854")

p1 <- (df %>%  group_by(Participant,Technique) %>% summarise(Time = median(Time), .groups="drop") %>%   
        ggplot(aes(x = Technique, y = Time, fill = Technique)) + 
        geom_boxplot(outlier.shape = NA) +
        ylab("Median Time (min)") +
        ylim(0, 3) + 
        geom_jitter(shape=20, position=position_jitter(0.1)) +
        theme_bw() + theme(legend.position = "none")) + scale_fill_manual(values=cbPalette)


p2 <- (df %>%  group_by(Participant,Difficulty,Technique) %>% summarise(Time = median(Time), .groups="drop") %>%   
        ggplot(aes(x = Difficulty, y = Time, fill = Technique)) + 
        geom_boxplot(outlier.shape = NA) +
        ylab("Median Time (min)") +
        ylim(0, 6) +
        #geom_jitter(shape=20, position=position_jitter(0.1)) +
        geom_point(size = 0.7, position=position_jitterdodge(0.1)) +
        theme_bw() + theme(legend.position = "none")) +
        #scale_fill_brewer()
        scale_fill_manual(values=cbPalette)

grid.arrange(p1, p2, nrow = 1, widths = c(1.5, 2.5))
```
Boxplots summarizing the results of our example. Dots represent the median time performance of each individual participant. 
:::


<!-- 
[I've made a try with plotly: individual plots look good but when using subplot, the bars are pretty narrow and I can't figure out how to fix that. ]{.g}

::: {#fig-exampleUsingPlotly}
```{r, echo=FALSE, warning=FALSE, fig.height=2.7, fig.width=8}
cbPalette <- c("#999999", "#E69F00", "#F15854")

df <- read.csv("example_data.csv", sep=",", header=TRUE, strip.white=TRUE)

p1 <- df %>%  
  group_by(Participant, Technique) %>%
  summarise(Time = median(Time), .groups="drop") %>%
  plot_ly(x = ~Technique, y = ~Time, type = "box", color = ~Technique,
          colors = cbPalette,
          line = list(color = "black"),
          marker = list(color = cbPalette), 
          boxpoints = "all", jitter = 0.3, pointpos = 0
          ) %>%
  layout(yaxis = list(title = "Median Time (min)", range = c(0,3)),
         xaxis = list(title = "Technique"),
         showlegend = FALSE, boxgap=0.01
         )


p2 <- df %>%  
  group_by(Participant, Difficulty, Technique) %>%
  summarise(Time = median(Time), .groups="drop") %>%
  plot_ly(x = ~Difficulty, y = ~Time, type = "box", color = ~Technique,
          colors = cbPalette,
          line = list(color = "black"),
          marker = list(color = cbPalette), 
          boxpoints = "all", jitter = 0.3, pointpos = 0
          ) %>%
  layout( yaxis = list(title = "Median Time (min)", range = c(0,6)),
         xaxis = list(title = "Difficulty"),
         showlegend = FALSE, boxmode = "group"
        )
#p1
#p2
subplot(p1, p2, widths = c(0.6, 0.4))
```
Boxplots summarizing the results of our example. Dots represent the median time performance of each individual participant. 
::: 
-->

We opt for a multiverse analysis [@Dragicevic:2019] to analyze the data, where we conduct a repeated-measures ANOVA with four different data-transformation methods:

1. *Log transformation (LOG).* Data are transformed with the logarithmic function. For our data, this is the most appropriate method as we drew samples from a log-normal distribution. 

2. *Aligned rank transformation (ART).* Data are transformed and analyzed with the ARTool [@wobbrock:2011;@elkin:2021].

3. *Pure rank transformation (RNK).* Data are transformed with the original rank transformation [@conover:1981], which does not require any data alignment.

4. *Inverse normal transformation (INT).* The data are transformed by using their normal scores. This rank-based method is simple to implement and has been commonly used in some disciplines. However, it has also received criticism [@Beasley:2009].  

For comparison, we also report the results of the regular parameteric ANOVA with no transformation (*PAR*). For each ANOVA analysis, we use a linear mixed-effects model, treating the participant identifier as a random effect. To simplify our analysis and like @elkin:2021, we consider random intercepts but no random slopes. For example, we use the following R code to create the model for the log-transformed response: 

```{r, echo=TRUE, warning=FALSE}
m.log <- lmer(log(Time) ~ Difficulty*Technique + (1|Participant), data = df)
```

The table below presents the *p*-values for the main effects of the two factors and their interaction: 

|        | PAR  | LOG  | ART | RNK | INT |
|--------|------|------|-----|-----|-----|
| Difficulty  | $1.8 \times 10^{-26}$ | $8.1 \times 10^{-47}$  |  $9.0 \times 10^{-43}$ | $4.3 \times 10^{-46}$ | $4.4 \times 10^{-44}$ |
| Technique   | $.10$ | $.18$  | $.00061$ | $.38$ | $.17$ |
| Difficulty $\times$ Technique | $.056$ | $.10$ | $.0017$ | $.24$ | $.23$ |
: *p*-values for main and interaction effects {.sm}

The disparity in findings between ART and the three alternative transformation methods is striking. ART suggests that all three effects are statistically significant. What adds to the intrigue is the fact that ART's *p*-values for *Technique* and its interaction with *Difficulty* are orders of magnitude lower than the *p*-values obtained from all other methods. We will observe similar discrepancies if we conduct contrast tests with the ART procedure [@elkin:2021], though we leave this as an exercise for the reader.

We also examine effect size measures, which are commonly reported in scientific papers. The table below presents results for partial $\eta^2$, which describes the ratio of variance explained by a variable or an interaction: 

|        | PAR  | LOG  | ART | RNK | INT |
|--------|------|------|-----|-----|-----|
| Difficulty  | $.64\ [.55, 1.0]$  | $.83\ [.79, 1.0]$  |  $.80\ [.76, 1.0]$ | $.83\ [.79, 1.0]$ | $.81\ [.77, 1.0]$  |
| Technique   | $.04\ [.00, 1.0]$ | $.03\ [.00, 1.0]$  | $.11\ [.03, 1.0]$ | $.02\ [.00, 1.0]$ | $.03\ [.00, 1.0]$ |
| Difficulty $\times$ Technique | $.10\ [.00, 1.0]$ | $.08\ [.00, 1.0]$  | $.16\ [.04, 1.0]$| $.06\ [.00, 1.0]$| $.06\ [.00, 1.0]$ |
: partial $\eta^2$ and its 95\% confidence interval {.sm}


We observe that ART exaggerates both the effect of *Technique* and its interaction with *Difficulty*. 


### Overview
The above example does not capture a rare phenomenon. We will show that ART's error inflation is systematic for a range of distributions that deviate from normality, both continuous and ordinal. We will explain how the problem emerges. We will also see that ART often performs worse than simpler methods that ART is widely considered to repair or improve, such as the pure rank transformation or no transformation at all. 

<!-- Even worse, while the rank transformation can inflate Type I and Type II errors on interactions, ART struggles to correctly infer both main and interaction effects.  -->

## Background {#background}

### Nonparametric statistics
Traditional analysis of variance (ANOVA) and t-tests are commonly known as *parametric* statistical procedures because they make assumptions about the underlying data distributions and their parameters, such as their means and standard deviations. In contrast, *nonparametric* statistical procedures make fewer or minimal assumptions about the underlying distributions. Representative examples of nonparametric methods are the Mann-Whitney U test [@Mann-Whitney-U] and Kruskal–Wallis test [@Kruskal-Wallis] for independent samples, and the Wilcoxon sign-rank test [@Wilcoxon] and Friedman test [@Friedman] for paired samples or repeated measures. These methods are widely used in research as they allow working with distributions of arbitrary shapes, while real data often deviate from the Gaussian model (normal distribution) or violate other assumptions, such as the homogeneity of variance (dependent groups) or the sphericity assumption (repeated measures). All these nonparametric tests are based on ranks, i.e., the rank order of observations. 

### Rank transformations
Common nonparametric methods pose significant analysis constraints compared to general linear models like ANOVA, as they only support simple experimental designs, e.g., containing a single independent variable (factor). Rank transformations aim to close the gap between nonparametric statistics and ANOVA.

**The rank transform.** The simple rank transform consists of sorting the raw observations and replacing them by their ranks. In case of ties, tied observations are assigned their average rank. For example, the two $0.3$ instances of the $Y$ responses in @fig-ART-explained (a), which are the lowest values in the dataset, receive a rank of $1.5$ (see $rank(Y)$), while the next value (a $0.4$) in the sequence receives a rank of $3$.

@conover:1981 showed that for one-factor designs, using ANOVAs on these ranks produces tests that are equivalent or good replacements of traditional nonparametric tests (see [Section 3](#interpretation)). However, a series of studies in the 80s raised caution flags on the use of the method, showing that it may confound main and interaction effects in two-factor and three-factor experimental designs. For an extensive review of these studies, we refer readers to @Sawilowsky:1990.

**The aligned rank transform.** Given these negative results, many researchers turned to aligned (or adjusted) rank transformations. @Sawilowsky:1990 discusses several variations of aligned rank-based transformations (ART) and tests, while @higgins:1990 detail the ART method that we evaluate in this paper for two-way designs. @wobbrock:2011 generalize it to more than two factors, while 10 years later, @elkin:2021 show how to apply it to multifactor contrast tests. @fig-ART-explained explains the general method for a design with two factors, $A$ and $B$.

![Example showing the use of the rank, ART, and INT transformation methods (a). Construction of ART for a two-factor design (b,c)](images/art-explained.svg){#fig-ART-explained width=80%}
         
The key intuition behind the transform is that responses are ranked after *aligning* them, such that effects of interest are separated from other effects. This means that for each effect of interest, either main or interaction, we produce a separate ranking. This is clearly demonstrated through the example dataset in @fig-ART-explained (a), where each aligned ranking ($ART_A$, $ART_B$, and $ART_{AB}$) is for testing a different effect ($A$, $B$, and $A \times B$, respectively).

Notice that even for this very small dataset ($n = 2$), these three rankings are distinct from each other, as well as very different from the ranking of the simple rank transform. Interestingly, equal responses (the two $0.3$ in our example dataset) may be assigned to very different ranks, while very different responses (such as $0.3$ and $3.0$ in our dataset) can be assigned to the same rank. Furthermore, differences between the two groups $a_1$ and $a_2$ within factor $A$ are significantly exaggerated by ART (see $ART_A$ ranks). We will later see that this behavior can lead to unstable results and conclusions, and is the source of the main problems of the method. For example, if we run a repeated-measures ANOVA on these ranks, we obtain a $p$-value $= .044$ for the effect of $A$. For comparison, if we run repeated-measures ANOVA on the ranks of simple rank transformation, we obtain a $p$-value $= .46$, while if we use a log-transformation that is more appropriate for these data, we obtain a $p$-value $= .85$.

@fig-ART-explained (b-c) details the calculation of the transformation, where we highlight the following terms: (i) residuals (in yellow) that represent the unexplained error (due to individual differences in our example); (ii) main effects (in green and pink) estimated from the observed means of the individual levels of the two factors; and (iii) interaction effect estimates (in blue). Observe that the estimates of the two main effects are subtracted from the interaction term. The objective of this approach is to eliminate the influence of main effects when estimating interaction effects. 

This is not the only alignment technique found in the research literature. @Sawilowsky:1990 suggests that at least for balanced designs, interactions could be removed when aligning main effects, in the same way main effects are removed when aligning interactions. This approach is also taken by @Leys:2010, who derived a common ranking for both main effects after subtracting the interaction term. We do not test these alternative alignment methods in this paper since at least in the HCI community, they are not commonly used. 

**The inverse normal transform.** 
A third transformation method that we evaluate is the rank-based inverse normal transformation (INT). INT is more than 70 years old [@VanDerWaerden:1952] and appears in several variations [@Beasley:2009;@Solomon2009]. Its general formulation is as follows: 

$$ 
INT(Y) = \Phi^{-1}(\frac{rank(Y) - c}{N - 2c + 1})
$${#eq-int}

where $N$ is the total number of observations and $\Phi^{-1}$ is the standard normal quantile function, which transforms a uniform distribution of ranks into a normal distribution. Different authors have used a different parameter $c$. In our experiments, we use the *Rankit* formulation [@Bliss:1956], where $c = 0.5$, since past simulation studies [@Solomon2009] have shown that it is more accurate than alternative formulations. However, as @Beasley:2009 report, the choice of $c$ is of minor importance. For our experiments, we implement the INT method in R as follows: 

```{r}
INT <- function(x){
	qnorm((rank(x) - 0.5)/length(x))
}
```

@fig-ART-explained (a) shows how this function transforms the responses for our example dataset. 

**Other non-parametric rank-based methods.** There are other rank-based statistical tests that deal with interactions, the ANOVA-type statistic (ATS) [@Brunner_ATS:2021] being the most representative one. @Kaptein:2010 introduced this method to the HCI community, advocating its application in the analysis of Likert-type data as a viable alternative to parametric ANOVA. In addition to ATS, @luepsen:2017, @luepsen:2018, and more recently @luepsen:2023 investigated several other multifactorial nonparametric methods. In particular, the author evaluated the hybrid ART+INT technique proposed by @Mansouri:1995, which applies INT on the ranks of ART. He also tested multifactorial generalizations of the van der Waerden test [@VanDerWaerden:1952] and the Kruskal-Wallis and Friedman tests [@Kruskal-Wallis;@Friedman]. The former is based on INT, but instead of using F-tests on the transformed values as part of ANOVA, it computes $\chi^2$ ratios over sums of squares. These two methods are not widely available --- an R implementation can be downloaded from Lupsen's web page [@luepsen_R]. 

### Experimental evaluations
Previous studies have assessed ART and related procedures through various Monte Carlo experiments, yielding conflicting results and conclusions.

**Results in support of ART**. The outcomes from numerous Monte Carlo experiments conducted in the 80s and 90s revealed the robustness of ART in testing interaction effects. Noteworthy instances include studies, such as those by @Salter:1993, which compared the method to parametric ANOVA. The authors found that ART remains robust even in the presence of outliers or specific non-normal distributions, such as the logistic, exponential, and double exponential distributions. Their findings indicated only a marginal increase in error rates (ranging from 6.0% to 6.3% instead of the expected 5%) when applied to the exponential distribution. Furthermore, ART demonstrated superior statistical power compared to parametric ANOVA. @Mansouri:1995 evaluated ART under a different set of non-normal distributions (normal, uniform, log-normal, exponential, double exponential, and Cauchy) in the presence of increasing main effects. Except for the Cauchy distribution, ART maintained Type I error rates close to nominal levels across all scenarios, irrespective of the magnitude of main effects. In contrast, the error rates of the rank transformation reached very high levels (up to $100\%$) as the magnitude of main effects increased, even under the normal distribution. ART only failed under the Cauchy distribution, which is well known to be pathological. 

More recently, @elkin:2021 compared ART to parametric t-tests for testing multifactor contrasts under six distributions: normal, log-normal, exponential, double exponential, Cauchy, and Student's t-distribution ($\nu=3$). Their results confirmed that ART keeps Type I error rates close to nominal levels across all distributions, except for the Cauchy distribution. In addition, they found that ART exhibits a higher power than the t-test. We emphasize, however, that the experimental methodoogy of @elkin:2021 did not allow for detecting the influence of a factor's main effect on the effect of a different factor or their interaction.

While most evaluation studies have focused on continuous distributions, @Payton:2006 have also studied how various transformations (rank, ART, log-transform, and squared-root transform) perform under the Poisson distribution, focusing again on interaction effects when main effects were present. The authors found that ART and parametric ANOVA (no transformation) performed best, keeping Type I error rates close to nominal levels. All other transformations inflated error rates.

**Warnings**. While the above results indicate that ART is a robust method, other studies have identified some serious issues. The second author of this paper has observed that, in certain cases, ART seems to detect spurious effects that alternative methods fail to identify [@casiez:2022]. Such informal observations, conducted with both simulated and real datasets, motivated us to delve deeper into the existing literature.

@Carletti:2005 report that *"aligned rank transform methods are more affected by unequal variances than analysis of variance especially when sample sizes are large."* Years later, @luepsen:2018 conducted a series of Monte Carlo experiments, comparing a range of rank-based transformations, including the rank transformation, ART, INT, a combination of ART and INT (ART+INT), and ATS. His experiments focused on a $2 \times 4$ balanced between-subjects design and a $4 \times 5$ severely unbalanced design and tested normal, uniform, discrete uniform (integer responses from 1 to 5), log-normal, and exponential distributions, with equal or unequal variances. Furthermore, they tested both interaction and main effects when the magnitude of other effects increased. The results revealed that ART inflates error rates beyond acceptable levels in several configurations: right-skewed distributions (log-normal and exponential), discrete responses, unequal variances, and unbalanced designs. @luepsen:2018 also found that using INT in combination with ART (ART+INT) is preferable to the pure ART technique. However, as the method still severely inflated error rates in many settings, @luepsen:2018 concluded that both ART and ART+INT are *"not recommendable."* 

Another interesting finding of @luepsen:2018 was that the simple rank transformation *"appeared not as bad as it is often described"* [@luepsen:2018], outperforming ART in certain configurations, such as discrete and skewed distributions, or unequal variances. These results are in full contradiction with the the findings of @Mansouri:1995.

The same author conducted an additional series of experiments [@luepsen:2017], focusing on two discrete distributions (uniform and exponential) with a varying number of discrete levels: 2, 4, and 7 levels with integer values for the uniform distribution, and 5, 10, and 18 levels with integer values for the exponential distribution. Again, the Type I error rates of both ART and ART+INT reached very high levels, but error rates were especially pronounced when the number of discrete levels became small and the sample size increased. The author provides a detailed analysis of why ART fails in these cases, summarized as follows:

> *"There is an explanation for the increase of the type I error rate when the number of distinct values gets smaller or the sample size larger: due to the subtraction of the other effects --- a linear combination of the means --- from the observed values even tiny differences between the means lead to large differences in the ranking"* [@luepsen:2017].

Given these results, the author's conclusion was the following: 

> *"the ART as well as the ART+INT cannot be applied to Likert and similar metric or ordinal scaled variables, e.g. frequencies like the number of children in a family or the number of goals, or ordinal scales with ranges from 1 to 5"* [@luepsen:2017]. 

**Results on other rank-based methods.** We are also interested in understanding how ART compares with INT, which is frequently used in some research domains, such as genetics research [@Beasley:2009]. @Beasley:2009 conducted an extensive evaluation of the method and reached the conclusion that *"INTs do not necessarily maintaint proper control over Type 1 error rates relative to the use of untransformed data unless they are coupled with permutation testing.* @luepsen:2018 included INT in his evaluation and found that in most cases, it maintained better control of Type I error rates compared to ART and the pure rank transformation, while also presenting a higher statistical power. However, he also identified several cases where INT failed to sufficiently control for Type I errors, such as design configurations with unequal variances in unbalanced designs or with skewed distributions, and when testing interactions when both mains effects were nonnull. 

In addition to INT, @luepsen:2018 evaluated the ATS method but found it to suffer from low power while presenting similar challenges as the rank transformation with regard to Type I error rates. Amongst all evaluated methods, the author identified the generalized van der Waerden test as the method that provided the best overall control of Type I error rates. More recently, @luepsen:2023 conducted a new series of experiments testing rank-based nonparametric methods on split-plot designs with two factors. While the author reported on various tradeoffs of the methods, he concluded that overall, the generalized van der Waerden test and the generalized Kruskal-Wallis and Friedman tests were the best performing methods.

### The use of ART in experimental research {#ART-use}
[F: Pesent the survey of papers using ART here?]{.f}

To analyze how the ART is used in experimental research, we collected in November 2023 the 39 most recent papers written in English and citing [@wobbrock:2011] on Google Scholar. The papers were published in Bio, Education, HCI, Haptic, Medecine, Neuroscience, VR domains. 64.1% of the papers reported experiments using a within-participants design, 7.7% a between-participants design and 28.2% a mixed-participants one. The number of factors ranged from 1 to 5, with 2 factors representing 64.1% of the experiments (1: 12.8%, 3: 15.4%). For 2 factors, the number of levels ranged between 2 and 15, with 2 levels being the most widely used (58.0%), followed by 3 (22.0%). The studies had between 12 and 468 participants (median = 24.0), with subgroups ranging from 8 to 48 (median = 20.0). For dependent variables, 51.3% of the studies used ratio variables and the remaining ordinal variables.  94.9% of the studies found at least one significant main effect in the results, using the ART. 80.0% of the studies studying the interaction between factors using the ART found at least one significant interaction. Only 12.8% of the papers made the data of their studies publicly available.

### Positioning our work
We see that ART is frequently used for the analysis of experimental results. Regrettably, the cautions raised by @luepsen:2017 and @luepsen:2018 have been widely overlooked, and ART is commonly applied to datasets, including Likert data with five or seven levels, where the method has been identified as particularly problematic. Given the contradictory findings in the existing literature, researchers grapple with significant dilemmas regarding which past recommendations to rely on.

Our goal is to verify Luepsen's findings by employing an alternative set of experimental configurations and a distinctly different methodology. In particular, we adopt a latent variable modeling approach that establishes a unified framework for data generation across all distributions. This approach allows us to demonstrate the shortcomings of rank-based approaches in effectively addressing interactions through the lens of removable interactions [@Loftus:1978]. Moreover, it facilitates the assessment of the methods through more suitable generation procedures for ordinal data, as suggested by @Liddell:2018.

With a focus on clarity, we chose to exclusively examine the three rank-based transformations presented earlier: the pure rank transformation (RNK), ART, and INT. While we do not elaborate on the performance of ATS in the main paper, additional experimental results are available in the supplementary materials. These findings indicate that its performance is comparable to the rank transformation but seems to be inferior to the simpler and more versatile INT. Our supplementary materials feature additional results regarding the performance of the generalized van der Waerden test, as well as the generalized Kruskal-Wallis and Friedman tests. Our findings do not support the conclusions of @luepsen:2018 and @luepsen:2023, as they indicate an exceptionally low power for these methods. Lacking compelling evidence of their potential strengths, we refrain from further discussion on these tests.

Finally, we chose to limit our investigation to balanced experimental designs. The rationale behind this decision is that we have not encountered any prior claims about ART's suitability for unbalanced data, and the ARTool [@artool] issues a warning in such situations. However, we present several case studies in Section 6, where data are often unbalanced.

## Interpreting effects {#interpretation}
The ART procedure was proposed as an alternative to the rank transformation [@conover:1981] for testing interactions. As @higgins:1990 explained, the rank transformation is non-linear and, as a result, it changes the structure of interactions. Therefore, *"interaction may exist in the transformed data but not in the original data, or vice versa"* [@higgins:1990]. @fig-interactions-rank demonstrates the problem through a representative example. In this example, the data have been sampled from perfectly normal distributions with equal variances. 

::: {#fig-interactions-rank}
```{r, echo=FALSE, message=FALSE, fig.height=3, fig.width = 7, warning=FALSE}
source("interactions_plot.R")

df <- read.csv("interactions_rank.csv", sep=",", header=TRUE, strip.white=TRUE)
df$Difficulty <- ordered(df$Difficulty, levels = c("easy", "medium", "hard"))
df_aggr <- aggregate(Y ~ Difficulty+Technique, data = df, mean)
df_rank <- aggregate(rank(Y) ~ Difficulty+Technique, data = df, mean)
colnames(df_rank) <- c("Difficulty", "Technique", "Y")

fig1 <- createRankInteractionPlot(df_aggr)
fig2 <- createRankInteractionPlot(df_rank, rnkscale = TRUE)
fig <- subplot(fig1, fig2, titleY = TRUE, titleX = TRUE, margin = 0.05, widths = c(0.5, 0.5)) %>% 
	        layout(margin = list(l = 40, r = 0, b = 0, t = 50, pad = 0))

fig
```
Visualization of interaction effect for a 3 $\times$ 2 experimental design before and after applying a rank transformation on an [example dataset](interactions_rank.csv) (within-participants design, $n = 20$). All data points represent means.
:::

We observe that while no interaction effect appears in the original data (lines are parallel), the rank transformation causes the trends to slightly change. In particular, differences are more pronounced for the middle points of the three-level factor ("medium difficulty"). This problem emerges when the main effect is strong on both factors. 

<!--  appear on all interacting factors [This sentence is not very clear to me]{.g}. The trend deformation is weak, so the problem may only become apparent in studies with high statistical power, when main effects and/or sample sizes are sufficiently large. [Gives the impression this is not a major problem]{.g}  -->

ART aims to correct this problem. However, non-linear transformations come into place in various ways in experimental designs [@Loftus:1978; @Wagenmakers:2012]. They can deform distributions, making the interpretation of observed effects especially challenging. Before presenting our experimental method and results, we discuss these problems and explain how our approach takes them into consideration. 

### What is the null hypothesis of interest?
To compare different statistical methods, we first need to assess whether these methods are comparable. If two methods are not designed to test the same null hypothesis, then making direct comparisons between them could be misleading. Let us elucidate this problem.  

**ANOVA and nonparametric tests.** The traditional ANOVA is used to test differences between two or more means. However, nonparametric tests often target other population parameters. For example, the Wilcoxon sign-rank test is commonly described as a test of medians for paired samples [@McDonald:2014] and is used when population means are not of interest, e.g., when population distributions are skewed. The Mann-Whitney U and the Kruskal–Wallis tests are used, instead, to assess whether two or more independent samples come from the same population, or more technically, whether the mean ranks of the groups are the same. They can be only interpreted as tests of medians under the strict assumption that the population distributions of all groups have identical shapes and scales [@Divine:2018]. 

**Rank transformations.** Interpreting the null hypothesis of interest of a rank transformation is more challenging. @conover:1981 show that the simple rank transformation procedure *RNK* is equivalent to the Mann-Whitney U and Kruskal–Wallis tests for independent samples. For paired samples, however, it results in a new test, which is different than the Wilcoxon sign-rank test and different than Friedman's test. Defining the null hypothesis of interest of ART is even more challenging because of the hybrid nature of the method. In particular, while ART is a rank-based transformation procedure, it aligns data with respect to means, where alignment is performed independently for each group.

**Dealing with the interpretation of main effects.** To partly avoid these interpretation issues, we focus on effects that apply monotonic transformations to population distributions. This also ensures a monotonic relationship between different measures of central tendency such as medians and means (with the exception of the Cauchy distribution, where the mean is undefined). In other words, if a treatment increases the population mean, it will also increase the population median. We present an example in @fig-distributions. The figure shows two population distributions corresponding to the two intermediate levels of difficulty of our illustrative example (see @fig-example). We observe that the increased difficulty of the task translates both the population mean and the median to the right. In this case, we expect a statistical test to reject the null hypothesis, no matter whether it tests the population mean, the median, or the overall distribution shape.   

::: {#fig-distributions}
```{r, echo=FALSE, message=FALSE, fig.height=3, fig.width = 9, warning=FALSE}
library(plotly)

palette <- c("#E69F00", "#009E73")

logsd <- 0.5
logm1 <- -0.3
logm2 <- 0.3

xs <- seq(0, 10, length.out = 1000)
y1s <- dlnorm(xs, logm1, logsd)
y2s <- dlnorm(xs, logm2, logsd)

m1 <- exp(logm1 + (logsd^2)/2)
m2 <- exp(logm2 + (logsd^2)/2)
med1 <- exp(logm1)
med2 <- exp(logm2)

anot_m1 <- list(x = m1, y = 0, text = "mean", xref = "x", yref = "y",
  showarrow = TRUE, arrowhead = 6, arrowsize = 1, arrowcolor = palette[1],
  ax = 0, ay = -40
)

anot_med1 <- list(x = med1, y = 0, text = "median", xref = "x", yref = "y", 
  showarrow = TRUE, arrowhead = 7, arrowsize = 1, arrowcolor = palette[1],
  ax = 0, ay = 30
)

anot_m2 <- list(x = m2, y = 0, text = "mean", xref = "x", yref = "y",
  showarrow = TRUE, arrowhead = 6, arrowsize = 1, arrowcolor = palette[2],
  ax = 0, ay = -43
)

anot_med2 <- list(x = med2, y = 0, text = "median", xref = "x", yref = "y", 
  showarrow = TRUE, arrowhead = 7, arrowsize = 1, arrowcolor = palette[2],
  ax = 0, ay = 30
)

annot1 <- paste("Log-normal: meanlog = ", logm1, ", sdlog = ", logsd)
annot2 <- paste("Log-normal: meanlog = ", logm2, ", sdlog = ", logsd)

fig <- plot_ly() %>%  
      add_lines(x = xs, y = y1s, visible = TRUE, type = 'scatter', mode = 'lines', text = annot1, hoverinfo = 'text', line=list(color=palette[1])) %>%
      add_lines(x = xs, y = y2s, visible = TRUE, type = 'scatter', mode = 'lines', text = annot2, hoverinfo = 'text', line=list(color=palette[2])) %>%      
      add_annotations(x = med1, y = max(y1s), text = "Difficulty Level 2", xref = "x", yref = "y", showarrow = F, font = list(color = palette[1]), xanchor = 'left') %>% 
      add_annotations(x = med2, y = max(y2s), text = "Difficulty Level 3", xref = "x", yref = "y", showarrow = F, font = list(color = palette[2]), xanchor = 'left') %>% 
      layout(annotations = anot_m1) %>% layout(annotations = anot_med1) %>%      
      layout(annotations = anot_m2) %>% layout(annotations = anot_med2) %>%    
      layout(
        showlegend = FALSE,
        xaxis = list(title = "Time (min)", range = c(0, 4.5), showgrid = F, showticks = T, ticks="outside", zeroline = F), 
        yaxis = list(showgrid = F, showticklabels = F, showline = F, range=c(0, max(y1s) + .05), fixedrange=T)
      ) %>% 
	      config(displayModeBar = TRUE, scrollZoom = FALSE, displaylogo = FALSE, modeBarButtonsToRemove = c("lasso2d", "select2d",  "zoomIn2d", "zoomOut2d", "autoscale", "hoverclosest", 'hoverCompare'))

fig

```
Time distributions for two task populations with difficulty levels 2 and 3 (see @fig-example). 
:::

Nevertheless, the increased difficulty of the task does not simply translate the distribution to the right. The shape and scale of the distribution also change --- the variance increases, and the mean and median do not increase by the same amount. Unfortunately, this poses a probem for ART's alignment procedure, which relies entirely on means. In particular, the more extreme values of a random sample that appear near the further right of the wider distribution (depicted in green in @fig-distributions) will have a significant impact on the calculation of the within-cell mean. This will lead to the exaggeration of all ranks within this cell. However, an additional issue arises. Such non-linear transformations complexify the interpretation of interactions.

### Interaction interpretation problems

Let us take a different [dataset](removable_interactions.csv) from a fictional experiment (within-participants design with $n = 24$) that evaluates the performance of two techniques (*Tech A* and *Tech B*) under two task difficulty levels (*easy* vs. *hard*). The experiment, for example, could test a mobile typing task, where the levels of difficulty correspond to texts of different lengths (*short* vs. *long*) under two typing techniques (*with* vs. *without auto-completion*). We assume that the researchers measure two dependent variables: task-completion time and perceived performance, which is measured through a five-level ordinal scale (from "very quick" to "very slow"). In this example, the main effects of task difficulty and technique are large. What is less clear, however, is whether there is an interaction between the two factors. 

**The problem of different scales.** @fig-interactions visualizes the means for each combination of the levels of the factors and highlights the possible interactions. Let us first concentrate on the first two plots that present results for the time measure. The trends in the left plot indicate an interaction effect, since the two lines seem to diverge as the task difficulty increases. 

::: {#fig-interactions}
```{r, echo=FALSE, message=FALSE, fig.height=3.5, fig.width = 9, warning=FALSE}
df <- read.csv("removable_interactions.csv", sep=",", header=TRUE, strip.white=TRUE)
dftime <- aggregate(Time ~ Difficulty+Technique, data = df, mean)
dfpref <- aggregate(PerceivedPerformance ~ Difficulty+Technique, data = df, mean)

fig1 <- createInteractionPlot(dftime)
fig2 <- createInteractionPlot(dftime, logscale = TRUE)
fig3 <- createInteractionPlot(dfpref, likert = TRUE)
#fig <- subplot(fig1, fig2, fig3, titleY = TRUE, titleX = TRUE, margin = 0.08, widths = c(0.32, 0.36, 0.32))

fig <- subplot(fig1, fig2, titleY = TRUE, titleX = TRUE, margin = 0.06) %>% subplot(fig3, titleY = TRUE, titleX = TRUE, margin = 0.07, widths = c(0.66, 0.33)) 

fig
```
The line charts visualize the effects of task difficulty (*easy* vs. *hard*) and technique (*Tech* A vs. *Tech B*) for two measures: task completion time (left and middle) and perceived performance (right). All data points represent group means. 
:::

But how meaningful is this interpretation of interaction? Time measurements are often taken from distributions of different scales, that is, large effects are harder to observe in quick tasks than in slow ones. For example, performance differences in sprint races are in the range of milli- or centiseconds, while differences in long-distance races can be in the range of several seconds or minutes. So if we compare the time performance of any two groups of people (e.g., 14- vs. 12-year-old children), we will always find that absolute differences grow as race distance increases. However, such trends do not necessarily reveal any real interactions, because they are simply due to observations at different time scales. This issue is not specific to running races. @Wagenmakers:2007 show that the standard deviation of response times increases linearly with their mean. This relationship is depicted in @fig-example and @fig-distributions, where the mean and the spread of the time distributions grow together as task difficulty increases. 

In our example in @fig-interactions, each task (typing a piece of text) is a sequence of elementary tasks (typing a word). We thus expect both means and standard deviations to grow as a function of the number of words in the text. In this case, meaningful interaction effects (e.g., Tech B suffers from more intense fatigue effects in longer texts) will be manifested as growing *time ratios* (i.e., as proportional differences) --- not as growing absolute time differences. An easy way to visually assess the presence of such interactions is to show time on a logarithmic scale, as shown in @fig-interactions (middle). Notice that the lines in the plot are now almost parallel, suggesting no interaction effect.

**Removable interactions.** The concept of *removable interactions*, that is, interactions that disappear after applying a monotonic non-linear transformation, was introduced by @Loftus:1978.  Over three decades later, @Wagenmakers:2012 revisited this work and found that psychology researchers are largely unaware of the concept, drawing incorrect conclusions about psychological effects on the basis of meaningless interactions. This issue also extends to data collected from questionnaires. The right plot in @fig-interactions shows results for perceived performance. Again, the line trends suggest an interaction effect. Unfortunately, the scale is ordinal, which means that distances between the five levels of the scale may not be perceived as equal by people. Furthermore, the scale is bounded, so the reason that the two lines are not parallel might be simply due to the absence of additional levels beyond the extreme "very slow" ranking. Concluding that there is a meaningful interaction here could be incorrect. @Liddell:2018 extensively discuss how ordinal scales deform interactions.

**Formal testing.** We now formally test the above interactions by using ANOVA with different transformation methods. Below, we present the *p*-value returned by each method for task-completion time: 

| PAR  | LOG  | ART | RNK | INT |
|------|------|-----|-----|-----|
| $.023$ | $.67$ | $.00073$ | $.66$ | $.67$ |
: *p*-values for interaction effect on task-completion time {.sm}

We observe that *RNK* and *INT* lead to *p*-values very close to the *p*-value of *LOG*, which suggests a similar interpretation of interaction effects. In contrast, ART returns a very low *p*-value (lower than the *p*-value of the regular ANOVA), showing that the method is extremely sensitive to scale effects.  

We also test the interaction effect on the ordinal dependent variable:

| PAR  | ART | RNK | INT | ATS |
|------|-----|-----|-----|-----|
| $.0020$ | $.00075$ | $.0067$ | $.0037$ | $.0081$ |
: *p*-values for interaction effect on perceived performance {.sm}

Notice that we omit the log-transformation method (*LOG*), as it is not relevant here. We conduct instead an analysis with the nonparametric ATS method [@Brunner_ATS:2021] as implemented in the R package *nparLD* [@nparLD]. All *p*-values are low, suggesting that an interaction effect exists. However, if we conduct a more appropriate analysis using an ordered probit model [@Liddell:2018; @Burkner:2019], we will reach the conclusion that there is no supportive evidence for such an effect (check our analysis in the supplementary material). We return to these models in later sections. An important observation here is that nonparametric procedures are not the answer to such problems.  

### Approach {#approach}
Our analysis shows that inference errors are not simply due to the lack of robustness of a statistical procedure. In the case of interaction effects, errors will also emerge when the procedure makes inference on the wrong scale. As @Wagenmakers:2012 explain, *"the dependent variable reflects merely the output of a latent psychological process",* and unfortunately, *"in most experimental paradigms the exact relationship between unobserved process and observed behavior is unknown ..."* 

Ideally, a statistical procedure should lead to conclusions that capture the true effects on the latent variable of interest. But as we discussed above, this might not be the case for the rank transformation methods that we study. For example, all four methods (*PAR*, *RNK*, *ART*, and *INT*) suggest that task difficulty interacts with use of technique on perceived performance because they disregard the fact that the observed data are simply projections on a discrete ordinal scale. Our goal is to understand how ART and the other methods deal with such problems. 

**Latent variables.** We assume that there is a latent variable $Y$ of interest that is different than the variable we observe. For example, a latent variable may represent the performance potential of a population of people, their working memory capacity, their perceived utility of a new technology, or their quality of life. For convenience, we assume that the latent variable is continuous and normally distributed. This assumption is common in latent variable modeling, e.g., in diffusion-decision models that predict response time and error in two-choice decision tasks [@Ratcliff_diffusion:2008], and ordinal models [@Liddell:2018].

**Observed variables.** Then, all dependent variables $Y'$ that we observe are derived from this latent variable through a monotonic transformation, thus $Y' = \mathcal{T}(Y)$, where $\mathcal{T}(y_1) \le \mathcal{T}(y_2)$ if and only if $y_1 \le y_2$. A transformation for example occurs when study participants perform a selection task or repond to a Likert-scale item through a questionnaire. @fig-conversions shows how we transform normal distributions to log-normal, binomial, and ordinal scales. 

::: {#fig-conversions}
```{r, echo=FALSE, message=FALSE, fig.height=3, fig.width = 9, warning=FALSE}
source("conversions_plot.R")

n <- 2000

xs <- c(rnorm(n, mean = 1.5, sd = 1), rnorm(n, mean = -1.5, sd = 1))

ys1 <- norm2lnorm(xs, meanlog = 0, sdlog = 1, mu = 0, sd = sd(xs))
plot1 <- plotConversion(xs, ys1, ylab = "Log-normal scale", ymax = 10)

ys2 <- norm2binom(xs, size = 10, prob=.1)
plot2 <- plotConversion(xs, ys2, ymin = - 0.3, ylab = "Binomial scale", as.density = F)

ys3 <- toOrdinal(xs, thresholds = c(-2.5, -1, 0, 2.5))
plot3 <- plotConversion(xs, ys3, ymin = 0.7, ymax = 5.3, ylab = "Ordinal scale", as.density = F)

fig <- cowplot::plot_grid(plot1, plot2, plot3, ncol= 3) 

fig
```
Transformation of normal latent variables to other continuous or discrete scales: log-normal (left), binomial (middle), and ordinal (right).  
:::

To transform the latent variable to a ratio scale (e.g., a log-normal and binomial scale), we adopt the distribution conversion approach of *faux* v1.2.1 [@faux], an R package for experimental simulations. We first derive the cumulative density distribution of the latent variable. We then use the inverse quantile function of the target distribution to derive the observed variable. For example, in @fig-conversions (left), where we transform the latent variable to a log-normal scale with parameters $\mu = 0$ and $\sigma = 1$, we use the following R function:  

```{r}
norm2lnorm <- function(x, meanlog = 0, sdlog = 1, mu = mean(x), sd = sd(x), ...) {
	 p <- pnorm(x, mu, sd) 
 	 qlnorm(p, meanlog, sdlog, ...) 
}
```
For the binomial scale of @fig-conversions (left), we use instead the inverse quantile function of the binomial distribution ```qbinom(p,size,prob)``` with parameters ```size = 10``` and ```prob = .1```, which respectively represent the number of Bernoulli trials and their success probability.

To transform the latent variable to an ordinal scale, we implement an ordered-probit model, as explained by @Liddell:2018. According to this model, we discretize the latent variable with thresholds that determine the ordinal levels of interest. For our example in @fig-conversions (right), we use as threshold the values $(-2.5, -1, 0, 2.5)$, defining an ordinal scale of five levels. Observe that these thresholds are not equidistant. 

**Interpreting effects.** Our approach allows us to simulate main and interaction effects on the latent variable $Y$ and observe them on the transformed variable $Y' = \mathcal{T}(Y)$. As discussed earlier, how to infer main effects is straightforward since we only study monotonic transformations here. Specifically, if we observe a main effect on the observed variable $Y'$, we can also conclude that there is a main effect on the latent variabe $Y$.

However, depending on how the statistical procedure we use deals with different scales, observations of interactions among two or more factors on the transformed variable $Y'$ may lead to incorrect conclusions about the presence of interactions on the latent space of $Y$. Such errors only emerge when the main effect of all interacting factors on the latent (or observed) variable is non-zero. In our analysis, we count them as statistical inference errors (Type I and II errors). Nevertheless, we try to distinguish them from typical inference errors, for which interaction interpretation issues do not come into place.

## Experimental method {#methodology}
We can now detail our experimental method. We evaluate the standard parametric approach (*PAR*) and the three rank-transformation methods (*RNK*, *INT*, and *ART*) that we introduced earlier. We conduct a series of Monte Carlo experiments that assess their performance under a variety of experimental configurations:

1. We evaluate *ratio* and *ordinal* data. For ratio data, we examine four representative *continuous* distributions (normal, log-normal, exponential, and Cauchy distribution) and two *discrete* distributions (Poisson and binomial distribution). For ordinal data, we examine distributions for 5- and 7-level Likert item responses, as well as responses with 20 levels, as they are typically used in NASA-TXL evaluation questionnaires.

2. We present results for five experimental designs. To simplify our presentation, we start with a 4 $\times$ 3 repeated-measures factorial design. We then show how our conclusions generalize to four additional designs: (i) a 2 $\times$ 3 between-subjects design; (ii) a 2 $\times$ 4 mixed design, with a between-subjects factor and a repeated-measures factor; (iii) a 2 $\times$ 2 $\times$ 2 repeated-measures design, and (iv) a 3 $\times$ 3 $\times$ 3 repeated-measures design.

3. We evaluate three sample sizes, $n=10$, $n=20$, and $n=30$, where $n$ represents the cell size in an experimental design. For within-subjects designs where all factors are treated as repeated measures, $n$ coincides with the number of subjects (or commonly human participants in HCI research). 

4. We test the robustness of the methods under unequal variances. 

5. In addition to Type I error rates, we compare the statistical power of the methods and also evaluate the quality of their effect size estimates.  

6. We assess the above measures for both main and interaction effects and examine how growing effects on one or two factors affect the Type I error rate on other factors and their interactions. Furthermore, we assess how a growing interaction effect, either in isolation or in combination with a main effect, can affect the inference of main effects.

Previous evaluations of rank transformation methods [@Beasley:2009; @luepsen:2018] have also tested unbalanced designs, where the cell size is not constant across all levels of a factor. When combined with unequal variances, unbalanced designs are often problematic for both parametric procedures [@blanca:2018] and rank transformation methods [@Beasley:2009; @luepsen:2018]. However, to simplify our analysis, we do not consider them here.

### Statistical modeling
To model the latent variable $Y$, we use a two-way (two factors) or a three-way (three factors) mixed-effects model. For simplicity, we explain here the model for two factors. Its extension to three factors is straightforward. The model has the following form:

$$ 
y_{ijk} = \mu + s_k + a_1 x_{1i} + a_2 x_{2j} + a_{12} x_{1i} x_{2j} + \epsilon_{ijk}
$${#eq-linear-model}

 - $\mu$ is the grand mean

 - $s_k$ is the random intercept effect of the *k*-th subject, where $k = 1..n$ 

 - $x_{1i}$ is a numerical encoding of the *i*-th level of factor $X_1$, where $i = 1..m_1$

 - $x_{2j}$ is a numerical encoding of the *j*-th level of factor $X_2$, where $j = 1..m_2$

 - $a_1$, $a_2$, and $a_{12}$ express the magnitude of main and interaction effects 

 - $\epsilon_{ijk}$ is the experimental error effect

To encode the levels of the two factors $x_{1i} \in X_1$ and $x_{2j} \in X_2$ we proceed as follows: 

1. We normalize the distance between their first and their second levels such that $x_{12} - x_{11} = 1$ and $x_{22} - x_{21} = 1$. This approach enables us to conveniently control for the main and interaction effects by simply varying the parameters $a_1$, $a_2$, and $a_{12}$.

2. For the remaining levels, we randomly sample from a uniform distribution that spans the range between these two extreme levels, i.e., between $x_{11}$ and $x_{12}$ for $X_1$, and between $x_{21}$ and $x_{22}$ for $X_2$. This approach allows us to generate and evaluate a substantial variety of configurations, each representing different relative effects between levels.  

3. We require all levels to sum up to 0, or $\sum\limits_{i=1}^{m_1}  x_{1i} = 0$ and $\sum\limits_{j=1}^{m_2}  x_{2j} = 0$, which ensures that the grand mean is $\mu$.

For example, we can encode a factor with four levels as $\{-.6, .4, .1, .1\}$ or as $\{-.5, .5, .3, -.3\}$. 

While random slope effects can have an impact on real experimental data [@barr:2013], we do not consider them here for two main reasons: (1) to be consistent with previous evaluations of the ART procedure [@elkin:2021]; and (2) because mixed-effects procedures with random slope effects are computationally demanding, adding strain to simulation resources. However, there is no good reason to believe that adding random slope effects would impact our findings and conclusions.

### Population control and distribution conversions 
To simplify our simulations, we fix the following population parameters: $\mu = 0$, $\sigma = 1$, and $\sigma_s = 1$. We then control the magnitude of effects by varying $a_1$, $a_2$, and, for some experiments $a_{12}$. @fig-effects presents the range of values that we test for $a_1$ and $a_2$. We also visualize their effects on the latent variable for factors with two, three, and four categorical levels. 

::: {#fig-effects}
```{r, echo=FALSE, message=FALSE, fig.height=3, fig.width = 9, warning=FALSE}
source("effects_plot.R")
plotEffects()
```
Varying $a_1$ or $a_2$ to control the magnitude of main effects. The plots show examples of population distributions on the latent variable $Y$ for factors ($X_1$ or $X_2$) with two (top), three (middle), or four categorical levels (bottom).    
:::

We follow the approach of @DeBruine:2021 and use the R package *faux* v1.2.1 [@faux] to simulate data for our mixed-effects models. We assume that the distributions of random intercepts and errors are normal, or $s_k \sim N(0,\sigma_s)$ and $\epsilon_{ijk} \sim N(0,\sigma)$.
To simulate the observed variable $Y'$, we then transform the response values $y_{ijk}$ as described in [Section 3](#approach). A key advantage of this method is that we can generate responses for any distribution, while we control effects on the latent variable in the exact same way.

### Implementation of rank transformation methods 
For the aligned rank transformation (ART), we use the R implementation of ARTool v0.11.1 [@artool]. For the pure rank transformation (RNK), we use R's *rank()* function. We use the *Rankit* formulation [@Bliss:1956] for the inverse normal transformation (INT), as explained earlier.

### Evaluation measures
Significance tests have two types of errors. *Type I errors*, or false positives, are mistaken rejections of the null hypothesis. Type II errors, of false negatives, are failures to reject a null hypothesis that is actually true. In our illustrative example in @fig-example, a Type I error is finding that there is an effect of the choice of the technique on time performance. A *Type II error*, in turn, is finding that the task difficulty has no effect on time performance. 

Statistical significance testing requires setting a significance threshold known as significance or $\alpha$ (alpha) level, with typical values $\alpha = .05$ and $\alpha = .01$. The Type I error rate of a well-behaved significance test should be close to this nominal alpha level. An error rate clearly above this level suggests that the significance test is too liberal, while an error rate clearly below this level suggests that the test is too conservative. Four of our experiments specifically assess the Type I error rate of the methods. We test two significance levels: $\alpha = .05$ and $\alpha = .01$. For brevity, we only report results for $\alpha = .05$ in the main paper and include additional results in our supplementary material. 

We do not directly evaluate Type II errors. Instead, we report on statistical *power* defined as $Power = 1 - \beta$, where $\beta$ is the rate of Type II errors. Significance tests do not provide any power guarantees. However, we can compare the power of different methods to evaluate their relative performance. In addition to power, we assess effect size estimates, focusing on the partial $\eta^2$ measure, which is commonly used in ANOVA. To evaluate the quality of estimates of different methods, we measure how well they correlate with the estimates of the optimal ground-truth method, e.g., parametric ANOVA over the normal latent variable. 

As we explained earlier, interaction effects are deformed when we transform the latent variable $Y$ to obtain the observed responses. Such transformations also affect the Type I error rates and power that we observe. Here, we are interested in evaluating these measures with respect to the effects we apply to the latent variable. We make this distinction whenever it is relevant in our analysis.  

### Hardware platform and iterations
Our experimental R code is available in our supplementary material. We ran our experiments seperately in a cluster of 8 machines Dell R640 Intel Xeon Silver 4112 2.6GHz with 4 cores and 64 GB memory. Our R code was parallelized to use all four cores of each machine. Some experiments took a few hours to complete, while others took several days. 

To estimate the power and Type I error rates of the four methods with enough precision, we ran $5000$ iterations for each population configuration and each sample size. 

<!--
We evaluate the Type I error rate, that is the rate of false positives of each method. To evaluate the Type I error rate for the interaction effect, we set $a_{12} = 0$. Likewise, to evaluate the Type I error rate for the second factor $X_2$, we set $a_{2} = 0$. For each population configuration and sample size $n$, we run $5000$ iterations and test two significance levels: $\alpha = .05$ and $\alpha = .01$. For brevity, we only report results for $\alpha = .05$ in the main paper, while additional results are presented in supplementary materialsxa
-->

## Results
Each experiment concentrated on a unique combination of distributions, experimental designs, and evaluation measures. Consequently, we organize our results into several subsections, addressing both main and interaction effects.

### Type I error rates in ratio scales {#ratio}
Our first experiments evaluate Type I error rates for ratio scales. We test a 4 $\times$ 3 repeated-measures design, where we refer to the 4-level factor as $X_1$ and the 3-level factor as $X_2$. For the observed response variable $Y' = \mathcal{T}(Y)$, we evaluate transformations to four continuous and two discrete distributions:  

1. No transformation, or $Y' = Y$. Distributions are normal with a grand mean $\mu = 0$ (e.g., see @fig-effects). 

2. Log-normal distribution $LogN(\mu, \sigma)$ with global parameters $\mu = 0$ and $\sigma = 1$. As discussed in [the introduction](#intro), the log-normal distribution is a good model for various measures bounded by zero, such as task-completion times.

3. Exponential distribution $Exp(\lambda)$ with a global parameter $\lambda = 2$. The exponential distribution naturally emerges when describing time elapsed between events. For example, we could use it to model the time a random person spends with a public display, or the waiting time before a new person approaches to interact with the display, when the average waiting time is $\frac{1}{\lambda}$. 

4. Cauchy distribution $Cauchy(x_0,\gamma)$ with global parameters $x_0 = 0$ and $\gamma = 1$. The Cauchy distribution is the distribution of the ratio of two independent normally distributed random variables. It rarely emerges in practice. However, it is commonly used in statistics to test the robustness of statistical procedures because both its mean and variance are undefined. As we discussed earlier, past evaluations of ART [@Mansouri:1995; @elkin:2021] show that the method fails under the Cauchy distribution.

5. The Poisson distribution $Pois(\lambda)$ with a single parameter $\lambda = 3$. It expresses the probability of a given number of events in a fixed interval of time. For example, we could use it to model the number of people who interact with a public display in an hour, when the average rate is $\lambda = 3$ people per hour.

6. The binomial distribution $B(n,p)$ with parameters $n = 10$ and $p=.1$. It frequently appears in HCI research, as it can model the number of successes and failures in a series of experimental tasks. For example, we could use it to model the number of errors that participants make in a series of $n = 10$ repetitions of a memorization task, when the average error rate is $10\%$, thus the average error probability is $p=.1$. 

**Main effects**. @fig-ratio-main presents Type I error rates for the main effect of $X_2$ as the magnitude of the main effect of $X_1$ increases. The results show a very good behavior for RNK and INT across all distributions. The regular parametric ANOVA (PAR) keeps error rates below $5\%$. However, error rates become extremely low for some distributions, suggesting a loss in statistical power. We confirm previous results that ART fails to control the Type I error rate under the Cauchy distribution [@Mansouri:1995; @elkin:2021]. However, we also show that the method can be problematic with other non-normal distributions. As the main effect on the first factor $X_1$ increases, Type I errors on the second factor $X_2$ grow and reach high levels. This pattern is particularly pronounced under the log-normal distribution. We also observe that for the binomial distribution, error rates are high ($\approx 11\%$ for $n = 20$) even when effects on $X_1$ are zero. In addition, error rates further grow when the sample size increases.

::: {#fig-ratio-main}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
library(dplyr)
library(tidyverse)
source("dataReaders.R")
source("plotlying.R")

prefix <- "1_test_4x3_Ratio"
alpha <- .05
distributions <- c("norm", "lnorm", "exp", "cauchy", "poisson", "binom")
dnames <- c("Normal", "Log-normal", "Exponential", "Cauchy", "Poisson", "Binomial")
df <- readlyData(prefix, alpha, 1, distributions, dnames)
plotlyError(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 64)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitude $a_1$ of the main effect of $X_1$
:::

**Contrasts**. The same problems appear when we run ART's procedure for contrasts [@elkin:2021]. @fig-ratio-contrasts shows our results, where we report average error rates for three pairwise comparisons (since $X_2$ has three levels). In the rest of the paper, we only show results for overall effects, since results for contrasts exhibit the same patterns. 

::: {#fig-ratio-contrasts}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "1_test_4x3_Contrasts"

df <- readlyData(prefix, alpha, 1, distributions, dnames)
plotlyError(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 52)
```
Average Type I error rates ($\alpha = .05$) for **contrasts on factor $X_2$** as a function of the magnitude $a_1$ of the main effect of $X_1$ 
:::

**Interaction effects**. @fig-ratio-interaction-1 presents Type I error rates for the interaction effect $X_1 \times X_2$, when the main effect on $X_2$ is zero while the main effect on $X_1$ increases. Overall, we observe the same trends as for main effects. Again, ART fails in similar ways, although its error rates are now slightly lower. 

::: {#fig-ratio-interaction-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "1_test_4x3_Ratio"
df <- readlyData(prefix, alpha, 1, distributions, dnames)

plotlyError(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX1", max = 52)
```
Type I error rates ($\alpha = .05$) for the **interaction** $X_1 \times X_2$ as a function of the magnitude $a_1$ of the main effect of $X_1$ 
:::

Finally, @fig-ratio-interaction-1 presents our results when the main effects on the two factors increase in parallel. Error rates become exceptionally high in some cases, growing up to $100\%$. 

::: {#fig-ratio-interaction-2}
```{r, echo=FALSE, message=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- readlyData(prefix, alpha, 0, distributions, dnames)

#xlab <- TeX("\\text{main effect of factors }X_1\\text{ and }X_2\\text{ }(a_1 = a_2)")
plotlyError(df, xlab = "magnitude of main effects", var = "rateX1X2", xvar = "effectX1", max = 105)
```
Type I error rates ($\alpha = .05$) for the **interaction** $X_1 \times X_2$ as a function of the magnitudes $a_1 = a_2$ of the main effects of $X_1$ and $X_2$ 
:::

However, these results require special attention since interaction interpretation issues come now into place. Let us examine the performance of each method: 

- *PAR*. Error rates are high for all non-normal distributions, and become even higher when the sample size increases. As we explained in [Section 3](#interpretation), the parametric method cannot deal with scale differences that emerge when effects are transformed in a non-linear fashion. If we interpreted interactions based on absolute mean differences, disregarding their different scales (i.e., if we decided that the trend in @fig-interactions (left) is indeed a valid interaction effect), we would not observe these high error rates. 

- *RNK.* Error rates explode when both main effects exceed a certain level (e.g., when $a_1, a_2 \ge 4$ and $n = 20$). As shown in @fig-interactions-rank, the problem is due to the way the rank transformation deforms interactions.

- *INT.* The method exhibits a better behavior than RNK. Errors start again increasing for all distributions, but not until main effects become significantly large. An exception is the binomial distribution, for which the error rates of INT and RNK are similar. We also observe that for continuous distributions, both INT and RNK are scale invariant, since their performance is not affected by the choice of scale. 

- *ART.* It keeps error rates at correct levels as long as population distributions are normal. For all other distributions, its error rates grow rapidly as effect sizes increase. We distinguish between two sources of errors: (i) lack of statistical robustness as observed in our previous tests; and (ii) interaction interpretation issues (see [Section 3](#interpretation)). Our results confirm that ART is not scale invariant. On the contrary, it is extremely sensitive to the scale of the observed data. 

### Type I error rates in ordinal scales {#ordinal}
Our second experiment evaluates Type I error rates for ordinal scales. We test again a $4 \times 3$ repeated-measures design. We focus on individual Likert items levels and implement an ordered-probit method [@Liddell:2018] to discretize the latent variable $Y$ into 5, 7, or 20 ordinal levels. To derive the discretization thresholds, we first consider the range $[-2SD, 2SD]$ , where $SD$ is the overall standard deviation of the responses $y_{ijk}$. We then divide this range into 5, 7, or 20 intervals, following two different strategies: (i) setting thresholds to be equidistant; or (ii) varying the intervals' size by randomly drawing thresholds in the above range. @fig-ordinal presents examples of equidistant and random thresholds for a 5-level scale when the magnitude of main effect of $X_1$ is either $a_1 = 2$ or $a_1 = 8$, while all other effects are zero. 

::: {#fig-ordinal}
```{r, echo=FALSE, message=FALSE, fig.height=2.5, fig.width = 9, warning=FALSE}
plotThresholds()
```
The four vertical lines in each plot represent thresholds defining 5-level ordinal scales. Thresholds are either equidistant (top) or randomly distanced one from each other (bottom) within a range of $\pm 2$ standard deviations around the grand mean $\mu = 0$.  
:::

**Main effects**. @fig-ordinal-main present Type I errors for $X_2$'s main effect, as we vary $X_1$'s magnitude of main effect. Our results indicate that PAR, RNK, and INT consistently maintain error rates close to $5\%$ across all tested ordinal scales. In contrast, ART exhibits a significant inflation of error rates, although this issue becomes less severe when the number of levels increases. Notably, error rates are more pronounced for thresholds at random distances and tend to increase with sample size. 

::: {#fig-ordinal-main}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "2_test_4x3_Ordinal"

distributions <- c("likert5", "likert5B", "likert7", "likert7B", "likert20", "likert20B")
dnames <- c("5 - equidistant", "5 - random", "7 - equidistant", "7 - random", "20 - equidistant", "20 - random")
df <- readlyData(prefix, alpha, 1, distributions, dnames)

plotlyError(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 52)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitude $a_1$ of the main effect of $X_1$
:::

**Interaction effects**. @fig-ordinal-interaction-1 displays error rates for the interaction $X_1 \times X_2$ with a single main effect applied to $X_1$. The observed patterns align with our previous findings; ART consistently inflates error rates, although to a lesser extent now.  

::: {#fig-ordinal-interaction-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}

plotlyError(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX1", max = 45)
```
Type I error rates ($\alpha = .05$) for the **interaction** $X_1 \times X_2$ as a function of the magnitude $a_1$ of the main effect of $X_1$ 
:::

Finally, @fig-ordinal-interaction-1 presents our results when main effects are applied to both factors. None of the methods successfully maintains low error rates across all conditions. Intriguingly, we note that ART consistently performs worse than PAR. INT performs worse than ART and PAR in one specific case ($a_2, a_2 = 8$ in a scale with 20 equidistant levels), but overall, it exhibits a better behavior than all other methods. 

::: {#fig-ordinal-interaction-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- readlyData(prefix, alpha, 0, distributions, dnames)
plotlyError(df, xlab = "magnitude of main effects", var = "rateX1X2", xvar = "effectX1", max = 105)

```
Type I error rates ($\alpha = .05$) for the **interaction** $X_1 \times X_2$ as a function of the magnitudes $a_1 = a_2$ of the main effects of $X_1$ and $X_2$ 
:::

### Type I errors across experimental designs {#extra}
Our third experiment investigates the generalizability of the aforementioned results to other experimental designs involving two or three factors. We assess five out of the six ratio scales examined earlier. We exclude the Cauchy distribution and replace it by a 5-level ordinal scale with random thresholds, as detailed above.

**Main effects**. @fig-designs-main illustrates Type I errors for the main effect of $X_2$ while varying the magnitude of the main effect of $X_1$, with a focus on $n = 20$. Across all cases, RNK and INT consistently maintain error rates close to $5%$. PAR's error rate remains overall close to $5\%$ but reaches notably low levels for specific combinations of designs ($2 \times 2 \times 2$ and $3 \times 3 \times 3$ repeated-measures) and distributions (log-normal and exponential). In contrast, ART inflates error rates for all non-normal distributions, with discrepancies across different designs. We observe that ART is particularly problematic in discrete distributions when applied to the $2 \times 2 \times 2$ and $3 \times 3 \times 3$ designs.

::: {#fig-designs-main}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "3_test-Designs"
alpha = 0.05

distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readData(prefix, n = 20, alpha, effectType = 1, distributions)
df <- reshapeByDesign(df, dnames)

plotlyErrorByDesign(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1", max = 62)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitude $a_1$ of the main effect of $X_1$ ($n = 20$)
:::

We provide additional results for the between-subjects design in @fig-2x3-main, where we vary the effect of $X_2$ and measure Type I error rates on $X_1$. ART's error rates for the three discrete distributions appear higher than those observed in @fig-designs-main, indicating a potential dependence on the number of levels of the factors. Again, we observe a consistent trend where error rates increase with the sample size.

::: {#fig-2x3-main}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix2 <- "3_test-Designs_2x3"

df2x3 <- readlyData2(prefix2, alpha, effectType = 2, distributions, dnames)

plotlyError(df2x3, xlab = "magnitude of main effect", var = "rateX1", xvar = "effectX2", max = 62)
```
Additional results for the $2 \times 3$ between-subjects design, where we measure Type I error rates ($\alpha = .05$) for the **main effect of $X_1$.** We now vary the magnitude $a_2$ of the main effect of $X_2$.
:::

**Interaction effects**. @fig-designs-interaction-1 displays Type I error rates for the interaction effect $X_1 \times X_2$ while varying the effect of $X_1$ ($n = 20$). We observe consistent trends in line with our previous findings. For additional insights, we direct readers to our raw experimental data and supplementary materials, which demonstrate that these trends persist across other interaction terms, namely $X_1 \times X_3$, $X_2 \times X_3$, and $X_1 \times X_2 \times X_3$, within the two 3-factor designs.

::: {#fig-designs-interaction-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotlyErrorByDesign(df, xlab = "magnitude of main effect", var = "rateX1X2", xvar = "effectX1", max = 62)
```
Type I error rates ($\alpha = .05$) for the **interaction** $X_1 \times X_2$ as a function of the magnitude $a_1$ of the main effect of $X_1$ ($n = 20$) 
:::

Our results in @fig-designs-interaction-2 confirm that all methods exhibit varying degrees of difficulty in accurately inferring interaction effects when main effects are present on all interacting factors. While PAR and ART perform well for normal distributions, their error rates escalate more rapidly across all other distributions. ART's error rates become exceptionally low for certain designs (e.g., $2 \times 3$ and $2 \times 2 \times 2$), suggesting a challenge in detecting small interaction effects when main effects are large. RNK exhibits a similar pattern for the $2 \times 2 \times 2$ design. Notably, both methods display high error rates under ordinal and binomial scales, although these rates are lower compared to those of PAR and ART.

::: {#fig-designs-interaction-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readData(prefix, n = 20, alpha, effectType = 0, distributions)
df <- reshapeByDesign(df, dnames)
plotlyErrorByDesign(df, xlab = "magnitude of main effects", var = "rateX1X2", xvar = "effectX1", max = 105)
```
Type I error rates ($\alpha = .05$) for the **interaction** $X_1 \times X_2$ as a function of the magnitudes $a_1 = a_2$ of the main effects of $X_1$ and $X_2$  ($n = 20$)
:::

### Type I errors under unequal variances
Many statistical procedures assume equal variances among all levels of each independent variable, or more strictly, among all possible pairs of independent variable levels (known as sphericity assumption in repeated-measures ANOVA). nonparametric tests are often mistakenly considered to be free of such as assumptions, but as we discussed earlier, this is generally incorrect.

Our fourth experiment evaluates the behavior of the four methods when populations on the latent variable $Y$ have unequal variances. We examine three 2-factor designs: (i) a $4 \times 3$ within-subjects design; (ii) a $2 \times 3$ between-subjects design; and (iii) a $2 \times 4$ mixed design. We set all effects to zero and then vary the ratio $r_{sd}$ of standard deviations between the levels of the first factor $X_1$. @fig-unequal shows the five ratios that we test when $X_1$ has two levels. When $X_1$ has additional levels ($4 \times 3$ design), their standard deviation is randomly drawn in a range between the standard deviations of the first two levels. 

::: {#fig-unequal}
```{r, echo=FALSE, message=FALSE, fig.height=1.5, fig.width = 9, warning=FALSE}
plotVariances()
```
Varying the ratio $r_{sd}$ of standard deviations between the levels of $X_1$. The plots show examples of population distributions for factors with two categorical levels.    
:::

**Main effects**. We begin by investigating how the four methods detect main effects on $X_1$. The interpretation of results in this scenario is challenging due to differences in the original populations — although they share the same means (and medians), their variances differ. Depending on the null hypothesis of interest, conclusions may vary. Complicating matters further, non-linear transformations can result in distributions with differing means and medians. Consequently, the choice of statistical method may lead to different outcomes; one method may be sensitive to mean differences while others to median differences.

@fig-unequal-main-1 presents the percentage of times each method rejects the null hypothesis ($\%$ positives) at a significance level of $\alpha = .05$. If the null hypothesis of interest posits that "$X_1$ has no effect on the mean of the latent variable $Y$," then these instances should be considered as Type I errors. Under normal distributions, all methods either moderately inflate or deflate error rates as $r_{sd}$ increases. For the $4 \times 3$ within-subjects design ($r_{sd} \ge 2.5$), error rates for PAR, RNK, and ART reach levels of around $7-8\%$, while error rates for INT drop below $3\%$ for the $2 \times 4$ mixed design ($r_{sd} \ge 2.5$).

::: {#fig-unequal-main-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "4_test_Unequal_Variances"
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readData(prefix, n = 20, alpha, effectType = 1, distributions)
df <- reshapeByDesign(df, dnames, effectvars = c("sd_ratio", "effectX1","effectX2","effectX1X2"))
plotlyErrorByDesign2(df, xlab = "max ratio of variances between levels of X1", var = "rateX1", xvar = "sd_ratio", max = 82, ytitle = 'Positives (%)')
```
The percentage of positives (i.e., rejecting the null hypothesis) for the **main effect of $X_1$** as the ratio of standard deviations $r_{sd}$ on $X_1$ increases ($n=20$). Depending on the hypothesis of interest, these percentages can be interpreted as Type I error rates ($\alpha = .05$).
:::

The performance of RNK and INT remains unchanged across continuous distributions, a result expected due to the preservation of medians in such scales. Conversely, mean differences grow with increasing $r_{sd}$, which explains why PAR's ratio of positives reaches high levels under these distributions. However, this trend is not consistent across all three designs. The behavior of ART is less clear, further supporting our argument that its null hypothesis of interest is ill-defined. Under discrete distributions, the trends of positives vary across designs, with PAR and ART exhibiting the highest rates. 

We also investigate the influence of unequal variances among the levels of $X_1$ on positive rates for $X_2$, as depicted in @fig-unequal-main-2. Those can be reliably considered as Type I errors because source populations defined by $X_2$ are identical. The error rates of PAR, RNK, and INT do not seem to be affected by an increase of $r_{sd}$. In contrast, ART yields notably high error rates, surpassing acceptable levels even in the case of normal distributions (check results for the $2 \times 4$ mixed design). The observed error rates are comparatively low for the $2 \times 3$ between-subjects design.

::: {#fig-unequal-main-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotlyErrorByDesign2(df, xlab = "max ratio of variances between levels of X1", var = "rateX2", xvar = "sd_ratio", max = 52)
```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as the ratio of standard deviations $r_{sd}$ on $X_1$ increases ($n=20$)
:::

**Interaction effects**. Finally, we measure Type I error rates for the interaction effect $X_1 \times X_2$, shown in @fig-unequal-interaction. The error rates of ART exhibit similar trends, with a somewhat lower level under the $4 \times 3$ within-subjects design (compared to @fig-unequal-main-2). Additionally, for this design, we observe error inflation for the three other methods, albeit to a significantly lesser degree.

::: {#fig-unequal-interaction}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
plotlyErrorByDesign2(df, xlab = "max ratio of variances between levels of X1", var = "rateX1X2", xvar = "sd_ratio", max = 52)
```
Type I error rates ($\alpha = .05$) for the **interaction effect $X_1 \times X_2$** as the ratio of standard deviations $r_{sd}$ on $X_1$ increases ($n=20$)
:::


### Main effects in the presence of interactions {#effect-sizes}
In all previous experiments, we assumed zero interaction effects. However, we also need to understand whether weak or strong interaction effects could affect the sensitivity of the methods in detecting main effects. Our fifth experiment evaluates the Type I error rate of the methods in the presence of an interaction effect alone, or alternatively, in the presence of a simultaneous main effect. We focus again on the three 2-factor experimental designs that we evaluated for our previous experiment and set the sample size to $n = 20$.

To simulate populations in which interactions emerge in the absence of main effects, we examine perfectly symmetric cross-interactions. To this end, we slightly change the method we use to encode the levels of each factor, such that levels are uniformly positioned around 0. For a factor with three levels, we numerically encode the levels as $\{-0.5, 0, 0.5\}$. For a factor with four levels, we encode them as $\{-0.5, -0.1667, 0.1667, 0.5\}$. 

**Interaction effect only**. We first test how the interaction effect alone influences the Type I error rate on $X_2$. @fig-interactions-main-2 presents our results. We observe that both PAR and ART fail for many configurations. Error rates are especially high in the case of the $4 \times 3$ within-subjects design and the $2\times 4$ mixed design under the log-normal and exponential distribution. ART exhibits the worst performance. In constrast, RNK and INT keep error rates close to nominal levels. However, when interaction effects become sufficiently large ($a_{12} > 4$), we observe that under the binomial and ordinal scale, they also start inflating errors.

::: {#fig-interactions-main-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "7_test_Interactions"
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readData(prefix, n = 20, alpha, effectType = 0, distributions)
df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotlyErrorByDesign2(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1X2", max = 104)

```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitude $a_{12}$ of the interaction effect $X_1 \times X_2$ ($n=20$)
:::

**Interaction effect combined with main effect**. We also evaluate the Type I error rate on $X_2$ when the interaction effect is combined with a main effect on $X_1$. @fig-interactions-main-3 presents our results. The error rates of ART and PAR now explode for all non-normal distributions and all three designs. But the performance of RNK and INT is also affected. Their error rates become extremely low under continuous distributions, which suggests a lack of power in detecting small main effects when strong effects of other factors are combined with strong interactions. In contrast, the Type I error of the two methods explodes under the binomial and ordinal scale. Interestingly, RNK exhibits the best performance in these tests.    

::: {#fig-interactions-main-3}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "7_test_Interactions"
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readData(prefix, n = 20, alpha, effectType = 6, distributions)
df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotlyErrorByDesign2(df, xlab = "magnitude of main effect", var = "rateX2", xvar = "effectX1X2", max = 104)

```
Type I error rates ($\alpha = .05$) for the **main effect of $X_2$** as a function of the magnitudes $a_1 = a_{12}$ of a combined main effect on $X_1$ and an interaction effect $X_1 \times X_2$ ($n=20$)
:::


### Statistical power {#power}
Our sixth experiment compares the statistical power of the four methods on the three 2-factor designs. Because there is a tradeoff between Type I and Type II errors, high power can simply be the result of a high Type I error rate. Since parallel effects can inflate errors (see our previous results), we focus here on single effects, both main or interaction effects. We only report results for $n=20$ (see supplementary materials for additional results). 

**Main effects**. We individually vary the magnitude of effect on $X_1$ and $X_2$ and observe the power of each method to detect this effect ($\alpha = .05$). Specifically, we manipulate the parameters $a_1$ or $a_2$ within the range of $0.4$ to $1.0$. Depending on the experimental design, this range allows us to simulate both low-power experiments (e.g., with less than $30\%$ power) and high-power experiments (e.g., with more than $80\%$ power). 

@fig-power-main-1 presents our results for $X_1$. In several configurations, the differences in power among the methods are marginal. To enhance clarity in our comparisons, we present the method rankings. Readers are encouraged to interact with the graphs for precise power values. PAR exhibits the highest power under the normal distribution, closely followed by INT. INT is also consistently the winning method across all non-normal distributions. PAR's power is notably low under the log-normal and exponential distribution.

ART initially appears advantageous under the three discrete distributions for small effects, likely due to inflated Type I errors even without an effect on $X_2$ (as observed in @fig-2x3-main). However, as effects grow larger, its relative power diminishes, and ART eventually becomes the least effective method.

::: {#fig-power-main-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "5_test-Power"
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readData(prefix, n = 20, alpha, effectType = 3, distributions)

df <- df %>% arrange(design,distr,effectX1,rateX1)  %>% group_by(design,distr,effectX1) %>% mutate(rank = rank(rateX1))
df <- as.data.frame(df) %>% reshapeByDesign(dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

#df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotlyErrorByDesign3(df, xlab = "magnitude of main effect", var = "rank", hovervar = "rateX1", xvar = "effectX1", max = 4.2, ytitle = 'Power (%) - ranking')
```
Ranking of methods for their power ($\alpha = .05$) to detect the **main effect of $X_1$** as a function of the magnitude of effect $a_1$ ($n=20$). Hover over the charts to inspect their power ($\%$).
:::

@fig-power-main-2 presents results for $X_2$. Given the different number of levels for this factor, we now assess a different range of power for each design. Nevertheless, we observe similar patterns. INT stands out as the most powerful method. Although ART surpasses PAR under the log-normal and exponential distributions, it does not demonstrate any other notable advantages compared to the other methods.  

::: {#fig-power-main-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- readData(prefix, n = 20, alpha, effectType = 4, distributions) # CHANGE n TO 20

df <- df %>% arrange(design,distr,effectX2,rateX2)  %>% group_by(design,distr,effectX2) %>% mutate(rank = rank(rateX2))
df <- as.data.frame(df) %>% reshapeByDesign(dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

#df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotlyErrorByDesign3(df, xlab = "magnitude of main effect", var = "rank", hovervar = "rateX2", xvar = "effectX2", max = 4.2, ytitle = 'Power (%) - ranking')
```
Ranking of methods for their power ($\alpha = .05$) to detect the **main effect of $X_2$** as a function of the magnitude of effect $a_2$ ($n=20$). Hover over the charts to inspect their power ($\%$).
:::

**Interaction effects**. We also vary the magnitude of the interaction effect $X_1 \times X_2$ by manipulating the parameter $a_{12}$ across the range of $0.5$ to $2.0$. We then measure the power to detect this interaction effect. Our results are summarized in @fig-power-interaction, aligning with findings for main effects. ART's relative advantage diminishes later under the binomial and ordinal distributions. Once again, the method's inflated Type I error rates could account for this observed trend.

::: {#fig-power-interaction}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
df <- readData(prefix, n = 20, alpha, effectType = 5, distributions)

df <- df %>% arrange(design,distr,effectX1X2,rateX1X2)  %>% group_by(design,distr,effectX1X2) %>% mutate(rank = rank(rateX1X2))
df <- as.data.frame(df) %>% reshapeByDesign(dnames, effectvars = c("effectX1","effectX2","effectX1X2"))

#df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotlyErrorByDesign3(df, xlab = "magnitude of interaction effect", var = "rank", hovervar = "rateX1X2", xvar = "effectX1X2", max = 4.2, ytitle = 'Power (%) - ranking')
```
Ranking of methods for their power ($\alpha = .05$) to detect the **interaction effect $X_1 \times X_2$** as a function of the magnitude of effect $a_{12}$ ($n=20$). Hover over the charts to inspect their power ($\%$).
:::


### Effect size estimates {#effect-sizes}
In our final experiment, we assess the accuracy of each method in estimating standardized effect sizes across the three 2-factor designs. We employ a parametric ANOVA on the latent variable $Y$ and calculate partial $\eta^2$ for the effects of $X_1$, $X_2$, and $X_1 \times X_2$, treating these values as ground truth estimates. Subsequently, we evaluate the corresponding partial $\eta^2$ using each of the four methods (PAR, RNK, ART, and INT) on the observed variable $Y'$.

Then, we compute the coefficient of determination $R^2$ ($R$ squared) over 1000 iterations, that is, the proportion of variation in the $\eta^2$ estimates of each method that is predictable from the estimate of the grand truth method. In each iteration, we randomly draw the magnitude of all three effects ($a_1$, $a_2$, and $a_{12}$) from a range $[-g, g]$, where $g$ varies in $\{0, 0.5, 1, 2, 4, 8\}$. This $R^2$ measure allows us to assess how close the estimates of each method are to estimates of the theoretically most appropriate method. We present the results for a sample size of $n=20$. 

Notice that this experiment studies a wider range of scenarios, where main and interaction effects are present at the same time.

**Main effects**. @fig-effect-main-1 depicts trends for the first factor $X_1$. All methods demonstrate high $R^2$ levels under the normal distribution. INT performs exceptionally well across all continuous distributions. However, its performance declines when the range of effects becomes large ($g \ge 4$). PAR's performance is strikingly low under the log-normal distribution and, to a lesser degree, under the exponential distribution. ART emerges as the second-worst method for these specific distributions. Finally, we observe low $R^2$ values for all methods under the binomial and ordinal distributions, where ART's performance is particularly problematic under low effect size ranges.  

::: {#fig-effect-main-1}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "6_test-Effect-Size"
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readData(prefix, n = 20, alpha= NA, effectType = -1, distributions)

df <- df %>% arrange(design,distr,effectX1,etaX1)  %>% group_by(design,distr,effectX1)
df <- as.data.frame(df) %>% reshapeByDesign(dnames, effectvars = c("effectX1","effectX2","effectX1X2"), groupvars = c("distr","method","n"))

#df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotlyErrorByDesign2(df, xlab = "range of effects (g)", var = "etaX1", xvar = "effectX1", min = -5, max = 103, ytitle = 'R squared')
```
$R^2$ scores measuring the relationship between estimates (partial $\eta^2$) of each method and estimates of the ground truth method for the main effect on $X_1$. We express these scores as a function of the range of the magnitude of effects $g$.
:::

@fig-effect-main-scatterplots illustrates the relationship between the $\eta^2$ estimated by each method and the $\eta^2$ of the ground truth method, when $g = 2$. For clarity, we focus on the normal, log-normal, and ordinal distributions. The proximity of data points to the black diagonal indicates the closeness of estimates to the ground truth. Points above the line are likely to overestimate the effect size, while points under the line are likely to underestimate it. We observe that PAR tends to underestimate effect sizes under the log-normal distribution, which is consistent with the low power of the method for this distribution (see @fig-power-main-1). In contrast, ART's estimates are spread across both sides. INT demonstrates the highest precision, followed by RNK. Both methods, however, fail to produce accurate estimates under the ordinal scale, tending to underestimate effects. Still, ART performs worse than the other methods.

::: {#fig-effect-main-scatterplots}
```{r, echo=FALSE, fig.height=3.4, message=FALSE, warning=FALSE}
source("effect_sizes_scatterplots.R")
prefix <- "6_scatter-Effect_Size"
df <- readlyDataPoints(prefix,  distributions=c("norm", "lnorm", "likert5B"), dnames = c("Normal", "Log-normal", "Ordinal (5 levels)"))
fig <- plotlyScatter(df, max = 0.51)

fig
```
Scatterplots showing the relationship between effect size estimates (partial $\eta^2$) of each method and estimates of the ground-truth method for the main effect of $X_1$, when the magnitude of the three effects is within the range $[-2, 2]$. 
:::

@fig-effect-main-scatterplots-2 shows the same relationship but for a larger range of effects ($g=8$). Notice that effect sizes are severely distorted by all four methods. We invite the reader to zoom in on the lower range of values (partial $\eta^2 < 0.6$), where ART's estimates for the two non-normal distributions appear as quasi-random. Interestingly, the performance of RNK and INT also deteriorates, as the methods seem to struggle with detecting main effects in the presence of strong interactions.

::: {#fig-effect-main-scatterplots-2}
```{r, echo=FALSE, fig.height=3.4, message=FALSE, warning=FALSE}
source("effect_sizes_scatterplots.R")
prefix <- "6_scatter-Effect_Size-g8"

df <- readlyDataPoints(prefix,  distributions=c("norm", "lnorm", "likert5B"), dnames = c("Normal", "Log-normal", "Ordinal (5 levels)"))
fig <- plotlyScatter(df, max = 1.1)

fig
```
Scatterplots showing the relationship between effect size estimates (partial $\eta^2$) of each method and estimates of the ground-truth method for the main effect of $X_1$, when the magnitude of the three effects is within the range $[-8, 8]$. 
:::

<!-- 
@fig-effect-main-2 presents $R^2$ trends for the second factor $X_2$. 
 These trends mirror the observed patterns for $X_1$ in @fig-effect-main-1. However, upon closer inspection of the results for the $2 \times 3$ between-subjects and the $2 \times 4$ mixed designs  across the three discrete distributions, the differences between the four methods are less pronounced.

::: {#fig-effect-main-2}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "6_test-Effect-Size"
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readData(prefix, n = 20, alpha= NA, effectType = -1, distributions)

df <- df %>% arrange(design,distr,effectX2,etaX2)  %>% group_by(design,distr,effectX2)
df <- as.data.frame(df) %>% reshapeByDesign(dnames, effectvars = c("effectX1","effectX2","effectX1X2"), groupvars = c("distr","method","n"))

#df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotlyErrorByDesign2(df, xlab = "range of effects (g)", var = "etaX2", xvar = "effectX2", min = -5, max = 103, ytitle = 'R squared')
```
$R^2$ scores measuring the relationship between estimates (partial $\eta^2$) of each method and estimates of the ground truth method for the main effect on $X_2$. We express these scores as a function of the range of the magnitude of effects $g$.
:::
 -->

We also show results for the interaction effect in @fig-effect-interaction. $R^2$ can approach zero or even become negative under certain conditions, implying that effect size estimates cannot be trusted. Both RNK and INT start failing when the range of effects becomes large. Additionally, we observe low $R^2$ values for all methods under an ordinal or binomial scale. These results suggest that interaction effect estimates are not reliable in this case. 

::: {#fig-effect-interaction}
```{r, echo=FALSE, fig.height=3.3, fig.width = 9, warning=FALSE}
prefix <- "6_test-Effect-Size"
distributions = c("norm", "lnorm", "exp", "poisson", "binom", "likert5B")
dnames = c("Normal", "Log-normal", "Exponential", "Poisson", "Binomial", "Ordinal (5 levels)")

df <- readData(prefix, n = 20, alpha= NA, effectType = -1, distributions)

df <- df %>% arrange(design,distr,effectX1X2,etaX1X2)  %>% group_by(design,distr,effectX1X2)
df <- as.data.frame(df) %>% reshapeByDesign(dnames, effectvars = c("effectX1","effectX2","effectX1X2"), groupvars = c("distr","method","n"))

#df <- reshapeByDesign(df, dnames, effectvars = c("effectX1","effectX2","effectX1X2"))
plotlyErrorByDesign2(df, xlab = "range of effects (g)", var = "etaX1X2", xvar = "effectX1X2", min = -51, max = 105, ytitle = 'R squared')
```
$R^2$ scores measuring the relationship between estimates (partial $\eta^2$) of each method and estimates of the ground truth method for the interaction effect $X_1 \times X_2$. We express these scores as a function of the range of the magnitude of effects $g$.
:::


## Case studies {#casestudies}
In this section, we examine some specific types of data through concrete examples. We focus on recent work (published in 2023) that has used ART and has made data publicly available. We identify problems and evaluate possible risks. We also show alternative methods for conducting the analysis and compare their outcomes.

### Binary responses

@Martin:2023 ran two experiments to investigate the psychological phenomenon of change blindness, that is, how humans fail to detect certain changes in a scene. They ran two experiments, but we focus here on their first experiment which evaluated how 22 participants succeeded in observing object manipulations in an immersive environment. The authors studied the influence of four different factors, namely the Type of manipulation (4 levels), its Distance from the observer (2 levels), the Complexity of the manipulated item (2 levels), and the field of view (not examined here). To analyze the effect of the first three factors, the authors used a $4 \times 2 \times 2$ repeated-measures ANOVA using ART. 

Their design has two particularities. First, the response variable is binary (Detected vs. Not Detected). Second, the design is unbalanced; each participant did not complete the same tasks, so their types and complexities did not appear with the same frequency among participants. The authors pointed to these issues to justify the use of ART: 

> *"Since we are dealing with **nonparametric, unbalanced data** and multiple factors, we apply the Aligned Rank Transform (ART) for nonparametric factorial ANOVA"* [@Martin:2023]. 

In their ART model, the authors treated the participant identifier and the task identifier as random effects. We could re-run their analysis to replicate the results presented in Table 1 [@Martin:2023]. However, we extended the analysis by employing two additional methods: (i) fitting a linear mixed-effects model (LMER function) without any transformation; and (ii) fitting a binomial generalized mixed-effects model (GLMER function with binomial distribution family). The latter method essentially performs a logistic regression and is considered the most appropriate parametric procedure for this type of data. Below, we present the *p*-values obtained from each method:

|  | ART | LMER | GLMER (binomial family) |
|------|-----|-----|-----|
| Type | $.92$ | $.016$ | $.036$ | 
| Distance | $.031$ | $.12$ | $.29$ |
| Complexity | $.023$ | $.0027$ | $.000012$ |
| Type $\times$ Distance | $.71$ | $.13$ | $.013$ |
| Type $\times$ Complexity | $.00077$ | $.0039$ | $.000098$ |
| Distance $\times$ Complexity | $.0011$ | $.0029$ | $.000032$ |
| Type $\times$ Distance $\times$ Complexity | $.00055$ | $.00016$ | $.00013$ |
: *p*-values keeping two significant digits - Type III hypothesis tests {.sm}

There are notable differences among the results obtained from the three methods. Specifically, GLMER provides evidence for a main effect of Type ($p=.036$ compared to $p=.92$ for ART) and an interaction effect Type $\times$ Distance ($p=.013$ compared to $p=.71$ for ART). In contrast, its $p$-value for the main effect of distance is significantly larger ($p=.29$ compared to $p=.031$ for ART). These discrepancies may raise questions regarding the conclusions of the paper on the influence of the above factors.

It is important to note, however, that the aforementioned *p*-values are outcomes of Type III hypothesis tests. While these tests are deemed more suitable for unbalanced data when interaction effects emerge, their use remains a subject of debate [@Smith:2022], and some experts argue that interpreting main effects in the presence of interactions may not be meaningful [@Hector:2010]. The ARTool raises a warning for such unbalanced designs:

> *F values of ANOVAs on aligned responses not of interest are not all ~0. ART may not be appropriate.*

We have not examined how design imbalance affects ART's accuracy. 

**Monte Carlo simulation.** What if the experimental design were balanced, and this warning did not appear? What risks would arise from using ART with such binary responses? To delve into this issue, we conduct a new Monte Carlo experiment.

As @Martin:2023, we investigate a $4 \times 2 \times 2$ repeated-measures design and set the number of participants to $n=22$. However, we now test a perfectly balanced design. We transform the latent variable to a Bernoulli distribution with a success probability parameter $p=.46$ ---average success rate found by @Martin:2023--- and then assess the Type I error rate of ART in the absence of any effect. We also assess the Type I error rate of regular ANOVA with no transformation (PAR). When responses are binary, RNK and INT do not alter the data, leading to results that coincide with those of PAR. Our results over 5000 iterations are as follows: [Provide percentages to ease reading?]{.g}

|  | $X_1$ | $X_2$ | $X_3$ | $X_1 \times X_2$ | $X_1 \times X_3$ | $X_2 \times X_3$ | $X_1 \times X_2 \times X_3$ |
|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
| PAR | $.047$ | $.051$ | $.048$ | $.057$ | $.048$ | $.049$ | $.044$ |
| ART | $.212$ | $.208$ | $.200$ | $.220$ | $.215$ | $.197$ | $.213$ |
: Type I error rates for $\alpha = .05$ {.sm}

|  | $X_1$ | $X_2$ | $X_3$ | $X_1 \times X_2$ | $X_1 \times X_3$ | $X_2 \times X_3$ | $X_1 \times X_2 \times X_3$ |
|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
| PAR | $.010$ | $.011$ | $.011$ | $.012$ | $.011$ | $.010$ | $.009$ |
| ART | $.095$ | $.094$ | $.095$ | $.105$ | $.097$ | $.095$ | $.092$ |
: Type I error rates for $\alpha = .01$ {.sm}

We observe that ART exhibits significant challenges with such binary data, as its error rates escalate to very high levels for all main and interaction effects. In contrast, PAR consistently maintains error rates close to nominal levels. Despite the higher rate of Type I errors, ART seems to detect main effects on $X_2$ and $X_3$ with lower power compared to PAR. For example, for $a_2 = 0.5$ (main effect of $X_2$), its power is $79\%$ compared to a power of $90\%$ for PAR ($\alpha = .05$). In conclusion, ART is completely inappropriate when responses are binary, even when the average rate of each binary outcome is close to $50\%$. 

### Likert items 

@Rosso:2023 conducted a body-swap experiment, where pairs participants ("dyads") performed a joint-finger tapping task under four different conditions, structured under two different factors:

- *Coupling*. Participants saw either their own hand (*Coupled*) or their partner's hand (*Uncoupled*).

- *Perspective*. Participants saw the hand from a first person (*1P*) perspective or from a second person perspective (*2P*). 

Overall, the experiment was structured as a $2 \times 2$ within-participants design with 38 participants (19 dyads). The authors investigated numerous measures. Here, we only revisit their analysis of the sense of ownership of the visually perceived hand, for which they used ART (Page 10). The variable was measured through a self-reported scale with five levels (1 to 5). The authors report:

> *Aligned rank transform (ART) ANOVA revealed significant main effects of Coupling (Df residual = 147, F = 104.353, p < 0.001) and Perspective (Df residual = 147, F = 8.983, p < 0.01) on the self-reported ownership ratings. The former indicates that participants were capable of telling apart their own hand from the partner’s regardless of the visual perspective, whilst the latter indicates that perceiving a hand in 1st person generally resulted in a stronger sense of ownership. Crucially, the interaction effect between Coupling and Perspective (Df residual = 147, F = 5.232, p < 0.05) revealed that the increase in ownership relative to the 2nd person perspective was significantly stronger when participants were coupled.* [@Rosso:2023]

Thus, the authors found supportive statistical evidence for both main effects and the interaction of the two factors. We replicated their analysis and found that the author did not conduct a repeated-measures ANOVA, i.e., they did not treat ratings from the same participant as independent. We re-analyzed the data with a mixed-effects *lmer* model, treating the participant identifier as a random effect nested under the identifier of participant pairs (dyads). Conclusions do not change since our analysis is more powerful, so $p$-values are now even smaller for all three effects. However, we also conducted the analysis using the three other methods (PAR, RNK, INT), and unfortunately, conclusions are very different:

|  | ART | PAR | RNK | INT |
|------|-----|-----|-----|-----|
| Coupling | $3.2 \times 10^{-19}$ | $4.9 \times 10^{-18}$ | $9.5 \times 10^{-20}$ | $5.4 \times 10^{-19}$ |
| Perspective | $.0016$ | $.20$ | $.25$ | $.29$ |
| Coupling $\times$ Perspective | $.015$ | $.78$ | $.90$  | $.95$ |
: *p*-values keeping two significant digits {.sm}

All four methods agree that there is strong evidence about the effect of Coupling. Nevertheless, the results of PAR, RNK, and INT for the effects of Perspective and Coupling $\times$ Perspective do not support the authors' conclusions. The discrepancy between the $p$-values returned by these methods and the $p$-values of ART is striking. 

One may notice that a single entry (out of 152 entries) in the above dataset is missing, so the design is slightly unbalanced, and the ARTool again outputs a warning. Could this imbalance explain the above differences? The answer is negative. If we complete the missing row with a neutral value (e.g., a 3 in the 5-level ordinal scale) to dismiss the warning, the resulting *p*-values will still be very similar to the ones reported above.  

We also conducted an analysis with a cumulative probit model, using a Bayesian statistics framework [@Burkner:2019], without informative priors:

```{r, eval=FALSE}
library(brms)
fit_ownership <- brm(
  formula = ownership ~ Coupling + Perspective + Coupling:Perspective 
    + (1|Dyad/Participant),
  data = ownership_data,
  family = cumulative("probit")
)
```

 For an extensive discussion about why using a Bayesian framework for this type of models, we refer interested readers to @Liddell:2018. We have previously used such models for the analysis of ordinal responses in two experimental studies [@fages:2022], but the HCI community is not familiar with this method.

Below, we present estimates of the regression coefficients of the model together with their $95\%$ credible intervals: 

| Coupling (Coupled $-$ Uncoupled) | Perspective (1P $-$ 2P) | Coupling $\times$ Perspective |
|------|-----|-----|
| $-1.95$ $[-2.58, -1.35]$ | $0.28$ $[-0.39, 0.95]$ | $-0.09$ $[-0.89, 0.75]$ |
: Regression coefficients and their $95\%$ credible intervals, expressed as standard deviations of the latent variable {.sm}

Notice that all effects are expressed in standard deviation units over the latent variable, so they are analogous to Cohen's $d$ standardized effect sizes. The above results suggest that participants' sense of ownership was $1.95$ standard deviations lower in the Coupled condition, where standard deviations refer to the continuous latent scale of sense of ownership. The whole $95\%$ is far below zero, which means that there is overwhelming evidence that this effect is strong. Credible intervals are analogous to the confidence intervals of frequential statistics, but unlike confidence intervals, they allow for a probabilistic interpretation. For example, we can conclude that the above credible interval contains the true difference in people's sense of ownership with a $95\%$ probability.  

In contrast, we observe that the credible intervals for Perspective and the interaction term extend from negative to positive values. Thus, there is no sufficient statistical evidence for these effects. These results are consistent with the results of our *lmer* models using PAR, RNK, and INT. In conclusion, using ART to analyze these data is problematic. Here, the presence of a strong effect on the first factor seems to make ART sensitive in detecting non-existent effects, or perhaps in exaggerating tiny effects. 

### Likert scales

Other authors have used ART to analyze Likert scales that group several items together. For example, @Karolus:2023 investigate different methods (referred to as "gamification types") for communicating feedback on users' task proficiency, such as during a writing task. The authors used a $3 \times 2$ between-participants design to structure the study, testing the following two factors:

- *Gamification type*. Each participant was presented with one of the following elements providing feedback: (i) a *progress bar* (ii) an *Emoji*, or (iii) *none*. 

- *Feedback type*. Feedback was either *continuous* or *revision*-based (provided at the end of the task to help participants revise). 

The authors collected responses from 147 participants through the Amazon Mechanical Turk Service. They analyzed a range of measures, but here we focus on their analysis of participants' responses to the Situational Motivation Scale (SIMS) [@Blanchard_SIMS:2000]. The scale consists of four subscales (intrinsic motivation, identified regulation, external regulation, and amotivation), where each contains four 7-level Likert items. For each subscale, the authors conducted an ANOVA using ART on the average score. Below, we present the results of the same analysis with all four methods for intrinsic motivation and identified regulation:

|  | ART | PAR | RNK | INT |
|------|-----|-----|-----|-----|
| Gamification | $.0057$ $(0.07)$| $.010$ $(0.06)$| $.0058$ $(0.07)$| $.0062$ $(0.07)$|
| Feedback | $.77$ | $.59$ | $.56$ | $.55$ |
| Gamification $\times$ Feedback | $.94$ | $.69$ | $.94$  | $.86$ |
: Intrinsic motivation: *p*-values keeping two significant digits (partial $\eta^2$ in parentheses when $p < .05$) {.sm}

|  | ART | PAR | RNK | INT |
|------|-----|-----|-----|-----|
| Gamification | $.019$ $(0.05)$ | $.011$ $(0.06)$ | $.013$ $(0.06)$ | $.0085$ $(0.07)$|
| Feedback | $.55$ | $.74$ | $.42$ | $.55$ |
| Gamification $\times$ Feedback | $.99$ | $.86$ | $.98$  | $.93$ |
: Identified regulation: *p*-values keeping two significant digits (partial $\eta^2$ in parentheses when $p < .05$) {.sm}

Overall, differences among the results of the four methods are reasonably small and support the authors' conclusions. We omit results for external regulation and amotivation for which all *p*-values are greater than $.05$ --- again, results are consistent among the four methods. 

In many situations, the above methods will behave correctly and produce similar results. However, it is difficult to know in advance when problems will arise. In a different experiment, @Siestrup:2023 investigated how different types of modifications in video sequences describing a story affect people's episodic memory. The experiment included a rating task, asking participants to rate how much the storyline of modified episodes deviated from an original version (from $1 = 0\%$ different to $6 = 100\%$ different). The authors used ART to analyze aggregated scores from multiple ratings per condition. Below, we present the results of our re-analysis with the four different methods:

|  | ART | PAR | RNK | INT |
|------|-----|-----|-----|-----|
| Version | $4.5 \times 10^{-20}$ | $2.7 \times 10^{-30}$ | $4.7 \times 10^{-20}$ | $2.0 \times 10^{-17}$ |
| Modification | $.034$ | $.11$ | $.23$ | $.40$ |
| Version $\times$ Modification | $.19$ | $.17$ | $.18$  | $.058$ |
: *p*-values keeping two significant digits {.sm}

We see now that results from different methods can lead to different conclusions. In particular, the authors' statement that *"modified videos that had already been presented during the fMRI session received lower story-change ratings than those that had been presented in the original version before"* [@Siestrup:2023], based on the observed effect of the second factor (Modification), is only supported by their analysis with ART. Effect size estimates for this factor also vary among methods, although their wide confidence intervals indicate that those estimates are highly uncertain: 

|  | ART | PAR | RNK | INT |
|------|-----|-----|-----|-----|
| partial $\eta^2$| $0.12$ $[.01, 1.0]$ | $0.07$ $[.00, 1.0]$| $0.04$ $[.00, 1.0]$| $0.02$ $[.00, 1.0]$|
: effect size estimates for Modification and their $95\%$ confidence intervals {.sm}

Several authors have argued that parametric statistics are robust enough to be used with Likert scales that group multiple items together [@Carifio:2008], or with individual Likert items with at least five levels [@Harper:2015]. However, @Liddell:2018 identified several situations in which parametric tests may fail, even when they are applied to averages of multiple ordinal items. For example, they can inflate errors when the variances are not equal. They can also deform interaction effects. Unfortunately, rank transformations suffer from similar problems, e.g., we observed that all methods can detect non-existent interactions in the presence of strong main effects (see @fig-interactions). We can show that such problems persist even when we take averages of multiple correlated items.

@Liddell:2018 recommend using ordered-probit models for the analysis of Likert scales. @Burkner:2019 demonstrate how to fit such models through specific examples. Here, we apply their approach to analyze participants' responses for intrinsic motivation [@Karolus:2023]. To this end, we do not average the individual Likert items of the scale. We treat instead items and participants as a random effects through the following mixed-effects model:  

```{r, eval=FALSE}
library(brms)
fit_intrinsic_motivation <- brm(
  formula = intrinsic_motivation ~ 1 + Gamification:Feedback  
    + (1|Item) + (1|Participant),
  data = sims_data,
  family = cumulative("probit")
)
```
Fitting this model allows us, for instance, to estimate the difference between the progresive bar and the Emoji representation to be $1.08$, $95\%$ Credible Interval $= [0.10, 2.06]$. Units represent again standard deviations of the latent psychological variable of intrinsic motivation. 

<!-- **Simulated data.** Do rank-transformation methods address the issues raised by @Liddell:2018?  


We run a Monte Carlo experiment the simulates the study of @Karolus:2023, but we focus instead on a fully-balanced $3 \times 2$ design with a cell size of $n=25$. Since we   


To this end, we consider a $3 \times 2$ between-participants design and fix the cell size to $n = 25$. This    -->  

<!-- ### Proportions: AUC and false alarm rate [@Siestrup:2023]
@Siestrup:2023 ran an experiment to study how different types of modifications in video sequences describing a story affect people's episodic memory. On the first day, 37 participants encoded videos (i.e., they watched them several times) of short toy stories. On the next day, the participants returned for an fMRI session and were presented either the originally encoded videos or modified versions of them. On the third day, the participants completed a post-fMRI memory test. They rewatched original and modified versions of the videos and had to decide whether the stories in the videos had been encoded during the first day. We focus on two response variables that measure participants' memory accuracy in the post-fMRI memory test: false alarm rate (FAR) and area under the curve (AUC).

The experiment studied the effect of two factors:

- *Modification*. The story videos presented on the second day (fMRI session) were either *modified* or *original*.

- *Version*. During the post-fMRI memory test, videos were displayed with a *gist* or a *surface* modification.  

The authors ran a $2 \times 2$ repeated-measures ANOVA using ART to test their effect on FAR and AUC.  

|  | ART | PAR | RNK | INT |
|------|-----|-----|-----|-----|
| Modification | $1.8 \times 10^{-7}$ ($0.54$) | $.0004$ ($0.30$) | $8.7 \times 10^{-6}$ ($0.43$)| $3.1 \times 10^{-5}$ ($0.39$)|
| Version | $2.6 \times 10^{-22}$ ($0.93$)  | $1.7 \times 10^{-12}$ ($0.75$) | $3.3 \times 10^{-16}$ ($0.88$) | $4.8 \times 10^{-14}$ ($0.80$)|
| Modification $\times$ Version | $.00014$ ($0.34$) | $.018$ ($0.15$) | $.016$ ($0.15$) | $.010$ ($0.17$) |
: FAR: *p*-values keeping two significant digits and partial $\eta^2$ (in parentheses) {.sm}

|  | ART | PAR | RNK | INT |
|------|-----|-----|-----|-----|
| Modification | $.00011$ ($0.34$)| $.0034$ ($0.21$) | $.00064$ ($0.28$) | $.00054$ ($0.29$)|
| Version | $1.7 \times 10^{-17}$ ($0.87$)| $1.3 \times 10^{-10}$ ($0.69$) | $8.3 \times 10^{-14}$ ($0.79$) | $3.1 \times 10^{-12}$ ($0.75$)|
| Modification $\times$ Version | $.0012$ ($0.26$)| $.016$ ($0.15$) | $.036$ ($0.12$) | $.026$ ($0.13$) |
: AUC: *p*-values keeping two significant digits and partial $\eta^2$ (in parentheses) {.sm}
 -->


## Recommendations {#recommendations}

## Conclusion {#conclusion}

## References {.unnumbered}

::: {#refs}
:::
